{
  "hash": "100b8d2c5be24fa22da67113041323f3",
  "result": {
    "engine": "knitr",
    "markdown": "# Files, Pipes, and Redirection {#sec-files-pipes}\n\n\n\n::: {.callout-note title=\"Learning Objectives\"}\nAfter completing this chapter, you will be able to:\n\n- Explain the Unix philosophy and how it shapes command-line tools\n- Read files efficiently using `cat`, `less`, `head`, and `tail`\n- Use input and output redirection to work with files\n- Chain commands together using pipes\n- Apply these tools to process genomic data files\n:::\n\n## The Unix Philosophy\n\nUnix tools are designed around a simple but powerful philosophy [@kernighan1984unix]:\n\n> Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.\n\nThis philosophy has profound implications for how you work:\n\n1. **Small, focused tools**: Each command does one specific task excellently\n2. **Composition**: Complex operations are built by combining simple tools\n3. **Text as interface**: Programs communicate through text, making them compatible\n\nUnderstanding this philosophy transforms how you approach computational problems. Instead of seeking one program that does everything, you learn to construct pipelines from simple components.\n\n## Standard Streams: The Foundation\n\nEvery Unix program has three standard communication channels, called **streams**:\n\n```\n         ┌──────────────┐\nstdin ───>│              │───> stdout\n    (0)   │   Program    │      (1)\n          │              │───> stderr\n          └──────────────┘      (2)\n```\n\n**Standard Input (stdin)**\n:   Where programs read input. File descriptor 0. Default: keyboard.\n\n**Standard Output (stdout)**\n:   Where programs write normal output. File descriptor 1. Default: terminal screen.\n\n**Standard Error (stderr)**\n:   Where programs write error messages. File descriptor 2. Default: terminal screen.\n\nThese streams can be redirected—connected to files or other programs—which is the key to building powerful pipelines.\n\n## Reading Files\n\n### Choosing the Right Tool\n\nDifferent situations call for different tools:\n\n| Command | Best For |\n|:--------|:---------|\n| `cat` | Small files, concatenating files |\n| `less` | Large files, browsing, searching |\n| `head` | First few lines of a file |\n| `tail` | Last few lines, monitoring logs |\n| `wc` | Counting lines, words, characters |\n\n: File reading commands and their uses {#tbl-reading-commands}\n\n### `cat`: Concatenate and Display\n\n```bash\n# Display a file\n$ cat data.txt\n\n# Display multiple files (concatenated)\n$ cat header.txt data.txt footer.txt\n\n# Number the output lines\n$ cat -n data.txt\n\n# Show non-printing characters (debugging whitespace)\n$ cat -A data.txt\n```\n\n::: {.callout-warning}\nNever use `cat` on large files! It will flood your terminal with text. For genomic data files, always use `head`, `less`, or process through pipes.\n:::\n\n### `less`: The Pager\n\nFor large files, `less` lets you navigate page by page:\n\n```bash\n$ less huge_file.txt\n```\n\nKey navigation commands in `less`:\n\n| Key | Action |\n|:----|:-------|\n| Space, f | Forward one page |\n| b | Back one page |\n| g | Go to beginning |\n| G | Go to end |\n| /pattern | Search forward |\n| ?pattern | Search backward |\n| n | Next search match |\n| N | Previous search match |\n| q | Quit |\n\n: Navigation keys in `less` {#tbl-less-keys}\n\n### `head` and `tail`: Peek at Extremes\n\n```bash\n# First 10 lines (default)\n$ head data.csv\n\n# First 20 lines\n$ head -n 20 data.csv\n\n# All but the last 5 lines\n$ head -n -5 data.csv\n\n# Last 10 lines (default)\n$ tail data.csv\n\n# Last 50 lines\n$ tail -n 50 data.csv\n\n# Everything after line 100\n$ tail -n +100 data.csv\n```\n\nA special use of `tail` is monitoring files that are being written:\n\n```bash\n# Follow a file as it grows (Ctrl+C to stop)\n$ tail -f analysis.log\n```\n\nThis is invaluable for watching job progress on computing clusters.\n\n## Output Redirection\n\nRedirection connects a program's output to a file instead of the terminal.\n\n### Overwrite: `>`\n\n```bash\n# Save command output to a file\n$ ls -l > file_list.txt\n\n# Save date to a file\n$ date > timestamp.txt\n\n# Redirect output from any command\n$ echo \"Hello, World!\" > greeting.txt\n```\n\n::: {.callout-caution}\nThe `>` operator **overwrites** existing files without warning! Double-check your filenames before running commands with `>`.\n:::\n\n### Append: `>>`\n\n```bash\n# Add to an existing file\n$ echo \"New line\" >> notes.txt\n\n# Append today's entries to a log\n$ date >> daily_log.txt\n$ echo \"Analysis completed\" >> daily_log.txt\n```\n\n### Redirecting Errors: `2>`\n\nSometimes you want to separate normal output from error messages:\n\n```bash\n# Send errors to a file, output to screen\n$ ls /nonexistent 2> errors.txt\n\n# Save output and errors to different files\n$ command > output.txt 2> errors.txt\n\n# Discard errors (send to /dev/null)\n$ command 2> /dev/null\n\n# Combine stdout and stderr to one file\n$ command &> all_output.txt\n```\n\n## Input Redirection\n\nInput redirection feeds file contents to a program's stdin:\n\n```bash\n# Count lines in a file (two equivalent ways)\n$ wc -l < data.txt\n$ wc -l data.txt\n\n# Sort input from a file\n$ sort < names.txt\n```\n\nWhile many commands can read files directly as arguments, input redirection is essential for programs that only read from stdin.\n\n## Pipes: Connecting Commands\n\n**Pipes** (`|`) are the secret weapon of Unix. They connect the stdout of one command to the stdin of another, creating a processing pipeline:\n\n```bash\ncommand1 | command2 | command3\n```\n\nData flows through the pipeline, transformed at each step, with no intermediate files created.\n\n### Simple Pipe Examples\n\n```bash\n# Count files in a directory\n$ ls | wc -l\n\n# Sort and remove duplicates\n$ cat names.txt | sort | uniq\n\n# Find large files\n$ ls -l | grep \"\\.csv\"\n```\n\n### Building Complex Pipelines\n\nStart simple and build up:\n\n```bash\n# Step 1: Look at raw data\n$ cat data.csv | head\n\n# Step 2: Extract a column (field 2, comma-delimited)\n$ cat data.csv | cut -d',' -f2 | head\n\n# Step 3: Remove header, sort\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort\n\n# Step 4: Count unique values\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c\n\n# Step 5: Sort by frequency\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c | sort -rn\n```\n\n::: {.callout-tip title=\"Pipeline Development Strategy\"}\n1. Start with `head` to see sample data\n2. Add one command at a time\n3. Verify output at each step\n4. Build up to the complete pipeline\n5. Save complex pipelines in scripts\n:::\n\n### The `tee` Command\n\nSometimes you want to save intermediate results while continuing the pipeline. The `tee` command splits output:\n\n```bash\n# Save to file AND continue pipeline\n$ cat data.txt | sort | tee sorted.txt | uniq\n```\n\nThis saves the sorted data to `sorted.txt` while simultaneously passing it to `uniq`.\n\n## Essential Pipeline Tools\n\nSeveral commands are designed to work in pipelines:\n\n### `sort`: Order Lines\n\n```bash\n# Alphabetical sort\n$ sort names.txt\n\n# Numeric sort\n$ sort -n numbers.txt\n\n# Reverse sort\n$ sort -r data.txt\n\n# Sort by specific column (field 3, tab-delimited)\n$ sort -t$'\\t' -k3 data.tsv\n\n# Sort numerically by field 2\n$ sort -t',' -k2 -n data.csv\n```\n\n### `uniq`: Remove Duplicates\n\n```bash\n# Remove adjacent duplicates (requires sorted input!)\n$ sort names.txt | uniq\n\n# Count occurrences\n$ sort names.txt | uniq -c\n\n# Show only duplicated lines\n$ sort names.txt | uniq -d\n\n# Show only unique lines\n$ sort names.txt | uniq -u\n```\n\n### `cut`: Extract Columns\n\n```bash\n# Extract column 2 (tab-delimited default)\n$ cut -f2 data.tsv\n\n# Extract columns 1 and 3\n$ cut -f1,3 data.tsv\n\n# Comma-delimited\n$ cut -d',' -f2 data.csv\n\n# Character positions 1-10\n$ cut -c1-10 data.txt\n```\n\n### `tr`: Translate Characters\n\n```bash\n# Convert lowercase to uppercase\n$ echo \"hello\" | tr 'a-z' 'A-Z'\n\n# Delete characters\n$ echo \"hello123\" | tr -d '0-9'\n\n# Squeeze repeated characters\n$ echo \"hello    world\" | tr -s ' '\n\n# Replace characters\n$ cat data.csv | tr ',' '\\t'  # CSV to TSV\n```\n\n### `grep`: Search for Patterns\n\nWe'll cover `grep` extensively in @sec-grep-regex, but here are basic uses:\n\n```bash\n# Find lines containing \"error\"\n$ grep \"error\" log.txt\n\n# Case-insensitive search\n$ grep -i \"warning\" log.txt\n\n# Invert match (lines NOT containing pattern)\n$ grep -v \"^#\" data.txt  # Remove comment lines\n\n# Count matches\n$ grep -c \"pattern\" file.txt\n```\n\n## Working with Large Genomic Data\n\nLet's apply these tools to real bioinformatics scenarios. Genomic data files are often massive—the human genome reference is over 3 GB. You must process them efficiently.\n\n### FASTA Format Basics\n\nFASTA files store sequences with headers starting with `>`:\n\n```\n>sequence1 description\nATGCGATCGATCGATCGATCG\nATCGATCGATCGATCGATCGA\n>sequence2 another description\nGCTAGCTAGCTAGCTAGCTAG\n```\n\n### Downloading Genomic Data\n\n```bash\n# Download human genome from NCBI using wget\n$ wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\\\nGCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n\n# Or using curl\n$ curl -O https://example.com/genome.fa.gz\n```\n\n### Working with Compressed Files\n\nMost genomic data is compressed. Unix tools handle this seamlessly:\n\n```bash\n# View compressed file without extracting\n$ zcat genome.fa.gz | head\n\n# Search compressed file\n$ zgrep \">\" genome.fa.gz | wc -l\n\n# Decompress and recompress\n$ gunzip genome.fa.gz         # Creates genome.fa\n$ gzip genome.fa              # Creates genome.fa.gz\n$ gzip -k genome.fa           # Keep original file\n```\n\n### Analyzing Sequence Data\n\n```bash\n# Count sequences (headers start with >)\n$ grep -c \"^>\" sequences.fa\n\n# List all sequence headers\n$ grep \"^>\" sequences.fa\n\n# Calculate total sequence length (remove headers and newlines)\n$ grep -v \"^>\" genome.fa | tr -d '\\n' | wc -c\n\n# Count nucleotides\n$ grep -v \"^>\" genome.fa | tr -d '\\n' | fold -w1 | sort | uniq -c\n```\n\n### Example Pipeline: GC Content\n\nLet's calculate the GC content (percentage of G and C bases) of a genome:\n\n```bash\n# Extract sequence, convert to single line, count G and C\n$ grep -v \"^>\" genome.fa | \\\n  tr -d '\\n' | \\\n  tr -cd 'GCgc' | \\\n  wc -c\n```\n\nThis pipeline:\n\n1. `grep -v \"^>\"` — removes header lines\n2. `tr -d '\\n'` — removes newlines to get one long sequence\n3. `tr -cd 'GCgc'` — deletes everything except G and C\n4. `wc -c` — counts remaining characters\n\n::: {.callout-note}\nThe backslash `\\` at the end of a line continues the command on the next line, making complex pipelines more readable.\n:::\n\n### Finding Restriction Sites\n\n```bash\n# Count EcoRI sites (GAATTC)\n$ grep -v \"^>\" genome.fa | tr -d '\\n' | grep -o \"GAATTC\" | wc -l\n\n# Find all start codons (ATG) and their positions\n$ grep -v \"^>\" genome.fa | tr -d '\\n' | grep -b -o \"ATG\" | head -20\n```\n\n### Stream Processing: Download and Analyze\n\nYou can process data without saving intermediate files:\n\n```bash\n# Download, decompress, and analyze in one pipeline\n$ curl -s https://url/to/genome.fa.gz | \\\n  gunzip -c | \\\n  grep -v \"^>\" | \\\n  tr -d '\\n' | \\\n  wc -c\n```\n\nThis streams data through memory, never writing to disk—essential when working with files larger than your available storage.\n\n## Best Practices\n\n### Pipeline Efficiency\n\n- **Filter early**: Reduce data volume as early as possible in the pipeline\n- **Test incrementally**: Build and verify step by step\n- **Use appropriate tools**: `less` for viewing, `head` for sampling\n- **Save complex pipelines**: Put them in shell scripts\n\n### Defensive Practices\n\n```bash\n# Check file exists before processing\n$ test -f data.txt && grep \"pattern\" data.txt\n\n# Preview destructive operations\n$ ls *.tmp                    # See what will be deleted\n$ rm *.tmp                    # Then delete\n\n# Use tee to save intermediate results\n$ complex_pipeline | tee checkpoint.txt | further_processing\n```\n\n## Summary\n\nThis chapter covered the tools for reading, writing, and connecting Unix commands:\n\n- Standard streams (stdin, stdout, stderr) enable program communication\n- Output redirection (`>`, `>>`) saves results to files\n- Input redirection (`<`) feeds file contents to commands\n- Pipes (`|`) connect commands into powerful pipelines\n- Tools like `sort`, `uniq`, `cut`, `tr`, and `grep` transform data\n- Genomic data analysis leverages these tools at scale\n\nThese concepts form the foundation of Unix data processing. Master them, and you can construct solutions to virtually any text-processing challenge.\n\n## Exercises {.unnumbered}\n\n::: {.callout-tip title=\"Practice Exercises\"}\n**Exercise 1: Redirection Practice**\n\n1. Use `ls -la` to list your home directory and save the output to `home_contents.txt`\n2. Append the current date and time to the end of `home_contents.txt`\n3. Count how many lines are in the file\n4. Display only the last 5 lines\n\n**Exercise 2: Pipeline Construction**\n\nCreate a file `numbers.txt` with one number per line (include some duplicates):\n```bash\necho -e \"5\\n3\\n8\\n3\\n1\\n5\\n9\\n3\\n7\" > numbers.txt\n```\n\nUsing pipes:\n1. Sort the numbers numerically\n2. Remove duplicates\n3. Show only the top 3 numbers\n4. Save the result to `top_three.txt`\n\n**Exercise 3: Text Processing**\n\nGiven a CSV file with format `name,score,grade`, write a pipeline to:\n1. Extract just the scores (column 2)\n2. Sort them numerically in descending order\n3. Calculate how many scores there are\n4. Find the top 5 scores\n\n**Exercise 4: Genomic Data**\n\nDownload a small bacterial genome and:\n1. Count the number of sequences\n2. Find the total sequence length\n3. Count each nucleotide (A, T, G, C)\n4. Search for a specific restriction enzyme site\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}