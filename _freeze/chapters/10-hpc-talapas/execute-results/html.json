{
  "hash": "7476eb370415a1e2eb80a5ae8674ec64",
  "result": {
    "engine": "knitr",
    "markdown": "# High-Performance Computing with Talapas {#sec-hpc-talapas}\n\n\n\n::: {.callout-note title=\"Learning Objectives\"}\nAfter completing this chapter, you will be able to:\n\n- Explain what high-performance computing (HPC) is and when to use it\n- Connect to Talapas via SSH and Open OnDemand\n- Navigate the storage system and understand quotas\n- Write and submit batch jobs using SLURM\n- Monitor job status and troubleshoot issues\n- Use software modules and manage environments\n- Follow best practices for responsible cluster use\n:::\n\n## What is High-Performance Computing?\n\nWhen your laptop isn't enough—when analyses run for days, require hundreds of gigabytes of memory, or need specialized hardware like GPUs—you need **high-performance computing** (HPC).\n\nAn HPC cluster is a collection of interconnected computers (called **nodes**) that work together to solve computational problems. Instead of upgrading your personal computer indefinitely, you submit jobs to a shared resource with far more capability.\n\n### When to Use HPC\n\nConsider HPC when:\n\n- Analyses take hours or days on your laptop\n- Data is too large to fit in your computer's memory\n- You need GPUs for machine learning\n- You want to run many similar jobs in parallel\n- Your analysis would monopolize your personal computer\n\n### Talapas: UO's Computing Cluster\n\n**Talapas** is the University of Oregon's HPC cluster, named from a Chinook word for coyote (the educator and keeper of knowledge). It's managed by Research Advanced Computing Services (RACS).\n\nKey resources:\n\n- Over 14,520 CPU cores\n- 129 TB of total memory\n- 89 GPUs (including NVIDIA A100 and H100)\n- 3+ petabytes of storage\n\n## Talapas Architecture\n\nUnderstanding the cluster architecture helps you use it effectively.\n\n```{mermaid}\n%%| label: fig-talapas-architecture\n%%| fig-cap: \"Talapas cluster architecture showing login nodes, scheduler, and compute nodes\"\nflowchart TB\n    U[Your Computer] --> SSH([SSH])\n    SSH --> L1[Login Node 1]\n    SSH --> L2[Login Node 2]\n    SSH --> L3[Login Node 3]\n    SSH --> L4[Login Node 4]\n    L1 --> S[SLURM Scheduler]\n    L2 --> S\n    L3 --> S\n    L4 --> S\n    S --> C1[CPU Nodes]\n    S --> C2[GPU Nodes]\n    S --> C3[High-Memory Nodes]\n    C1 --> ST[(Shared Storage)]\n    C2 --> ST\n    C3 --> ST\n    \n    style U fill:#e3f2fd,stroke:#333,stroke-width:2px\n    style L1 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L2 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L3 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L4 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style S fill:#f9d71c,stroke:#333,stroke-width:2px\n    style C1 fill:#98fb98,stroke:#333,stroke-width:2px\n    style C2 fill:#ffa07a,stroke:#333,stroke-width:2px\n    style C3 fill:#dda0dd,stroke:#333,stroke-width:2px\n    style ST fill:#c8e6c9,stroke:#333,stroke-width:2px\n```\n\n### Login Nodes vs. Compute Nodes\n\n::: {.callout-warning title=\"Never Run Jobs on Login Nodes\"}\n**Login nodes** are where you:\n\n- Log in and manage files\n- Edit scripts\n- Submit jobs\n- Check job status\n\n**Do NOT** run computationally intensive work on login nodes! They're shared by everyone, and heavy processes will be terminated.\n\n**Compute nodes** are where your actual work runs, accessed through the job scheduler.\n:::\n\n### Node Types\n\n| Node Type | Cores | Memory | Purpose |\n|:----------|:------|:-------|:--------|\n| Standard CPU | 28-128 | 128-512 GB | General computation |\n| Large Memory | 56 | 1-4 TB | Memory-intensive work |\n| GPU | 28-48 | 256-512 GB | Machine learning, CUDA |\n\n: Talapas node types {#tbl-node-types}\n\n### Partitions (Queues)\n\nJobs are submitted to **partitions** with different characteristics:\n\n| Partition | Time Limit | Notes |\n|:----------|:-----------|:------|\n| `compute` | 24 hours | General purpose (default) |\n| `short` | 24 hours | Short jobs |\n| `long` | 14 days | Extended jobs |\n| `gpu` / `longgpu` | 24h / 14d | GPU-enabled nodes |\n| `fat` / `longfat` | 24h / 14d | High-memory nodes |\n| `preempt` | varies | Can use idle resources (may be interrupted) |\n\n: Talapas partitions {#tbl-partitions}\n\n## Getting Access\n\n### Step 1: Join a PIRG\n\nA **PIRG** (Principal Investigator Research Group) is your access group. Your PI must add you to their PIRG, or contact RACS to create a new one.\n\n### Step 2: Request Access\n\nVisit [racs.uoregon.edu/request-access](https://racs.uoregon.edu/request-access)\n\nYour credentials:\n\n- **Username**: Your Duck ID\n- **Password**: Your UO password\n\n## Connecting to Talapas\n\n### Option 1: SSH (Command Line)\n\nFrom your terminal:\n\n```bash\n# Connect to Talapas\n$ ssh yourduckid@login.talapas.uoregon.edu\n\n# Or connect to a specific login node\n$ ssh yourduckid@login1.talapas.uoregon.edu\n```\n\n**Off campus?** Connect to the [UO VPN](https://service.uoregon.edu/TDClient/2030/Portal/KB/ArticleDet?ID=101392) first.\n\n**Windows users**: Use PuTTY, MobaXterm, Windows Terminal, or VS Code with the Remote-SSH extension.\n\n### Option 2: Open OnDemand (Web Browser)\n\nFor a graphical interface, use Open OnDemand:\n\n[ondemand.talapas.uoregon.edu](https://ondemand.talapas.uoregon.edu)\n\nOpen OnDemand provides:\n\n- File browser for uploads/downloads\n- Interactive applications (RStudio, JupyterLab, MATLAB)\n- Job composer and monitoring\n- Terminal in browser\n\nThis is excellent for beginners and interactive data analysis.\n\n## Storage on Talapas\n\n```{mermaid}\n%%| label: fig-storage-structure\n%%| fig-cap: \"Talapas storage hierarchy\"\nflowchart LR\n    R[\"/\"] --> H[\"/home\"]\n    R --> P[\"/projects\"]\n    H --> HD[\"/home/duckid<br/>250 GB\"]\n    P --> PG[\"/projects/PIRG\"]\n    PG --> PGU[\"/projects/PIRG/duckid<br/>Your workspace\"]\n    PG --> PGS[\"/projects/PIRG/shared<br/>Shared with group\"]\n    \n    style HD fill:#87ceeb,stroke:#333,stroke-width:2px\n    style PGU fill:#98fb98,stroke:#333,stroke-width:2px\n    style PGS fill:#f9d71c,stroke:#333,stroke-width:2px\n```\n\n### Directory Structure\n\n| Location | Quota | Purpose |\n|:---------|:------|:--------|\n| `/home/duckid` | 250 GB | Config files, small scripts |\n| `/projects/PIRG/duckid` | Shared (default 2 TB) | Large data, analysis |\n| `/projects/PIRG/shared` | Shared | Files shared with group |\n| `/tmp` (on compute nodes) | Local to node | Temporary scratch space |\n\n: Talapas storage locations {#tbl-storage}\n\n### Check Your Storage\n\n```bash\n# Home directory usage\n$ df -h ~\n\n# Project directory usage\n$ du -sh /projects/myPIRG/myusername\n\n# Detailed breakdown\n$ du -h --max-depth=1 /projects/myPIRG/myusername\n```\n\n::: {.callout-important}\n**Data is NOT automatically backed up!** You are responsible for backing up important files. Consider using Git for code and external storage for irreplaceable data.\n:::\n\n## SLURM: The Job Scheduler\n\n**SLURM** (**S**imple **L**inux **U**tility for **R**esource **M**anagement) is the software that manages the cluster [@yoo2003slurm]. It:\n\n- Allocates resources fairly among users\n- Queues jobs when resources are busy\n- Tracks resource usage\n\n```{mermaid}\n%%| label: fig-slurm-workflow\n%%| fig-cap: \"SLURM job lifecycle from submission to completion\"\nflowchart LR\n    A[Submit Job] --> SB([sbatch])\n    SB --> B[Queue]\n    B --> W([Wait])\n    W --> C[Run on Compute Node]\n    C --> CO([Complete])\n    CO --> D[Output Files]\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n```\n\n## Writing SLURM Batch Scripts\n\nA batch script combines SLURM directives with your commands:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=my_analysis      # Job name (shows in queue)\n#SBATCH --partition=compute         # Partition (queue)\n#SBATCH --account=myPIRG            # Your PIRG name\n#SBATCH --output=output_%j.log      # Standard output (%j = job ID)\n#SBATCH --error=error_%j.log        # Standard error\n#SBATCH --time=04:00:00             # Time limit (HH:MM:SS)\n#SBATCH --nodes=1                   # Number of nodes\n#SBATCH --ntasks-per-node=1         # Tasks per node\n#SBATCH --cpus-per-task=4           # CPUs per task\n#SBATCH --mem=16G                   # Memory per node\n\n# Load required modules\nmodule load R/4.3.3\n\n# Change to working directory\ncd /projects/myPIRG/myusername/analysis\n\n# Run your program\nRscript my_analysis.R\n```\n\n### Understanding SLURM Directives\n\n| Directive | Description | Example |\n|:----------|:------------|:--------|\n| `--partition` | Queue to use | `compute`, `gpu`, `fat` |\n| `--account` | PIRG for accounting | `myPIRG` |\n| `--time` | Maximum runtime | `1-12:00:00` (1 day, 12 hrs) |\n| `--nodes` | Number of nodes | `1` |\n| `--cpus-per-task` | Cores per task | `8` |\n| `--mem` | Total memory per node | `32G` |\n| `--mem-per-cpu` | Memory per CPU | `4G` |\n| `--gres` | Generic resources | `gpu:1` |\n\n: SLURM directives reference {#tbl-slurm-directives}\n\n::: {.callout-tip}\n**Request only what you need!** Smaller resource requests often run sooner. Start conservative and increase if jobs fail.\n:::\n\n## Submitting and Managing Jobs\n\n### Submit a Job\n\n```bash\n# Submit batch job\n$ sbatch my_script.sh\nSubmitted batch job 12345678\n```\n\n### Check Job Status\n\n```bash\n# Your jobs\n$ squeue -u $USER\n\n# All jobs on a partition\n$ squeue -p compute\n\n# Detailed job info\n$ scontrol show job 12345678\n```\n\n### Job Status Codes\n\n| Code | Meaning |\n|:-----|:--------|\n| PD | Pending (waiting in queue) |\n| R | Running |\n| CG | Completing |\n| CD | Completed |\n| F | Failed |\n| CA | Cancelled |\n\n: SLURM job status codes {#tbl-status-codes}\n\n### Cancel a Job\n\n```bash\n# Cancel specific job\n$ scancel 12345678\n\n# Cancel all your jobs\n$ scancel -u $USER\n```\n\n### Check Resource Usage\n\nAfter a job completes:\n\n```bash\n$ sacct -j 12345678 --format=JobID,JobName,Elapsed,MaxRSS,State\n```\n\n## Interactive Jobs\n\nFor testing and development, use interactive jobs instead of batch:\n\n```bash\n# Start interactive session\n$ srun --pty --partition=compute --account=myPIRG \\\n       --time=2:00:00 --cpus-per-task=4 --mem=8G /bin/bash\n```\n\nYour prompt will change from `login1` to something like `n049`, indicating you're on a compute node.\n\nInteractive jobs are ideal for:\n\n- Testing workflows before batch submission\n- Debugging code\n- Quick data exploration\n- Running GUI applications via Open OnDemand\n\n## Software Modules\n\nTalapas uses the **Lmod** module system to manage software. This lets multiple versions coexist without conflicts.\n\n### Basic Module Commands\n\n```bash\n# List available modules\n$ module avail\n\n# Search for specific software\n$ module spider R\n$ module spider python\n\n# Load a module\n$ module load R/4.3.3\n$ module load miniconda3/20240410\n\n# See loaded modules\n$ module list\n\n# Unload a module\n$ module unload R\n\n# Reset to default state\n$ module purge\n```\n\n### In Batch Scripts\n\n::: {.callout-important}\nAlways load modules **after** the `#SBATCH` directives in your batch scripts:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=analysis\n#SBATCH --partition=compute\n#SBATCH --account=myPIRG\n# ... other directives ...\n\n# Load modules here\nmodule load R/4.3.3\n\n# Now run your code\nRscript analysis.R\n```\n:::\n\n## GPU Jobs\n\nFor machine learning and GPU-accelerated computing:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=gpu_job\n#SBATCH --partition=gpu             # GPU partition\n#SBATCH --account=myPIRG\n#SBATCH --time=1-00:00:00           # 1 day\n#SBATCH --nodes=1\n#SBATCH --gres=gpu:1                # Request 1 GPU\n#SBATCH --mem=32G\n\n# Load CUDA and your software\nmodule load cuda/12.4\nmodule load miniconda3/20240410\n\n# Activate your environment\nconda activate ml_env\n\n# Run GPU code\npython train_model.py\n```\n\nCheck available GPUs:\n\n```bash\n$ /packages/racs/bin/slurm-show-gpus\n```\n\n## A Complete Example\n\nLet's walk through a realistic bioinformatics workflow:\n\n### 1. Prepare Your Script\n\nCreate `genome_analysis.sh`:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=genome_analysis\n#SBATCH --partition=compute\n#SBATCH --account=myPIRG\n#SBATCH --output=logs/genome_%j.out\n#SBATCH --error=logs/genome_%j.err\n#SBATCH --time=08:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n\n# Exit on error\nset -e\n\n# Load modules\nmodule load bwa/0.7.17\nmodule load samtools/1.17\n\n# Set variables\nGENOME=\"/projects/myPIRG/shared/reference/human_GRCh38.fa\"\nREADS_R1=\"/projects/myPIRG/myuser/data/sample_R1.fastq.gz\"\nREADS_R2=\"/projects/myPIRG/myuser/data/sample_R2.fastq.gz\"\nOUTPUT=\"/projects/myPIRG/myuser/results\"\n\n# Create output directory\nmkdir -p $OUTPUT\n\n# Log start time\necho \"Starting analysis at $(date)\"\n\n# Run alignment\nbwa mem -t $SLURM_CPUS_PER_TASK $GENOME $READS_R1 $READS_R2 | \\\n    samtools sort -@ 4 -o $OUTPUT/aligned.bam\n\n# Index BAM file\nsamtools index $OUTPUT/aligned.bam\n\n# Log completion\necho \"Analysis completed at $(date)\"\n```\n\n### 2. Submit and Monitor\n\n```bash\n# Create logs directory\n$ mkdir -p logs\n\n# Submit job\n$ sbatch genome_analysis.sh\nSubmitted batch job 12345678\n\n# Monitor status\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n          12345678   compute genome_a   duckid   R       5:32      1 n042\n\n# Check output in real-time\n$ tail -f logs/genome_12345678.out\n```\n\n## Best Practices\n\n### Do\n\n- Request only the resources you need\n- Test with small datasets first\n- Use `--time` conservatively (but not too short)\n- Clean up old files regularly\n- Use `/projects` for large data\n- Document your workflows\n- Acknowledge RACS in publications\n\n### Don't\n\n- Run heavy computation on login nodes\n- Leave jobs running indefinitely\n- Store sensitive data without encryption\n- Ignore error messages\n- Request maximum resources \"just in case\"\n\n### Acknowledgment\n\nWhen publishing research using Talapas, include this acknowledgment:\n\n> \"The authors acknowledge Research Advanced Computing Services (RACS) at the University of Oregon for providing computing resources that have contributed to the research results reported within this publication.\"\n\n## Getting Help\n\n### Resources\n\n- **Documentation**: [uoracs.github.io/talapas2-knowledge-base](https://uoracs.github.io/talapas2-knowledge-base/)\n- **Service Desk**: [hpcrcf.atlassian.net/servicedesk](https://hpcrcf.atlassian.net/servicedesk/customer/portal/1)\n- **Email**: racs@uoregon.edu\n- **Office Hours**: Knight Library, Dream Lab 122E (Wednesdays 1-3pm)\n\n## Summary\n\nHigh-performance computing extends your analytical capabilities beyond personal computers:\n\n- Talapas provides thousands of CPUs, GPUs, and terabytes of storage\n- Connect via SSH or the Open OnDemand web interface\n- SLURM manages job scheduling and resource allocation\n- Batch scripts combine resource requests with your commands\n- Software modules manage different software versions\n- Responsible use ensures fair access for everyone\n\nStart simple—submit small test jobs, verify they work, then scale up. The HPC learning curve is real, but the capability it provides is essential for modern computational research.\n\n## Exercises {.unnumbered}\n\n::: {.callout-tip title=\"Practice Exercises\"}\n**Exercise 1: First Connection**\n\n1. Log into Talapas via SSH\n2. Check which login node you're on with `hostname`\n3. Explore your home directory and check your quota\n4. Find your project directory\n\n**Exercise 2: Module Practice**\n\n1. Search for available R versions\n2. Load R and verify with `R --version`\n3. Search for Python versions\n4. Create a script that loads both R and Python\n\n**Exercise 3: Interactive Job**\n\n1. Start an interactive session with 2 CPUs and 4GB memory\n2. Note how your prompt changes\n3. Run a simple R command\n4. Exit the interactive session\n\n**Exercise 4: Batch Job**\n\n1. Write a batch script that:\n   - Requests 1 hour, 2 CPUs, 8GB memory\n   - Loads R\n   - Runs a simple R script that generates a plot\n2. Submit the job\n3. Monitor its progress\n4. Examine the output files\n:::\n",
    "supporting": [
      "10-hpc-talapas_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}