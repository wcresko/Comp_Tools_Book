# Files, Pipes, and Redirection {#sec-files-pipes}

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(gt)
theme_set(theme_minimal())
```

::: {.callout-note title="Learning Objectives"}
After completing this chapter, you will be able to:

- Explain the Unix philosophy and how it shapes command-line tools
- Read files efficiently using `cat`, `less`, `head`, and `tail`
- Use input and output redirection to work with files
- Chain commands together using pipes
- Apply these tools to process genomic data files
:::

## The Unix Philosophy

Unix tools are designed around a simple but powerful philosophy [@kernighan1984unix]:

> Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.

This philosophy has profound implications for how you work:

1. **Small, focused tools**: Each command does one specific task excellently
2. **Composition**: Complex operations are built by combining simple tools
3. **Text as interface**: Programs communicate through text, making them compatible

Understanding this philosophy transforms how you approach computational problems. Instead of seeking one program that does everything, you learn to construct pipelines from simple components.

## Standard Streams: The Foundation

Every Unix program has three standard communication channels, called **streams**:

```
         ┌──────────────┐
stdin ───>│              │───> stdout
    (0)   │   Program    │      (1)
          │              │───> stderr
          └──────────────┘      (2)
```

**Standard Input (stdin)**
:   Where programs read input. File descriptor 0. Default: keyboard.

**Standard Output (stdout)**
:   Where programs write normal output. File descriptor 1. Default: terminal screen.

**Standard Error (stderr)**
:   Where programs write error messages. File descriptor 2. Default: terminal screen.

These streams can be redirected—connected to files or other programs—which is the key to building powerful pipelines.

## Reading Files

### Choosing the Right Tool

Different situations call for different tools:

| Command | Best For |
|:--------|:---------|
| `cat` | Small files, concatenating files |
| `less` | Large files, browsing, searching |
| `head` | First few lines of a file |
| `tail` | Last few lines, monitoring logs |
| `wc` | Counting lines, words, characters |

: File reading commands and their uses {#tbl-reading-commands}

### `cat`: Concatenate and Display

```bash
# Display a file
$ cat data.txt

# Display multiple files (concatenated)
$ cat header.txt data.txt footer.txt

# Number the output lines
$ cat -n data.txt

# Show non-printing characters (debugging whitespace)
$ cat -A data.txt
```

::: {.callout-warning}
Never use `cat` on large files! It will flood your terminal with text. For genomic data files, always use `head`, `less`, or process through pipes.
:::

### `less`: The Pager

For large files, `less` lets you navigate page by page:

```bash
$ less huge_file.txt
```

Key navigation commands in `less`:

| Key | Action |
|:----|:-------|
| Space, f | Forward one page |
| b | Back one page |
| g | Go to beginning |
| G | Go to end |
| /pattern | Search forward |
| ?pattern | Search backward |
| n | Next search match |
| N | Previous search match |
| q | Quit |

: Navigation keys in `less` {#tbl-less-keys}

### `head` and `tail`: Peek at Extremes

```bash
# First 10 lines (default)
$ head data.csv

# First 20 lines
$ head -n 20 data.csv

# All but the last 5 lines
$ head -n -5 data.csv

# Last 10 lines (default)
$ tail data.csv

# Last 50 lines
$ tail -n 50 data.csv

# Everything after line 100
$ tail -n +100 data.csv
```

A special use of `tail` is monitoring files that are being written:

```bash
# Follow a file as it grows (Ctrl+C to stop)
$ tail -f analysis.log
```

This is invaluable for watching job progress on computing clusters.

## Output Redirection

Redirection connects a program's output to a file instead of the terminal.

### Overwrite: `>`

```bash
# Save command output to a file
$ ls -l > file_list.txt

# Save date to a file
$ date > timestamp.txt

# Redirect output from any command
$ echo "Hello, World!" > greeting.txt
```

::: {.callout-caution}
The `>` operator **overwrites** existing files without warning! Double-check your filenames before running commands with `>`.
:::

### Append: `>>`

```bash
# Add to an existing file
$ echo "New line" >> notes.txt

# Append today's entries to a log
$ date >> daily_log.txt
$ echo "Analysis completed" >> daily_log.txt
```

### Redirecting Errors: `2>`

Sometimes you want to separate normal output from error messages:

```bash
# Send errors to a file, output to screen
$ ls /nonexistent 2> errors.txt

# Save output and errors to different files
$ command > output.txt 2> errors.txt

# Discard errors (send to /dev/null)
$ command 2> /dev/null

# Combine stdout and stderr to one file
$ command &> all_output.txt
```

## Input Redirection

Input redirection feeds file contents to a program's stdin:

```bash
# Count lines in a file (two equivalent ways)
$ wc -l < data.txt
$ wc -l data.txt

# Sort input from a file
$ sort < names.txt
```

While many commands can read files directly as arguments, input redirection is essential for programs that only read from stdin.

## Pipes: Connecting Commands

**Pipes** (`|`) are the secret weapon of Unix. They connect the stdout of one command to the stdin of another, creating a processing pipeline:

```bash
command1 | command2 | command3
```

Data flows through the pipeline, transformed at each step, with no intermediate files created.

### Simple Pipe Examples

```bash
# Count files in a directory
$ ls | wc -l

# Sort and remove duplicates
$ cat names.txt | sort | uniq

# Find large files
$ ls -l | grep "\.csv"
```

### Building Complex Pipelines

Start simple and build up:

```bash
# Step 1: Look at raw data
$ cat data.csv | head

# Step 2: Extract a column (field 2, comma-delimited)
$ cat data.csv | cut -d',' -f2 | head

# Step 3: Remove header, sort
$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort

# Step 4: Count unique values
$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c

# Step 5: Sort by frequency
$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c | sort -rn
```

::: {.callout-tip title="Pipeline Development Strategy"}
1. Start with `head` to see sample data
2. Add one command at a time
3. Verify output at each step
4. Build up to the complete pipeline
5. Save complex pipelines in scripts
:::

### The `tee` Command

Sometimes you want to save intermediate results while continuing the pipeline. The `tee` command splits output:

```bash
# Save to file AND continue pipeline
$ cat data.txt | sort | tee sorted.txt | uniq
```

This saves the sorted data to `sorted.txt` while simultaneously passing it to `uniq`.

## Essential Pipeline Tools

Several commands are designed to work in pipelines:

### `sort`: Order Lines

```bash
# Alphabetical sort
$ sort names.txt

# Numeric sort
$ sort -n numbers.txt

# Reverse sort
$ sort -r data.txt

# Sort by specific column (field 3, tab-delimited)
$ sort -t$'\t' -k3 data.tsv

# Sort numerically by field 2
$ sort -t',' -k2 -n data.csv
```

### `uniq`: Remove Duplicates

```bash
# Remove adjacent duplicates (requires sorted input!)
$ sort names.txt | uniq

# Count occurrences
$ sort names.txt | uniq -c

# Show only duplicated lines
$ sort names.txt | uniq -d

# Show only unique lines
$ sort names.txt | uniq -u
```

### `cut`: Extract Columns

```bash
# Extract column 2 (tab-delimited default)
$ cut -f2 data.tsv

# Extract columns 1 and 3
$ cut -f1,3 data.tsv

# Comma-delimited
$ cut -d',' -f2 data.csv

# Character positions 1-10
$ cut -c1-10 data.txt
```

### `tr`: Translate Characters

```bash
# Convert lowercase to uppercase
$ echo "hello" | tr 'a-z' 'A-Z'

# Delete characters
$ echo "hello123" | tr -d '0-9'

# Squeeze repeated characters
$ echo "hello    world" | tr -s ' '

# Replace characters
$ cat data.csv | tr ',' '\t'  # CSV to TSV
```

### `grep`: Search for Patterns

We'll cover `grep` extensively in @sec-grep-regex, but here are basic uses:

```bash
# Find lines containing "error"
$ grep "error" log.txt

# Case-insensitive search
$ grep -i "warning" log.txt

# Invert match (lines NOT containing pattern)
$ grep -v "^#" data.txt  # Remove comment lines

# Count matches
$ grep -c "pattern" file.txt
```

## Working with Large Genomic Data

Let's apply these tools to real bioinformatics scenarios. Genomic data files are often massive—the human genome reference is over 3 GB. You must process them efficiently.

### FASTA Format Basics

FASTA files store sequences with headers starting with `>`:

```
>sequence1 description
ATGCGATCGATCGATCGATCG
ATCGATCGATCGATCGATCGA
>sequence2 another description
GCTAGCTAGCTAGCTAGCTAG
```

### Downloading Genomic Data

```bash
# Download human genome from NCBI using wget
$ wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\
GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz

# Or using curl
$ curl -O https://example.com/genome.fa.gz
```

### Working with Compressed Files

Most genomic data is compressed. Unix tools handle this seamlessly:

```bash
# View compressed file without extracting
$ zcat genome.fa.gz | head

# Search compressed file
$ zgrep ">" genome.fa.gz | wc -l

# Decompress and recompress
$ gunzip genome.fa.gz         # Creates genome.fa
$ gzip genome.fa              # Creates genome.fa.gz
$ gzip -k genome.fa           # Keep original file
```

### Analyzing Sequence Data

```bash
# Count sequences (headers start with >)
$ grep -c "^>" sequences.fa

# List all sequence headers
$ grep "^>" sequences.fa

# Calculate total sequence length (remove headers and newlines)
$ grep -v "^>" genome.fa | tr -d '\n' | wc -c

# Count nucleotides
$ grep -v "^>" genome.fa | tr -d '\n' | fold -w1 | sort | uniq -c
```

### Example Pipeline: GC Content

Let's calculate the GC content (percentage of G and C bases) of a genome:

```bash
# Extract sequence, convert to single line, count G and C
$ grep -v "^>" genome.fa | \
  tr -d '\n' | \
  tr -cd 'GCgc' | \
  wc -c
```

This pipeline:

1. `grep -v "^>"` — removes header lines
2. `tr -d '\n'` — removes newlines to get one long sequence
3. `tr -cd 'GCgc'` — deletes everything except G and C
4. `wc -c` — counts remaining characters

::: {.callout-note}
The backslash `\` at the end of a line continues the command on the next line, making complex pipelines more readable.
:::

### Finding Restriction Sites

```bash
# Count EcoRI sites (GAATTC)
$ grep -v "^>" genome.fa | tr -d '\n' | grep -o "GAATTC" | wc -l

# Find all start codons (ATG) and their positions
$ grep -v "^>" genome.fa | tr -d '\n' | grep -b -o "ATG" | head -20
```

### Stream Processing: Download and Analyze

You can process data without saving intermediate files:

```bash
# Download, decompress, and analyze in one pipeline
$ curl -s https://url/to/genome.fa.gz | \
  gunzip -c | \
  grep -v "^>" | \
  tr -d '\n' | \
  wc -c
```

This streams data through memory, never writing to disk—essential when working with files larger than your available storage.

## Best Practices

### Pipeline Efficiency

- **Filter early**: Reduce data volume as early as possible in the pipeline
- **Test incrementally**: Build and verify step by step
- **Use appropriate tools**: `less` for viewing, `head` for sampling
- **Save complex pipelines**: Put them in shell scripts

### Defensive Practices

```bash
# Check file exists before processing
$ test -f data.txt && grep "pattern" data.txt

# Preview destructive operations
$ ls *.tmp                    # See what will be deleted
$ rm *.tmp                    # Then delete

# Use tee to save intermediate results
$ complex_pipeline | tee checkpoint.txt | further_processing
```

## Summary

This chapter covered the tools for reading, writing, and connecting Unix commands:

- Standard streams (stdin, stdout, stderr) enable program communication
- Output redirection (`>`, `>>`) saves results to files
- Input redirection (`<`) feeds file contents to commands
- Pipes (`|`) connect commands into powerful pipelines
- Tools like `sort`, `uniq`, `cut`, `tr`, and `grep` transform data
- Genomic data analysis leverages these tools at scale

These concepts form the foundation of Unix data processing. Master them, and you can construct solutions to virtually any text-processing challenge.

## Exercises {.unnumbered}

::: {.callout-tip title="Practice Exercises"}
**Exercise 1: Redirection Practice**

1. Use `ls -la` to list your home directory and save the output to `home_contents.txt`
2. Append the current date and time to the end of `home_contents.txt`
3. Count how many lines are in the file
4. Display only the last 5 lines

**Exercise 2: Pipeline Construction**

Create a file `numbers.txt` with one number per line (include some duplicates):
```bash
echo -e "5\n3\n8\n3\n1\n5\n9\n3\n7" > numbers.txt
```

Using pipes:
1. Sort the numbers numerically
2. Remove duplicates
3. Show only the top 3 numbers
4. Save the result to `top_three.txt`

**Exercise 3: Text Processing**

Given a CSV file with format `name,score,grade`, write a pipeline to:
1. Extract just the scores (column 2)
2. Sort them numerically in descending order
3. Calculate how many scores there are
4. Find the top 5 scores

**Exercise 4: Genomic Data**

Download a small bacterial genome and:
1. Count the number of sequences
2. Find the total sequence length
3. Count each nucleotide (A, T, G, C)
4. Search for a specific restriction enzyme site
:::
