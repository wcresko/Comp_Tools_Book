# Tidy Data Principles {#sec-tidy-data}

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(gt)
theme_set(theme_minimal())
```

::: {.callout-note title="Learning Objectives"}
After completing this chapter, you will be able to:

- Define tidy data and explain its three principles
- Identify common problems in messy datasets
- Apply tidy data principles to your own data
- Distinguish between different data types
- Implement best practices for data organization
- Use appropriate file formats for data storage
- Use the tidyverse packages for data manipulation
- Apply dplyr verbs: filter, select, mutate, arrange, summarize
- Join multiple data frames using various join types
- Reshape data between wide and long formats with pivot functions
- Chain operations using the pipe operator
- Work with tibbles as modern data frames
- Understand when to use data.table for high-performance data wrangling
:::

## Why Data Organization Matters

Before you can analyze data, you must organize it. The structure of your data profoundly affects how easily you can work with it. A well-organized dataset can be analyzed in minutes; a poorly organized one might require hours of preprocessing.

Consider this scenario: you've completed an experiment and collected data in a spreadsheet. Now you need to analyze it. If your data is organized consistently and logically, analysis is straightforward. If it's scattered across multiple tabs with inconsistent formatting, merged cells, and color-coded values, you're in for a frustrating experience.

**Tidy data** provides a standard way to organize data that makes analysis easier. It's not the only valid way to structure data, but it's particularly well-suited for analysis in R and Python.

## The Three Principles of Tidy Data

Hadley Wickham formalized the concept of tidy data in a influential paper [@wickham2014tidy]. Tidy data follows three rules:

::: {.callout-important title="The Three Rules of Tidy Data"}
1. Each **variable** forms a column
2. Each **observation** forms a row
3. Each **value** has its own cell
:::

Let's unpack what these mean.

### Variables as Columns

A **variable** is a characteristic that you measure, observe, or categorize. Examples:

- Gene expression level
- Treatment group
- Patient age
- Sample ID
- Measurement date

Each variable should have its own column with a descriptive header.

### Observations as Rows

An **observation** is a single unit of data collection—typically one measurement from one subject at one time point. Each observation gets its own row.

### Values in Cells

Each cell contains exactly one value. No merged cells, no multiple values crammed together, no implicit values indicated by color or formatting.

## Tidy vs. Messy Data: Examples

### A Tidy Dataset

```{r}
#| label: tbl-tidy-example
#| tbl-cap: "Example of tidy data organization"

tidy_data <- tibble(
  sample_id = c("S001", "S002", "S003", "S001", "S002", "S003"),
  timepoint = c("0h", "0h", "0h", "24h", "24h", "24h"),
  gene = rep("BRCA1", 6),
  expression = c(4.2, 3.8, 4.5, 8.1, 7.9, 9.2)
)

tidy_data |>
  gt() |>
  tab_header(title = "Gene Expression Data (Tidy Format)")
```

This is tidy because:

- Each column is a variable (sample_id, timepoint, gene, expression)
- Each row is an observation (one expression measurement)
- Each cell has one value

### A Messy Dataset (Same Data)

A common messy format spreads measurements across columns:

```{r}
#| label: tbl-messy-example
#| tbl-cap: "The same data in messy (wide) format"

messy_data <- tibble(
  sample_id = c("S001", "S002", "S003"),
  `BRCA1_0h` = c(4.2, 3.8, 4.5),
  `BRCA1_24h` = c(8.1, 7.9, 9.2)
)

messy_data |>
  gt() |>
  tab_header(title = "Gene Expression Data (Messy Format)")
```

This is messy because:

- Time point and gene are embedded in column names
- Each row represents multiple observations
- Adding genes or timepoints requires adding columns

### Why It Matters

With tidy data, operations are straightforward:

```{r}
#| label: tidy-operations

# Calculate mean expression by timepoint
tidy_data |>
  group_by(timepoint) |>
  summarize(mean_expression = mean(expression))

# Easy to filter and plot
tidy_data |>
  filter(timepoint == "24h")
```

With messy data, you must first reshape it—or write complex code to work around the structure.

## Common Data Problems

### Problem 1: Column Headers Are Values

Often, column names contain data values rather than variable names:

```{r}
#| label: headers-are-values

# Messy: years in column headers
population_messy <- tibble(
  country = c("USA", "Canada", "Mexico"),
  `2020` = c(331, 38, 129),
  `2021` = c(332, 38, 130),
  `2022` = c(333, 39, 131)
)

population_messy |> gt()
```

Solution: "Pivot" the data to create a `year` column:

```{r}
#| label: pivot-longer

population_tidy <- population_messy |>
  pivot_longer(
    cols = `2020`:`2022`,
    names_to = "year",
    values_to = "population"
  )

population_tidy |> gt()
```

### Problem 2: Multiple Variables in One Column

Sometimes multiple pieces of information are crammed into one cell:

```{r}
#| label: multiple-vars

# Messy: rate contains both values
messy_rates <- tibble(
  country = c("USA", "Canada", "Mexico"),
  rate = c("1000/50000", "200/10000", "500/25000")
)

messy_rates |> gt()
```

Solution: Separate into distinct columns:

```{r}
#| label: separate-cols

tidy_rates <- messy_rates |>
  separate(rate, into = c("cases", "population"), sep = "/", convert = TRUE)

tidy_rates |> gt()
```

### Problem 3: Variables in Both Rows and Columns

Complex tables sometimes mix variables across dimensions:

```{r}
#| label: both-dimensions

# Messy: measurement type in rows, conditions in columns
messy_experiment <- tibble(
  measurement = c("weight", "height", "weight", "height"),
  subject = c("A", "A", "B", "B"),
  control = c(70, 170, 65, 165),
  treatment = c(68, 170, 64, 166)
)

messy_experiment |> gt()
```

This requires multiple steps to tidy: pivot longer, then pivot wider.

### Problem 4: Multiple Observational Units

When a single table contains data about different types of things:

```{r}
#| label: multiple-units

# Messy: patient and hospital info in same table
combined_data <- tibble(
  patient_id = c("P001", "P002"),
  patient_name = c("Alice", "Bob"),
  hospital_id = c("H1", "H1"),
  hospital_name = c("General", "General"),
  hospital_beds = c(500, 500),  # Repeated!
  diagnosis = c("Flu", "Cold")
)

combined_data |> gt()
```

Solution: Normalize into separate tables linked by keys.

## Best Practices for Data Organization

### Naming Conventions

::: {.callout-tip title="Data File Naming Rules"}
**File names:**

- Use descriptive names: `patient_outcomes_2024.csv`
- Include dates in ISO format: `YYYY-MM-DD`
- Avoid spaces (use underscores or hyphens)
- Keep names concise but informative

**Column names:**

- Use lowercase with underscores: `sample_id`, `gene_expression`
- Make names meaningful: `treatment_group` not `tg`
- Avoid special characters and spaces
- Start with a letter, not a number
:::

### File Formats

| Format | Extension | Best For |
|:-------|:----------|:---------|
| Comma-separated | `.csv` | General tabular data |
| Tab-separated | `.tsv` | Data containing commas |
| Plain text | `.txt` | Simple data, scripts |
| Excel | `.xlsx` | Sharing with non-programmers |
| RDS | `.rds` | R-specific data with types preserved |
| Parquet | `.parquet` | Large datasets, fast reading |

: Common data file formats {#tbl-file-formats}

For long-term storage and sharing, prefer plain text formats (CSV, TSV) over proprietary formats. They're human-readable and don't require specific software.

### Documentation

Always create a **data dictionary** (also called a codebook) documenting:

- Variable names and descriptions
- Units of measurement
- Allowable values or categories
- Coding of missing values
- Source of the data
- Date created/modified

### Preserving Raw Data

::: {.callout-warning title="Never Modify Raw Data"}
Always keep an unchanged copy of your original data. Create a separate processed version for analysis. This allows you to:

- Retrace your steps if something goes wrong
- Rerun analysis with different preprocessing
- Share the original data with collaborators
:::

## Data Types

Understanding data types helps you organize and analyze correctly.

### Categorical vs. Quantitative

```{r}
#| label: tbl-data-types-table
#| tbl-cap: "Classification of data types"

data_types <- tibble(
  Category = c("Categorical", "Categorical", "Quantitative", "Quantitative"),
  Type = c("Ordinal", "Nominal", "Ratio", "Interval"),
  Description = c(
    "Ordered categories",
    "Unordered categories",
    "True zero, ratios meaningful",
    "No true zero"
  ),
  Examples = c(
    "small, medium, large",
    "red, blue, green",
    "height, weight, concentration",
    "temperature (°C), calendar year"
  )
)

data_types |>
  gt() |>
  tab_header(title = "Data Type Classification")
```

### R Data Types

R has specific data types for these categories:

```{r}
#| label: r-types

# Numeric (continuous)
temperature <- c(37.2, 36.8, 38.1)
class(temperature)

# Integer
counts <- c(1L, 2L, 3L)
class(counts)

# Character
sample_names <- c("control", "treatment", "treatment")
class(sample_names)

# Factor (categorical)
treatment <- factor(c("low", "medium", "high"), 
                   levels = c("low", "medium", "high"),
                   ordered = TRUE)
class(treatment)
treatment

# Logical
passed_qc <- c(TRUE, TRUE, FALSE)
class(passed_qc)
```

### When to Use Factors

Factors are R's way of handling categorical variables. Use them when:

- You have a limited set of possible values
- The order matters (ordinal data)
- You want specific grouping in analyses and plots

```{r}
#| label: factor-example

# Character vector sorts alphabetically
treatments <- c("High", "Low", "Medium", "Low", "High")
sort(treatments)

# Factor maintains meaningful order
treatments_factor <- factor(treatments, 
                           levels = c("Low", "Medium", "High"))
sort(treatments_factor)
```

## Structuring Spreadsheets for Analysis

If you must use spreadsheets for data entry, follow these guidelines:

::: {.callout-important title="Spreadsheet Best Practices"}
1. **One header row only** — Put variable names in row 1
2. **No merged cells** — They break data structure
3. **No color-coding as data** — Use a column instead
4. **Consistent missing values** — Use `NA`, not blank, `-`, or `N/A`
5. **One sheet per dataset** — Don't spread data across tabs
6. **Export as CSV** — Preserve as plain text
7. **No calculations in raw data** — Keep raw and calculated data separate
:::

### Before and After

**Bad spreadsheet practices:**

- Merged cells for visual grouping
- Color-coded cells indicating categories
- Multiple header rows
- Notes in random cells
- Mixed data types in columns
- Embedded calculations

**Good spreadsheet practices:**

- Clean rectangular data
- Single header row
- Consistent formatting throughout
- Separate documentation sheet
- Explicit coding of all information

## The Tidyverse: A Collection of Data Science Packages

The **tidyverse** is a collection of R packages designed for data science that share a common philosophy and work seamlessly together. These packages make data manipulation, visualization, and analysis more intuitive and readable.

### Core Tidyverse Packages

| Package | Purpose |
|:--------|:--------|
| **dplyr** | Data manipulation (filter, select, mutate, etc.) |
| **ggplot2** | Data visualization |
| **tidyr** | Data tidying (pivot, separate, etc.) |
| **readr** | Fast data import |
| **purrr** | Functional programming |
| **stringr** | String manipulation |
| **forcats** | Working with factors |
| **tibble** | Modern data frames |

: Core tidyverse packages {#tbl-tidyverse}

Load all core packages at once:

```{r}
#| label: load-tidyverse
#| eval: false

library(tidyverse)
```

## The Pipe Operator

One of the most powerful features of the tidyverse is the **pipe operator**, which allows you to chain operations together in a readable way.

### The Native Pipe: `|>`

R 4.1+ includes a native pipe operator `|>`:

```{r}
#| label: native-pipe

# Without pipe: nested, hard to read
sqrt(sum(abs(c(-4, 9, -16))))

# With pipe: reads left to right
c(-4, 9, -16) |>
  abs() |>
  sum() |>
  sqrt()
```

### The magrittr Pipe: `%>%`

The tidyverse also provides `%>%` from the magrittr package, which has additional features:

```{r}
#| label: magrittr-pipe

library(dplyr)

# Both work similarly for most cases
c(1, 2, 3, 4, 5) |> mean()
c(1, 2, 3, 4, 5) %>% mean()
```

::: {.callout-tip}
The pipe transforms `x |> f(y)` into `f(x, y)`. Think of it as "take this, then do that." This makes code read like a recipe: take data, then filter, then select, then summarize.
:::

## Data Manipulation with dplyr

The **dplyr** package provides a consistent set of "verbs" for data manipulation. These functions are designed to be intuitive and compose well together.

### The Five Core dplyr Verbs

| Verb | Action |
|:-----|:-------|
| `filter()` | Pick rows based on conditions |
| `select()` | Pick columns by name |
| `arrange()` | Reorder rows |
| `mutate()` | Create or modify columns |
| `summarize()` | Reduce to summary statistics |

: The five core dplyr verbs {#tbl-dplyr-verbs}

Let's create sample data to demonstrate these:

```{r}
#| label: sample-data

# Sample experimental data
experiments <- tibble(
  sample_id = paste0("S", 1:12),
  treatment = rep(c("control", "drug_A", "drug_B"), each = 4),
  replicate = rep(1:4, 3),
  expression = c(10.2, 11.1, 9.8, 10.5,   # control
                 15.3, 14.8, 16.2, 15.0,   # drug_A
                 18.1, 19.2, 17.5, 18.8),  # drug_B
  cell_count = c(1000, 1050, 980, 1020,
                 1100, 1080, 1150, 1090,
                 1200, 1250, 1180, 1220)
)

experiments
```

### Filtering Rows with `filter()`

Select rows that meet specific conditions:

```{r}
#| label: filter-examples

# Single condition
experiments |>
  filter(treatment == "drug_A")

# Multiple conditions (AND)
experiments |>
  filter(treatment == "drug_A", expression > 15)

# OR conditions
experiments |>
  filter(treatment == "drug_A" | treatment == "drug_B")

# Using %in% for multiple values
experiments |>
  filter(treatment %in% c("drug_A", "drug_B"))
```

### Selecting Columns with `select()`

Choose which columns to keep:

```{r}
#| label: select-examples

# Select specific columns
experiments |>
  select(sample_id, treatment, expression)

# Select range of columns
experiments |>
  select(sample_id:replicate)

# Exclude columns with minus
experiments |>
  select(-cell_count)

# Select using helper functions
experiments |>
  select(starts_with("exp"))

experiments |>
  select(where(is.numeric))
```

### Arranging Rows with `arrange()`

Sort data by one or more columns:
```{r}
#| label: arrange-examples

# Ascending order (default)
experiments |>
  arrange(expression)

# Descending order
experiments |>
  arrange(desc(expression))

# Multiple columns
experiments |>
  arrange(treatment, desc(expression))
```

### Creating Columns with `mutate()`

Add new columns or modify existing ones:

```{r}
#| label: mutate-examples

experiments |>
  mutate(
    # Create new columns
    log_expression = log2(expression),
    expression_per_cell = expression / cell_count * 1000,
    # Modify existing
    treatment = toupper(treatment)
  )
```

### Replacing Columns with `transmute()`

If you only want to keep the new columns (discarding all others), use `transmute()`:

```{r}
#| label: transmute-examples

experiments |>
  transmute(
    sample_id,  # Keep this column
    log_expression = log2(expression),
    normalized = expression / cell_count * 1000
  )
```

This is useful when you're creating a derived dataset and don't need the original columns.

### Summarizing Data with `summarize()`

Collapse multiple rows into summary statistics:

```{r}
#| label: summarize-examples

experiments |>
  summarize(
    mean_expression = mean(expression),
    sd_expression = sd(expression),
    n_samples = n()
  )
```

### Grouping with `group_by()`

The real power of `summarize()` comes with `group_by()`:

```{r}
#| label: group-by-examples

# Summary statistics by treatment
experiments |>
  group_by(treatment) |>
  summarize(
    mean_expression = mean(expression),
    sd_expression = sd(expression),
    n = n()
  )

# Multiple grouping variables
experiments |>
  group_by(treatment) |>
  summarize(
    mean_expr = mean(expression),
    max_expr = max(expression),
    min_expr = min(expression)
  ) |>
  arrange(desc(mean_expr))
```

### Chaining Operations Together

The power of dplyr shines when chaining multiple operations:

```{r}
#| label: chaining-example

experiments |>
  filter(treatment != "control") |>           # Remove controls
  mutate(log_expr = log2(expression)) |>      # Log transform
  group_by(treatment) |>                       # Group by treatment
  summarize(
    mean_log_expr = mean(log_expr),
    se = sd(log_expr) / sqrt(n())
  ) |>
  arrange(desc(mean_log_expr))                 # Sort by mean
```

## Tibbles: Modern Data Frames

**Tibbles** are the tidyverse's enhanced version of data frames. They have several advantages:

### Creating Tibbles

```{r}
#| label: create-tibble

# Create a tibble
my_tibble <- tibble(
  x = 1:5,
  y = x^2,  # Can reference earlier columns
  z = c("a", "b", "c", "d", "e")
)

my_tibble

# Convert data frame to tibble
as_tibble(mtcars) |> head()
```

### Tibble Advantages

1. **Better printing**: Shows first 10 rows and fits columns to screen
2. **No partial matching**: `df$col` won't match `df$column`
3. **No string-to-factor conversion**: Characters stay as characters
4. **Preserves column types**: Subsetting always returns a tibble

```{r}
#| label: tibble-advantages

# Data frame subsetting can surprise you
df <- data.frame(x = 1:3, y = 4:6)
class(df[, 1])  # Returns vector!

# Tibble subsetting is consistent
tb <- tibble(x = 1:3, y = 4:6)
class(tb[, 1])  # Returns tibble
```

## Additional dplyr Functions

### Conditional Logic with `case_when()`

```{r}
#| label: case-when

experiments |>
  mutate(
    expression_level = case_when(
      expression < 12 ~ "low",
      expression < 17 ~ "medium",
      TRUE ~ "high"
    )
  )
```

### Counting with `count()`

```{r}
#| label: count-example

experiments |>
  count(treatment)

experiments |>
  count(treatment, sort = TRUE)
```

### Distinct Values with `distinct()`

```{r}
#| label: distinct-example

experiments |>
  distinct(treatment)
```

### Renaming with `rename()`

```{r}
#| label: rename-example

experiments |>
  rename(expr = expression, cells = cell_count)
```

### Subsetting Rows with `slice()`

While `filter()` selects rows based on conditions, `slice()` selects rows by position:

```{r}
#| label: slice-example

# First 3 rows
experiments |>
  slice(1:3)

# Last 2 rows
experiments |>
  slice_tail(n = 2)

# Random sample
set.seed(42)
experiments |>
  slice_sample(n = 3)

# Top 3 by expression
experiments |>
  slice_max(expression, n = 3)
```

### Extracting Columns with `pull()`

To extract a single column as a vector (rather than a data frame), use `pull()`:

```{r}
#| label: pull-example

# Returns a vector, not a data frame
experiments |>
  filter(treatment == "drug_A") |>
  pull(expression)

# Useful for passing to functions that expect vectors
experiments |>
  pull(expression) |>
  mean()
```

## Joining Data Frames

In real-world analysis, data often lives in multiple tables. **Joins** combine tables based on matching values in key columns.

### Types of Joins

| Join Type | Result |
|:----------|:-------|
| `inner_join()` | Keep only rows with matches in both tables |
| `left_join()` | Keep all rows from left table, add matching data from right |
| `right_join()` | Keep all rows from right table, add matching data from left |
| `full_join()` | Keep all rows from both tables |
| `semi_join()` | Keep rows from left table that have matches in right |
| `anti_join()` | Keep rows from left table that have NO matches in right |

: Types of joins in dplyr {#tbl-join-types}

### Join Example

Let's create two related tables:

```{r}
#| label: join-setup

# Sample information
samples <- tibble(
  sample_id = c("S1", "S2", "S3", "S4"),
  patient = c("Patient_A", "Patient_B", "Patient_A", "Patient_C"),
  timepoint = c("baseline", "baseline", "week4", "baseline")
)

# Expression measurements (note: S3 is missing, S5 is extra)
expression_data <- tibble(
  sample_id = c("S1", "S2", "S4", "S5"),
  gene_A = c(5.2, 3.1, 4.8, 6.0),
  gene_B = c(2.1, 1.8, 2.5, 3.2)
)

samples
expression_data
```

### Left Join (Most Common)

Keep all rows from the left table, add matching data from right:

```{r}
#| label: left-join-example

left_join(samples, expression_data, by = "sample_id")
```

Note that S3 has `NA` values for the expression columns (no match in right table), and S5 is not included (not in left table).

### Inner Join

Keep only rows with matches in both tables:

```{r}
#| label: inner-join-example

inner_join(samples, expression_data, by = "sample_id")
```

### Anti Join

Find rows in the left table with no match in the right:

```{r}
#| label: anti-join-example

# Which samples are missing expression data?
anti_join(samples, expression_data, by = "sample_id")
```

### Joining on Multiple Columns

When tables share multiple key columns, specify them all:

```{r}
#| label: multi-key-join
#| eval: false

# Join on multiple columns
left_join(df1, df2, by = c("patient_id", "visit_date"))

# When column names differ between tables
left_join(df1, df2, by = c("id" = "patient_id"))
```

::: {.callout-warning}
When joining, watch for columns with the same name but different meanings (e.g., "year" in both tables meaning different things). dplyr will automatically suffix them with `.x` and `.y`, but it's better to rename ambiguous columns before joining.
:::

## Reshaping Data with tidyr

We've already seen `pivot_longer()` and `separate()`. Let's complete the picture with their counterparts.

### Pivot Wider

Convert long data back to wide format with `pivot_wider()`:

```{r}
#| label: pivot-wider-setup

# Long format data
gene_expression_long <- tibble(
  sample = c("S1", "S1", "S2", "S2", "S3", "S3"),
  gene = c("BRCA1", "TP53", "BRCA1", "TP53", "BRCA1", "TP53"),
  expression = c(5.2, 3.1, 4.8, 2.9, 6.1, 3.5)
)

gene_expression_long
```

```{r}
#| label: pivot-wider-example

# Spread genes into columns
gene_expression_long |>
  pivot_wider(names_from = gene, values_from = expression)
```

### When to Use Wide vs. Long

- **Long format**: Better for analysis and plotting with ggplot2
- **Wide format**: Better for certain statistical tests, viewing many variables at once, or sharing with collaborators expecting a spreadsheet layout

You can easily convert between formats as needed:

```{r}
#| label: pivot-round-trip

# Round trip: long -> wide -> long
stocks <- tibble(
  date = as.Date('2024-01-01') + 0:2,
  AAPL = c(185, 187, 184),
  GOOGL = c(140, 142, 141),
  MSFT = c(375, 378, 376)
)

stocks |>
  pivot_longer(-date, names_to = "ticker", values_to = "price") |>
  pivot_wider(names_from = ticker, values_from = price)
```

### Unite: Combining Columns

The `unite()` function is the opposite of `separate()`:

```{r}
#| label: unite-example

# Data with separate date components
date_parts <- tibble(
  year = c(2024, 2024, 2024),
  month = c(1, 2, 3),
  day = c(15, 20, 25),
  value = c(100, 150, 200)
)

date_parts

# Combine into a single date column
date_parts |>
  unite(date, year, month, day, sep = "-")
```

You can convert the united column to a proper date type:

```{r}
#| label: unite-with-type

library(lubridate)

date_parts |>
  unite(date, year, month, day, sep = "-") |>
  mutate(date = ymd(date))
```

### Separate Rows

Sometimes cells contain multiple values. Use `separate_rows()` to split them into separate rows:

```{r}
#| label: separate-rows-example

# Common in survey data: multiple selections in one cell
survey <- tibble(
  respondent = c("Alice", "Bob"),
  favorite_foods = c("pizza, tacos, sushi", "burgers, fries")
)

survey

# Split into separate rows
survey |>
  separate_rows(favorite_foods, sep = ", ")
```

### Complete and Expand

When data has implicit missing values, use `complete()` to make them explicit:

```{r}
#| label: complete-example

# Data with missing combinations
observations <- tibble(
  site = c("A", "A", "B"),
  year = c(2020, 2021, 2020),
  count = c(10, 15, 8)
)

observations

# Make all combinations explicit
observations |>
  complete(site, year, fill = list(count = 0))
```

Use `crossing()` to generate all combinations of values:

```{r}
#| label: crossing-example

crossing(
  treatment = c("control", "drug_A", "drug_B"),
  timepoint = c(0, 24, 48),
  replicate = 1:3
)
```

## High-Performance Data Wrangling with data.table

While the tidyverse is excellent for learning and most data analysis tasks, the **data.table** package offers substantial performance advantages for large datasets. It's worth knowing about, especially if you work with big data.

### Why data.table?

data.table excels in several areas:

1. **Speed**: Often 10-100x faster than dplyr for large datasets
2. **Memory efficiency**: Modifies data in place rather than making copies
3. **Concise syntax**: Complex operations in a single expression
4. **No dependencies**: Minimal external package requirements

### Basic data.table Syntax

data.table uses a compact `DT[i, j, by]` syntax:

- **i**: Which rows? (like `filter`)
- **j**: What to do? (like `select`, `mutate`, `summarize`)
- **by**: Grouped by what? (like `group_by`)

```{r}
#| label: datatable-example
#| eval: false

library(data.table)

# Convert to data.table
dt <- as.data.table(experiments)

# Filter and summarize in one expression
dt[treatment == "drug_A",
   .(mean_expr = mean(expression)),
   by = treatment]

# Equivalent dplyr:
# experiments |>
#   filter(treatment == "drug_A") |>
#   group_by(treatment) |>
#   summarize(mean_expr = mean(expression))
```

### When to Use data.table

Consider data.table when:

- Your dataset has millions of rows
- You're doing repetitive operations on large data
- Memory is constrained
- Speed is critical (e.g., real-time analysis)

For learning and most everyday work, stick with dplyr—it's more readable and has excellent documentation.

### dtplyr: Best of Both Worlds

The **dtplyr** package lets you write dplyr code that runs on data.table:

```{r}
#| label: dtplyr-example
#| eval: false

library(dtplyr)

# Create a lazy data.table
lazy_dt <- lazy_dt(experiments)

# Use dplyr syntax, get data.table speed
lazy_dt |>
  filter(treatment != "control") |>
  group_by(treatment) |>
  summarize(mean_expr = mean(expression)) |>
  as_tibble()  # Convert back to tibble
```

This gives you dplyr's readable syntax with much of data.table's performance.

## Summary

Tidy data provides a consistent structure that simplifies analysis:

- Each variable is a column
- Each observation is a row
- Each value occupies one cell

Key principles for data organization:

- Use descriptive, consistent naming conventions
- Document your data with a data dictionary
- Preserve raw data separately from processed data
- Choose appropriate file formats for your needs
- Understand the difference between data types

The tidyverse provides powerful tools for working with tidy data:

- The **pipe operator** (`|>` or `%>%`) chains operations for readable code
- **dplyr verbs** provide intuitive data manipulation:
  - `filter()` selects rows by condition
  - `select()` picks columns by name
  - `mutate()` creates or modifies columns
  - `arrange()` sorts rows
  - `summarize()` computes summary statistics
  - `group_by()` enables grouped operations
  - `slice()` and `pull()` provide positional subsetting and column extraction
- **Joins** (`left_join()`, `inner_join()`, `anti_join()`, etc.) combine data from multiple tables
- **Tibbles** are enhanced data frames with consistent behavior
- Functions like `case_when()`, `count()`, and `distinct()` handle common tasks

The **tidyr** package provides tools for reshaping data:

- `pivot_longer()` converts wide data to long format
- `pivot_wider()` converts long data to wide format
- `separate()` splits one column into multiple columns
- `unite()` combines multiple columns into one
- `separate_rows()` splits cells with multiple values into rows
- `complete()` and `crossing()` handle missing combinations

For high-performance needs, **data.table** offers:

- Concise `DT[i, j, by]` syntax
- 10-100x speed improvements on large datasets
- Memory efficiency through modification by reference
- **dtplyr** as a bridge for those who prefer dplyr syntax

These practices and tools apply across data science workflows. Well-organized data combined with tidyverse tools makes analysis efficient, reproducible, and shareable.

## Exercises {.unnumbered}

::: {.callout-tip title="Practice Exercises"}
**Exercise 1: Identify Tidy Data**

For each dataset description, determine if it's tidy. If not, explain what violates tidy principles:

1. Patient data with columns: patient_id, age, blood_pressure_systolic, blood_pressure_diastolic
2. Gene expression with columns: gene_name, sample_1, sample_2, sample_3
3. Survey data with columns: respondent_id, question, response
4. Weather data with columns: city, 2023_temp, 2024_temp, 2023_precip, 2024_precip

**Exercise 2: Reshape Data**

Given this messy dataset:
```
Country   1990   2000   2010
USA       250    280    310  
Canada    27     31     35
Mexico    86     100    120
```

Write R code using `pivot_longer()` to convert it to tidy format.

**Exercise 3: Data Dictionary**

Create a data dictionary for an experiment tracking:
- Gene expression levels measured at 0, 12, and 24 hours
- Three treatment groups (control, low dose, high dose)
- Twenty biological replicates per group
- Quality control pass/fail flags

**Exercise 4: Spot the Problems**

Review a dataset you've worked with (or your lab's data). Identify any violations of tidy data principles and propose solutions.

**Exercise 5: dplyr Practice**

Using the built-in `mtcars` dataset:
1. Filter to only cars with 6 or 8 cylinders
2. Select only the columns `mpg`, `cyl`, `hp`, and `wt`
3. Create a new column `hp_per_cyl` (horsepower per cylinder)
4. Arrange by `mpg` in descending order

**Exercise 6: Grouped Summaries**

Using the `iris` dataset:
1. Calculate the mean and standard deviation of `Sepal.Length` for each `Species`
2. Find the maximum `Petal.Width` for each species
3. Count the number of observations per species
4. Chain all these operations using the pipe operator

**Exercise 7: Data Transformation Pipeline**

Create a complete analysis pipeline that:
1. Starts with the `mtcars` dataset
2. Filters to automatic transmission cars (`am == 0`)
3. Groups by number of cylinders
4. Calculates mean mpg, mean horsepower, and count per group
5. Creates a new column classifying fuel efficiency as "good" (mpg >= 20) or "poor"
6. Arranges by mean mpg descending

**Exercise 8: Tibble Exploration**

1. Convert `mtcars` to a tibble
2. Compare how the data frame and tibble display when printed
3. Test subsetting behavior: what class does `mtcars[, 1]` return vs `as_tibble(mtcars)[, 1]`?

**Exercise 9: Joining Tables**

Create two tibbles:
```r
patients <- tibble(
  patient_id = c("P001", "P002", "P003", "P004"),
  name = c("Alice", "Bob", "Carol", "David"),
  age = c(45, 52, 38, 61)
)

lab_results <- tibble(
  patient_id = c("P001", "P002", "P002", "P005"),
  test = c("glucose", "glucose", "cholesterol", "glucose"),
  result = c(95, 110, 185, 88)
)
```

1. Perform a left join to add lab results to patient information
2. Use an anti-join to find patients with no lab results
3. Use an inner join to keep only patients who have lab results
4. Explain why P005 appears or doesn't appear in each result

**Exercise 10: Reshaping Practice**

Given this long-format data:
```r
measurements <- tibble(
  sample = rep(c("A", "B", "C"), each = 2),
  metric = rep(c("weight", "height"), 3),
  value = c(70, 175, 65, 168, 80, 182)
)
```

1. Use `pivot_wider()` to create columns for weight and height
2. Calculate BMI (weight / (height/100)^2) on the wide data
3. Convert back to long format with `pivot_longer()`

**Exercise 11: Separate and Unite**

1. Given data with a combined column `"2024-01-15"`, use `separate()` to split it into year, month, and day columns
2. Use `unite()` to combine them back, but with slashes as separators (`"2024/01/15"`)
3. When might you prefer `separate_rows()` over `separate()`?
:::
