[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "",
    "text": "Preface\nWelcome to Foundational Computational Tools for Bioengineers, a comprehensive guide designed to equip graduate students with essential computational skills for modern research in biology, bioengineering, and related life sciences.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Why This Book?",
    "text": "Why This Book?\nModern scientific research is fundamentally computational. Whether you’re analyzing genomic sequences, processing microscopy images, modeling biological systems, or managing experimental data, computational literacy has become as essential as laboratory technique. Yet many graduate students enter their programs without formal training in these foundational skills.\nThis book bridges that gap by providing a practical, hands-on introduction to the core computational tools you’ll use throughout your scientific career.\n\n\n\n\n\n\nLearning Philosophy\n\n\n\nThis is a practical course, and we will learn by doing. Each chapter includes worked examples, exercises, and real-world applications drawn from bioengineering and life sciences research.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nBy the end of this book, you will be able to:\n\nUnderstand computer architecture and choose appropriate computing resources for your analyses\nNavigate file systems and automate tasks using Unix/Linux commands\nWrite shell scripts to create reproducible analysis pipelines\nProcess and analyze large biological datasets using command-line tools\nProgram in R for statistical analysis and data visualization\nApply tidy data principles for effective data management\nUse Git and GitHub for version control and collaboration\nSubmit and manage jobs on high-performance computing clusters",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Prerequisites",
    "text": "Prerequisites\nNo prior programming experience is required! However, you should have:\n\nAccess to a computer running Windows, macOS, or Linux\nWillingness to learn through practice and experimentation\nPatience with yourself as you develop new skills",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nEach chapter builds on previous material, so I recommend reading them in order, at least initially. Key features include:\n\n\nCallout boxes highlighting important concepts, tips, and warnings\n\nCode examples that you can copy and modify for your own work\n\nPractice exercises to reinforce your learning\n\nCross-references connecting related concepts across chapters\n\n\n\n\n\n\n\nHands-On Practice\n\n\n\nThe best way to learn computational skills is to type the commands yourself. Avoid copying and pasting when possible—the muscle memory of typing commands is part of the learning process.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Course Materials",
    "text": "Course Materials\nThis book accompanies a graduate-level course taught at the University of Oregon’s Phil and Penny Knight Campus for Accelerating Scientific Impact. Additional materials, including lecture slides and homework assignments, are available on the course website.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book draws on many excellent resources, including Software Carpentry workshops (Software Carpentry, 2024a, 2024b), the tidyverse documentation (Wickham et al., 2023), and the experiences of countless students who have taken this course and provided valuable feedback.\nSpecial thanks to the Research Advanced Computing Services (RACS) team at the University of Oregon for their support with Talapas computing resources, and to the Knight Campus community for fostering an environment of interdisciplinary learning.\n\nLast updated: ?var:date",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0). You are free to share and adapt this material for non-commercial purposes, provided you give appropriate credit and distribute your contributions under the same license.\n\n\n\n\nSoftware Carpentry (2024a). The unix shell.\n\n\nSoftware Carpentry (2024b). Version control with git.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction to Computational Tools",
    "section": "",
    "text": "1.1 The Computational Revolution in Biology\nModern biological research has undergone a profound transformation. The advent of high-throughput technologies—from next-generation sequencing to automated microscopy to mass spectrometry—has generated an explosion of data that far exceeds what can be analyzed manually. Consider these examples:\nTo make sense of this data deluge, researchers must be fluent in computational tools. This fluency goes beyond simply running pre-made software—it requires understanding how to manipulate data, automate analyses, and ensure reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#the-computational-revolution-in-biology",
    "href": "chapters/01-introduction.html#the-computational-revolution-in-biology",
    "title": "1  Introduction to Computational Tools",
    "section": "",
    "text": "A single human genome sequence contains approximately 6 billion base pairs\n\nA typical RNA-seq experiment generates tens of millions of short reads\nProteomics studies can identify and quantify thousands of proteins simultaneously\nHigh-content imaging screens produce terabytes of image data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-learn-computational-skills",
    "href": "chapters/01-introduction.html#why-learn-computational-skills",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.2 Why Learn Computational Skills?",
    "text": "1.2 Why Learn Computational Skills?\nThere are compelling reasons to invest time in developing computational proficiency:\nSpeed and Scale. Computational tools allow you to perform thousands of operations with single commands. Tasks that would take days by hand can be completed in seconds.\nReproducibility. Scripts and code serve as a complete record of your analysis. Unlike clicking through graphical interfaces, command-line workflows can be exactly reproduced, shared, and verified (Sandve et al., 2013).\nFlexibility. General-purpose programming languages can handle virtually any data format or analysis type. You’re not limited to what software developers anticipated.\nAccess to Specialized Tools. Many cutting-edge analysis tools in bioinformatics are only available through the command line. Learning to use the shell opens doors to thousands of free programs developed by scientists for scientists.\nHigh-Performance Computing. Large-scale analyses require computing clusters. These systems are accessed and controlled through command-line interfaces.\n\n\n\n\n\n\nCareer Impact\n\n\n\nComputational skills are increasingly valued in academic and industry positions. Investing in these skills now will pay dividends throughout your career.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#coding-vs.-scripting-whats-the-difference",
    "href": "chapters/01-introduction.html#coding-vs.-scripting-whats-the-difference",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.3 Coding vs. Scripting: What’s the Difference?",
    "text": "1.3 Coding vs. Scripting: What’s the Difference?\nYou’ll often hear the terms “coding” and “scripting” used interchangeably, but there are technical distinctions worth understanding.\nCompiled Languages (Coding)\nCompiled languages require a separate compilation step that translates human-readable source code into machine code before execution. Examples include:\n\nC/C++\nFortran\nRust\nGo\n\nThe compilation process produces optimized executable files that run very quickly. However, compiled languages typically require more setup and are less forgiving of errors during development.\nInterpreted Languages (Scripting)\nInterpreted languages (or scripting languages) execute code line-by-line at runtime, without a separate compilation step. Examples include:\n\nBash/Shell\nPython\nR\nJulia\nPerl\n\nInterpreted languages offer greater flexibility and faster development cycles at the cost of some execution speed. They’re ideal for data analysis, automation, and interactive exploration.\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between compiled and interpreted languages has become increasingly blurred. Modern systems like Julia use just-in-time (JIT) compilation to achieve near-compiled performance with scripting-language convenience. Python and R can also interface with compiled C/C++ code for performance-critical operations.\n\n\nWhich Should You Learn?\nFor most biological research, interpreted languages are the right choice. They offer:\n\nRapid prototyping and experimentation\nInteractive data exploration\nExtensive libraries for scientific computing\nLower barriers to entry for beginners\n\nIn this course, we focus primarily on Bash (the Unix shell) and R, with exposure to high-performance computing concepts that may eventually lead you to compiled languages if your research requires it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#tools-we-will-cover",
    "href": "chapters/01-introduction.html#tools-we-will-cover",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.4 Tools We Will Cover",
    "text": "1.4 Tools We Will Cover\nThis book and accompanying course introduce you to a carefully selected set of tools that form the foundation of computational research in the life sciences.\nUnix/Linux and the Shell\nThe Unix shell (specifically Bash) provides:\n\nA powerful interface for file manipulation\nTools for processing large text-based data files\nThe ability to combine simple programs into complex workflows\nAccess to remote computing systems\n\nUnderstanding Unix is essential because it underlies most scientific computing infrastructure, from laptops to supercomputers.\nR Statistical Programming\nR is a programming language and environment specifically designed for statistical computing and graphics (R Core Team, 2024). It offers:\n\nComprehensive statistical analysis capabilities\nPublication-quality graphics\nThousands of packages for specialized analyses\nIntegration with reproducible document systems (Quarto, R Markdown)\nAn excellent IDE (RStudio)\nQuarto and Reproducible Documents\nQuarto is an open-source scientific publishing system (Posit Software, PBC, 2024) that allows you to:\n\nCombine narrative text with executable code\nGenerate reports, presentations, websites, and books\nEnsure your analyses are fully reproducible\nShare your work in multiple formats\n\nThis book itself was written using Quarto!\nGit and GitHub\nGit is a version control system that tracks changes to your files over time (Chacon & Straub, 2014). GitHub is a web-based platform for hosting Git repositories. Together, they enable:\n\nComplete history of your project’s development\nSafe experimentation with new ideas\nCollaboration with colleagues worldwide\nSharing code and data publicly\nHigh-Performance Computing\nTalapas is the University of Oregon’s computing cluster. You’ll learn to:\n\nAccess and navigate the cluster\nSubmit and manage batch jobs\nUse software modules\nScale your analyses beyond your laptop’s capabilities",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#setting-up-your-computing-environment",
    "href": "chapters/01-introduction.html#setting-up-your-computing-environment",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.5 Setting Up Your Computing Environment",
    "text": "1.5 Setting Up Your Computing Environment\nBefore diving into the technical content, you’ll need to set up your computing environment. The required software depends on your operating system.\nAll Operating Systems\nThese tools work across Windows, macOS, and Linux:\n\n\nR — Download from r-project.org\n\n\nRStudio — Download from posit.co/download/rstudio-desktop\n\n\nVisual Studio Code — Download from code.visualstudio.com\n\n\nGit — Download from git-scm.com\n\nmacOS Users\nmacOS includes a Unix-based operating system. To access the shell:\n\nOpen Terminal (found in Applications → Utilities) or\nInstall iTerm2 for a more feature-rich terminal experience\nLinux Users\nLinux systems include multiple terminal applications. Open your distribution’s default terminal emulator, often called “Terminal” or “Console.”\nWindows Users\nWindows requires additional setup to access a Unix-like environment. The recommended approach uses Windows Subsystem for Linux (WSL):\n\n\n\n\n\n\nWindows Users: Additional Steps Required\n\n\n\nFollow these steps to install Ubuntu on Windows:\n\nRun Windows PowerShell as administrator\nType wsl --install and press Enter\nRestart your computer\nSearch for and install Ubuntu from the Microsoft Store, or type wsl --install -d ubuntu in PowerShell\nOpen Ubuntu and set up a username and password (doesn’t need to match your Windows login)\nRun sudo apt update followed by sudo apt upgrade to update packages\n\n\n\nA detailed guide is available at ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-10.\nTalapas Access\nTo use the university’s computing cluster:\n\nEnsure you’re a member of a PIRG (Principal Investigator Research Group)\nYour login credentials are your Duck ID and password\nOff-campus access requires the UO VPN\n\nWe’ll cover Talapas access in detail in Chapter 15.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#course-structure-and-expectations",
    "href": "chapters/01-introduction.html#course-structure-and-expectations",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.6 Course Structure and Expectations",
    "text": "1.6 Course Structure and Expectations\nThis book is designed to accompany a hands-on course where most of class time is devoted to coding practice rather than lectures. Expect to:\n\nFollow along with examples during class sessions\nComplete practice exercises to reinforce concepts\nSubmit homework assignments demonstrating your skills\nDevelop a final project applying what you’ve learned to your own research\n\n\n\n\n\n\n\nLearning Takes Practice\n\n\n\nComputational skills develop through repeated practice. Don’t be discouraged if concepts don’t click immediately—everyone struggles at first. The key is persistence and willingness to experiment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#getting-help",
    "href": "chapters/01-introduction.html#getting-help",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.7 Getting Help",
    "text": "1.7 Getting Help\nWhen you encounter problems (and you will!), several resources are available:\nManual Pages. Unix commands have built-in documentation accessible via man command_name. Type q to exit.\nHelp Functions. In R, use ?function_name or help(function_name) to access documentation.\nThe Internet. Sites like Stack Overflow contain solutions to virtually every common error message. Learning to search effectively is a crucial skill.\nGenerative AI. Tools like ChatGPT and Claude can explain concepts, debug code, and suggest solutions. However, always verify AI-generated code and understand what it does before using it.\nEach Other. Your classmates are experiencing the same learning curve. Collaboration and discussion accelerate everyone’s learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#summary",
    "href": "chapters/01-introduction.html#summary",
    "title": "1  Introduction to Computational Tools",
    "section": "\n1.8 Summary",
    "text": "1.8 Summary\nThis chapter introduced the rationale for learning computational tools and provided an overview of the skills you’ll develop. Key takeaways:\n\nComputational literacy is essential for modern biological research\nScripting languages (Bash, R, Python) are ideal for data analysis\nVersion control and reproducibility are professional best practices\nLearning requires patience, practice, and willingness to make mistakes\n\nIn the next chapter, we’ll explore computer systems architecture to build your mental model of how computers work—knowledge that will inform everything that follows.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#exercises",
    "href": "chapters/01-introduction.html#exercises",
    "title": "1  Introduction to Computational Tools",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\n\nEnvironment Check: Verify that you can open a terminal on your computer. What appears when you type echo \"Hello World\" and press Enter?\nSoftware Installation: Install R and RStudio if you haven’t already. Open RStudio and type 1 + 1 in the console. What happens?\nReflection: Write a paragraph describing one computational challenge in your current or planned research. What skills from this course might help address it?\n\n\n\n\n\n\n\nChacon, S., & Straub, B. (2014). Pro git.\n\n\nPosit Software, PBC (2024). Quarto: Open-source scientific and technical publishing system.\n\n\nR Core Team (2024). R: A language and environment for statistical computing.\n\n\nSandve, G. K. et al. (2013). Ten simple rules for reproducible computational research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html",
    "href": "chapters/02-computer-systems.html",
    "title": "2  Computer Systems Architecture",
    "section": "",
    "text": "2.1 What Is a Computer System?\nUnderstanding how computers work at a fundamental level will help you make better decisions about computational resources, write more efficient code, and troubleshoot problems effectively. This chapter provides the conceptual foundation for everything that follows.\nA computer system consists of four major components working together:\nHardware. The physical components you can touch—processors, memory chips, storage devices, and input/output peripherals.\nSoftware. Programs and instructions that tell the hardware what to do. This includes operating systems, applications, and your own scripts.\nFirmware. Low-level software embedded directly in hardware components, providing basic control functions.\nData. The information being processed, stored, and transmitted by the system.\ngraph TD\n    A[Input Devices] --&gt; B[CPU]\n    B --&gt; C[Memory]\n    C --&gt; B\n    B --&gt; D[Output Devices]\n    E[Storage] --&gt; C\n    C --&gt; E\n\n\n\n\nFigure 2.1: Basic computer system architecture showing the flow of data between components\nAt the heart of every computation is the fetch-decode-execute cycle, which the CPU performs billions of times per second.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-fetch-decode-execute",
    "href": "chapters/02-computer-systems.html#sec-fetch-decode-execute",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.2 The Fetch-Decode-Execute Cycle",
    "text": "2.2 The Fetch-Decode-Execute Cycle\nEvery instruction your programs execute follows the same fundamental pattern:\n\n\n\n\n\ngraph LR\n    A[Fetch Instruction] --&gt; B[Decode Instruction]\n    B --&gt; C[Execute Instruction]\n    C --&gt; D[Store Results]\n    D --&gt; A\n\n\n\n\nFigure 2.2: The fundamental cycle underlying all computer operations\n\n\n\n\n\n\nFetch: The CPU retrieves the next instruction from memory\n\nDecode: The CPU interprets what the instruction means\n\nExecute: The CPU performs the specified operation\n\nStore: Results are saved to memory or a register\n\nThis cycle repeats continuously, with modern processors executing billions of cycles per second. Understanding this cycle helps explain why some operations are fast (data already in registers) while others are slow (waiting for data from disk).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-cpu",
    "href": "chapters/02-computer-systems.html#sec-cpu",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.3 The Central Processing Unit (CPU)",
    "text": "2.3 The Central Processing Unit (CPU)\nThe CPU is often called the “brain” of the computer, though this metaphor oversimplifies its function. The CPU is really a very fast calculator that follows instructions precisely.\nCPU Components\nThe CPU contains several critical components:\nControl Unit (CU). Directs the operation of the processor, managing the fetch-decode-execute cycle.\nArithmetic Logic Unit (ALU). Performs mathematical calculations and logical comparisons.\nRegisters. Ultra-fast temporary storage locations (typically 64 bits each) for data being actively processed.\nCache. High-speed memory buffer that stores frequently accessed data to reduce trips to main memory.\nKey CPU Concepts\nSeveral metrics describe CPU performance:\nClock Speed. Measured in gigahertz (GHz), this indicates how many cycles the CPU can perform per second. A 3.5 GHz processor performs 3.5 billion cycles per second.\nCores. Modern CPUs contain multiple independent processing units (cores). An 8-core processor can theoretically perform 8 operations simultaneously.\nThreads. Virtual cores that allow better utilization of physical cores through simultaneous multithreading (SMT) or hyper-threading.\nInstruction Set. The low-level language the CPU understands. Common architectures include x86-64 (Intel/AMD) and ARM (Apple Silicon, most phones).\n\n\n\n\n\n\nNote\n\n\n\nMore cores and higher clock speeds don’t always mean faster performance for your specific task. Many analyses are limited by memory access speed or storage I/O rather than raw CPU power.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-gpus",
    "href": "chapters/02-computer-systems.html#sec-gpus",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.4 Graphics Processing Units (GPUs)",
    "text": "2.4 Graphics Processing Units (GPUs)\nWhile CPUs are designed for complex sequential tasks, GPUs excel at simple parallel operations. Originally designed for rendering graphics (where millions of pixels need similar calculations), GPUs have become essential for:\n\nMachine learning and deep neural networks\nScientific simulations\nCryptocurrency mining\nImage and video processing\n\nGPU Architecture\nGPUs differ fundamentally from CPUs:\nStreaming Multiprocessors (SMs). Processing clusters containing many smaller cores.\nCUDA Cores / Stream Processors. Thousands of simple processing units optimized for parallel computation.\nVideo Memory (VRAM). Dedicated high-bandwidth memory separate from system RAM.\nCPU vs. GPU: When to Use Each\n\n\nTable 2.1: Comparison of CPU and GPU characteristics\n\n\n\n\n\n\n\n\nCharacteristic\nCPU\nGPU\n\n\n\nDesign Focus\nSequential, complex tasks\nParallel, simple tasks\n\n\nCore Count\n4-64 powerful cores\nThousands of smaller cores\n\n\nMemory\nLarge cache, lower bandwidth\nSmaller cache, higher bandwidth\n\n\nBest For\nOperating systems, complex logic, serial tasks\nGraphics, ML/AI, simulations\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf your computation involves applying the same operation to millions of data points (like image processing or matrix multiplication), GPUs can provide dramatic speedups. If your computation requires complex branching logic and sequential dependencies, CPUs are more appropriate.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-memory-types",
    "href": "chapters/02-computer-systems.html#sec-memory-types",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.5 Types of Computer Memory",
    "text": "2.5 Types of Computer Memory\nUnderstanding the memory hierarchy helps explain performance characteristics of different operations.\nRandom Access Memory (RAM)\nRAM provides temporary working space for programs and data currently in use. Key characteristics:\nVolatile. Contents are lost when power is removed.\nRandom Access. Any memory location can be accessed with equal speed (unlike sequential access).\nTypes. - DRAM (Dynamic RAM): Main system memory, requires constant refresh - SRAM (Static RAM): Used in CPU caches, faster but more expensive\nModern systems typically have 8-64 GB of RAM, though scientific workstations and servers may have much more.\nPersistent Storage\nUnlike RAM, storage devices retain data without power:\nHard Disk Drives (HDDs)\n\nMechanical spinning platters with magnetic storage\nLarge capacities at low cost (many terabytes)\nSlower access (~10ms latency, 100-200 MB/s throughput)\nSusceptible to physical damage\n\nSolid State Drives (SSDs)\n\nNo moving parts (NAND flash memory)\nMuch faster access (~0.1ms latency, 500-7000 MB/s throughput)\nMore expensive per gigabyte than HDDs\nMore durable and energy-efficient\n\n\n\n\n\n\n\nImportant\n\n\n\nThe memory hierarchy creates significant performance differences. Data in CPU registers can be accessed in nanoseconds, data in RAM in tens of nanoseconds, but data on disk takes milliseconds—a difference of six orders of magnitude!\n\n\nThe Memory Hierarchy\nFigure 2.3 illustrates the tradeoffs between speed, capacity, and cost:\n\n\n\n\n\n\n\nFigure 2.3: The memory hierarchy: faster storage is smaller and more expensive",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-operating-systems",
    "href": "chapters/02-computer-systems.html#sec-operating-systems",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.6 Operating Systems",
    "text": "2.6 Operating Systems\nAn operating system (OS) is the software layer that manages hardware resources and provides services to applications. Think of it as a translator between your programs and the physical computer.\nCore Functions\nOperating systems handle:\nProcess Management. Creating, scheduling, and terminating programs. Managing how CPU time is shared among competing processes.\nMemory Management. Allocating RAM to programs, implementing virtual memory, protecting programs from interfering with each other’s memory.\nFile System Management. Organizing data on storage devices, managing directories and permissions, handling read/write operations.\nDevice Management. Providing standardized interfaces to diverse hardware through device drivers.\nSecurity. User authentication, access control, protection against malicious software.\nCommon Operating Systems\nFor scientific computing, you’ll encounter:\nLinux. Open-source Unix-like OS dominant in servers and clusters. Most bioinformatics tools assume Linux.\nmacOS. Apple’s Unix-based desktop OS. Provides native access to Unix tools.\nWindows. Microsoft’s desktop OS. Now supports Linux tools through WSL (see Section 1.5.4).\n\n\n\n\n\n\nThe Principle of Least Privilege\n\n\n\nA fundamental security concept: every program and user should operate using the minimum permissions necessary to accomplish their task. This limits the damage that can occur from mistakes or malicious actions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-computing-environments",
    "href": "chapters/02-computer-systems.html#sec-computing-environments",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.7 Computing Environments",
    "text": "2.7 Computing Environments\nResearch computing happens across a spectrum of environments, from your laptop to massive cloud data centers.\nLocal Computing\nLocal computing refers to resources physically present and directly controlled by you—your laptop, desktop, or lab workstation.\nAdvantages:\n\nComplete control over hardware and software\nData stays on your premises\nNo network dependency for computation\nPredictable performance\nOne-time purchase cost\n\nDisadvantages:\n\nLimited scale (constrained by hardware)\nYou’re responsible for maintenance and backups\nHigh upfront cost for powerful machines\nResources sit idle during off-hours\nCluster Computing\nA computing cluster is a collection of interconnected computers (nodes) working together as a single system. The University of Oregon’s Talapas is an example.\n\n\n\n\n\ngraph TB\n    M[Head/Login Node] --&gt; N1[Compute Node 1]\n    M --&gt; N2[Compute Node 2]\n    M --&gt; N3[Compute Node 3]\n    M --&gt; N4[Compute Node N]\n    N1 -.-&gt; S[(Shared Storage)]\n    N2 -.-&gt; S\n    N3 -.-&gt; S\n    N4 -.-&gt; S\n    U[Users] --&gt; M\n\n\n\n\nFigure 2.4: Simplified cluster architecture with head node, compute nodes, and shared storage\n\n\n\n\nKey components include:\nHead/Login Node. Where users log in and submit jobs. Not for running computations!\nCompute Nodes. Worker machines that actually run your analyses.\nResource Manager. Software (like SLURM) that schedules jobs and allocates resources fairly.\nShared Storage. Parallel file systems accessible from all nodes.\nAdvantages:\n\nMassive scalability (add nodes as needed)\nSpecialized hardware (GPUs, high-memory nodes)\nShared among many users and projects\nProfessional management and maintenance\n\nDisadvantages:\n\nRequires learning job scheduling systems\nJobs may wait in queue\nShared resources mean competition for access\nLess control over software environment\nCloud Computing\nCloud computing provides on-demand computing resources over the internet, with pay-as-you-go pricing. Major providers include Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure.\n\n\n\n\n\ngraph TB\n    R[Region] --&gt; AZ1[Availability Zone 1]\n    R --&gt; AZ2[Availability Zone 2]\n    AZ1 --&gt; C[Compute]\n    AZ1 --&gt; S[Storage]\n    AZ2 --&gt; C2[Compute]\n    AZ2 --&gt; S2[Storage]\n    U[Users] -.Internet.-&gt; R\n\n\n\n\nFigure 2.5: Cloud computing architecture with multiple availability zones\n\n\n\n\nAdvantages:\n\nElastic scaling (instantly add or remove resources)\nNo upfront hardware investment\nGlobal availability\nManaged services reduce operational burden\nBuilt-in redundancy and disaster recovery\n\nDisadvantages:\n\nOngoing costs can exceed on-premises solutions\nVendor lock-in with proprietary services\nRequires reliable internet connectivity\nData transfer costs (especially egress fees)\nLess control compared to local systems\nChoosing the Right Environment\n\n\nTable 2.2: Comparison of computing environments\n\n\n\n\n\n\n\n\n\nFactor\nLocal\nCluster\nCloud\n\n\n\nScale\nSmall\nLarge\nUnlimited\n\n\nControl\nComplete\nModerate\nLimited\n\n\nCost Model\nCapital expense\nShared allocation\nPay-per-use\n\n\nLearning Curve\nLow\nModerate\nModerate-High\n\n\nBest For\nDevelopment, small analyses\nLarge batch jobs\nBurst computing, web services\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA common workflow: develop and test locally, run production analyses on a cluster, use cloud resources for specialized needs or when cluster queues are full.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#sec-computing-evolution",
    "href": "chapters/02-computer-systems.html#sec-computing-evolution",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.8 The Evolution of Scientific Computing",
    "text": "2.8 The Evolution of Scientific Computing\nComputing infrastructure has evolved dramatically over the past several decades:\n\n\n\n\n\ngraph LR\n    A[1960s-70s&lt;br/&gt;Mainframes] --&gt; B[1980s-90s&lt;br/&gt;Personal Computers]\n    B --&gt; C[1990s-2000s&lt;br/&gt;Client-Server]\n    C --&gt; D[2000s&lt;br/&gt;Clusters/Grids]\n    D --&gt; E[2006+&lt;br/&gt;Cloud Computing]\n    E --&gt; F[2020s&lt;br/&gt;Edge/Hybrid]\n\n\n\n\nFigure 2.6: Evolution of computing paradigms from mainframes to modern hybrid approaches\n\n\n\n\nKey drivers of this evolution include:\n\n\nPerformance Needs: Ever-increasing computational demands from research\n\nEconomics: Pursuit of cost optimization and economies of scale\n\nConnectivity: Internet bandwidth improvements enabling remote computing\n\nVirtualization: Technologies that abstract hardware from software\n\nModern research computing increasingly uses hybrid approaches, combining local development with cluster and cloud resources based on the specific needs of each project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#summary",
    "href": "chapters/02-computer-systems.html#summary",
    "title": "2  Computer Systems Architecture",
    "section": "\n2.9 Summary",
    "text": "2.9 Summary\nThis chapter provided foundational knowledge about computer systems:\n\nComputers consist of hardware, software, firmware, and data working together\nThe CPU executes instructions through the fetch-decode-execute cycle\nGPUs excel at parallel tasks; CPUs excel at sequential, complex operations\nMemory hierarchy creates significant performance differences\nOperating systems manage resources and provide services to applications\nComputing environments range from local to cluster to cloud, each with tradeoffs\n\nUnderstanding these concepts will help you choose appropriate resources for your analyses, interpret performance characteristics, and communicate effectively with system administrators.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#additional-reading",
    "href": "chapters/02-computer-systems.html#additional-reading",
    "title": "2  Computer Systems Architecture",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo deepen your understanding of computer systems architecture:\n\n\n“Code: The Hidden Language of Computer Hardware and Software” by Charles Petzold — An accessible introduction to how computers work from first principles\n\n“Computer Organization and Design” by Patterson and Hennessy — The classic textbook on computer architecture\n\nPutting the “You” in CPU — A free, interactive online guide to how processors work\n\nHow Computers Really Work by Matthew Justice — A practical guide covering hardware, software, and everything in between\n\nAWS, GCP, and Azure Documentation — Each cloud provider offers extensive free training on cloud computing concepts\n\nFor GPU computing and parallel processing:\n\n\nNVIDIA CUDA Documentation — Comprehensive guides for GPU programming\n\nAn Even Easier Introduction to CUDA — NVIDIA’s beginner-friendly CUDA tutorial",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#exercises",
    "href": "chapters/02-computer-systems.html#exercises",
    "title": "2  Computer Systems Architecture",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\n\n\nSystem Exploration: On your computer, find out:\n\nHow many CPU cores does your system have?\nHow much RAM is installed?\nWhat type of storage (SSD or HDD) do you have?\nWhat operating system and version are you running?\n\n\nMemory Hierarchy: Explain why reading data from disk is so much slower than reading from RAM. What implications does this have for analyzing large datasets?\n\nComputing Environment Selection: For each scenario, which computing environment (local, cluster, or cloud) would you recommend and why?\n\nTesting a new analysis script on a small dataset\nRunning a genome assembly requiring 256 GB of RAM\nHosting a web application for your lab’s research tools\nTraining a deep learning model requiring multiple GPUs for two weeks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html",
    "href": "chapters/03-unix-fundamentals.html",
    "title": "3  Unix Fundamentals",
    "section": "",
    "text": "3.1 What Is Unix?\nUnix is a family of operating systems originally developed at Bell Labs in 1969 and publicly released in 1973 (Kernighan & Pike, 1984). What began as a project for a single computer has become the foundation of modern computing infrastructure.\nToday, Unix and its derivatives power:\nLinux is an open-source implementation of Unix principles that has become the dominant operating system for scientific computing. When we refer to “Unix commands” in this book, the same commands work on Linux, macOS, and Windows Subsystem for Linux (WSL).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-what-is-unix",
    "href": "chapters/03-unix-fundamentals.html#sec-what-is-unix",
    "title": "3  Unix Fundamentals",
    "section": "",
    "text": "Most web servers on the internet\nScientific computing clusters (including Talapas)\nmacOS (Apple’s desktop operating system)\nAndroid phones (via the Linux kernel)\nEmbedded systems in everything from routers to cars\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearning Unix is an investment that pays dividends throughout your career. The commands you learn today will work on systems you encounter decades from now—Unix’s longevity is remarkable in the fast-changing world of technology.\n\n\nThe Unix Philosophy\nThe tools we use in Unix embody a design philosophy that emerged from Bell Labs in the 1970s:\n\nDo One Thing And Do It Well\n\nEach Unix command is designed to perform a single task excellently. By combining simple, well-designed tools, you can build powerful and complex workflows. This “minimalist and modular” approach means:\n\nTools are easier to learn (each does one thing)\nTools are easier to combine (standardized input/output)\nComplex tasks become chains of simple operations\n\nYou’ll see this philosophy throughout Unix: cat just displays files, grep just searches for patterns, sort just sorts lines. The magic happens when you combine them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-shell-interface",
    "href": "chapters/03-unix-fundamentals.html#sec-shell-interface",
    "title": "3  Unix Fundamentals",
    "section": "\n3.2 The Shell: Your Interface to Unix",
    "text": "3.2 The Shell: Your Interface to Unix\nThe shell is a program that interprets your commands and communicates with the operating system. Think of it as a translator between human-readable instructions and the computer’s internal operations.\nWhile graphical user interfaces (GUIs) like Finder on Mac or File Explorer on Windows are intuitive for casual use, the shell offers:\n\n\nSpeed: Type commands faster than clicking through menus\n\nPower: Access to thousands of specialized tools\n\nAutomation: Script repetitive tasks\n\nRemote Access: Work on distant computers identically to local ones\n\nReproducibility: Document exactly what you did\n\nBash: The Default Shell\nBash (Bourne Again SHell) is the default shell on most Unix systems and is what you’ll use throughout this course. Other shells exist (zsh, fish, tcsh), but Bash is the most widely used and what you’ll encounter on computing clusters.\nAccessing the Shell\nHow you access the shell depends on your operating system:\n\nmacOS\n\nOpen the Terminal application (Applications → Utilities → Terminal) or use a third-party terminal like iTerm2.\n\nLinux\n\nOpen your distribution’s terminal application, often called “Terminal” or “Console.”\n\nWindows\n\nAfter installing WSL (see Section 1.5.4), search for “Ubuntu” or your installed Linux distribution in the Start menu.\n\n\nWhen you open the terminal, you’ll see a prompt indicating the system is ready for your command:\nusername@computer:~$\nThe prompt typically shows your username, the computer name, your current directory (~ means your home directory), and a $ symbol indicating you’re a regular user (a # would indicate administrator/root access).\nEssential Keyboard Shortcuts\nLearning these shortcuts will dramatically speed up your command-line work:\n\n\nTable 3.1: Essential shell keyboard shortcuts\n\n\n\nShortcut\nAction\n\n\n\nTab\nAuto-complete commands, file names, and paths\n\n\n\n↑ / ↓\n\nScroll through command history\n\n\nCtrl+A\nMove cursor to beginning of line\n\n\nCtrl+E\nMove cursor to end of line\n\n\nCtrl+U\nDelete everything before cursor\n\n\nCtrl+K\nDelete everything after cursor\n\n\nCtrl+W\nDelete word before cursor\n\n\nCtrl+R\nSearch through command history\n\n\nCtrl+C\nCancel current command\n\n\n\nCtrl+L or clear\n\nClear the terminal screen\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTab completion is your best friend! Start typing a file name and press Tab—the shell will complete it for you. If there are multiple matches, press Tab twice to see all options. This prevents typos and saves enormous amounts of typing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-shell-command-anatomy",
    "href": "chapters/03-unix-fundamentals.html#sec-shell-command-anatomy",
    "title": "3  Unix Fundamentals",
    "section": "\n3.3 Anatomy of a Shell Command",
    "text": "3.3 Anatomy of a Shell Command\nEvery shell command follows a consistent structure:\ncommand -options arguments\nLet’s break this down:\n\nCommand\n\nThe program or built-in function you want to run. This is required.\n\n\nOptions (also called flags)\n\nModify the command’s behavior. Usually preceded by - (single letter) or -- (full word). Optional.\n\nArguments\n\nWhat the command should operate on—typically file or directory names. Sometimes optional.\n\n\nFor example:\nls -l Documents\nHere, ls is the command (list directory contents), -l is an option (use long format with details), and Documents is the argument (which directory to list).\n\n\n\n\n\n\nNote\n\n\n\nOptions can often be combined. For example, ls -l -h -a can be written as ls -lha. The order of combined options usually doesn’t matter.\n\n\nGetting Help: The man Command\nEvery Unix command has a manual page accessible via the man command:\n$ man ls\nThis opens the manual for ls, showing all available options and usage examples. Navigation within man:\n\n\nSpace or f — Move forward one page\n\nb — Move back one page\n\n/pattern — Search for “pattern” (press n for next match)\n\nh — Show help for navigating the manual\n\nq — Quit and return to the shell\n\nFor a quicker summary, many commands support --help:\n$ ls --help\n\n\n\n\n\n\nTip\n\n\n\nWhen you encounter an unfamiliar command, man should be your first stop. The manuals are comprehensive and authoritative—they’re written by the people who created the commands.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-file-system",
    "href": "chapters/03-unix-fundamentals.html#sec-file-system",
    "title": "3  Unix Fundamentals",
    "section": "\n3.4 Understanding the File System",
    "text": "3.4 Understanding the File System\nUnix organizes everything as files within a hierarchical directory structure. Understanding this structure is essential for navigation.\nThe Directory Tree\nThe entire file system branches from a single root directory, represented by /:\n/\n├── Users/\n│   └── wcresko/\n│       ├── Documents/\n│       ├── Downloads/\n│       └── projects/\n├── Applications/\n├── Library/\n└── tmp/\nKey directories include:\n\n/\n\nThe root directory. Everything else is inside this.\n\n/home or /Users\n\nContains user home directories. Your files live here.\n\n~\n\nShorthand for your home directory (e.g., /home/wcresko).\n\n/tmp\n\nTemporary files that may be deleted on reboot.\n\nAbsolute vs. Relative Paths\nA path specifies the location of a file or directory. Paths come in two forms:\n\nAbsolute Paths\n\nStart from the root directory (/). They work from anywhere because they specify the complete location.\n/Users/wcresko/Documents/analysis.R\n\nRelative Paths\n\nStart from your current directory. Shorter but depend on where you are.\nDocuments/analysis.R\n\n\n\n\n\n\n\nFigure 3.1: Understanding file system paths: This diagram shows how an absolute path like /home/catchen/working is constructed by following the directory tree from root (/) through each level.\n\n\nSpecial directory references:\n\n\n. (single dot) — current directory\n\n.. (double dot) — parent directory (one level up)\n\n~ — your home directory\n\n\n\n\n\n\n\nTip\n\n\n\nUse Tab completion! Start typing a file or directory name and press Tab to autocomplete. This saves typing and prevents errors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-navigation-commands",
    "href": "chapters/03-unix-fundamentals.html#sec-navigation-commands",
    "title": "3  Unix Fundamentals",
    "section": "\n3.5 Essential Navigation Commands",
    "text": "3.5 Essential Navigation Commands\nWhere Am I? pwd\n\nThe pwd command (print working directory) shows your current location:\n$ pwd\n/Users/wcresko/projects/analysis\nAlways know where you are before running commands—especially destructive ones!\nWhat’s Here? ls\n\nThe ls command (list) shows directory contents:\n$ ls\ndata/  results/  analysis.R  README.md\nCommon options make ls more informative:\n\n\nTable 3.2: Common options for the ls command\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n-l\nLong format (permissions, size, date)\n\n\n-a\nShow hidden files (those starting with .)\n\n\n-h\nHuman-readable file sizes (KB, MB, GB)\n\n\n-t\nSort by modification time (newest first)\n\n\n-S\nSort by file size (largest first)\n\n\n-r\nReverse sort order\n\n\n-F\nAdd symbols to show file types (/ for directories, * for executables)\n\n\n\n\n\n\nExample with multiple options:\n$ ls -lah\ntotal 32K\ndrwxr-xr-x  4 wcresko staff 128B Nov 15 14:30 .\ndrwxr-xr-x 12 wcresko staff 384B Nov 14 09:15 ..\n-rw-r--r--  1 wcresko staff 2.1K Nov 15 14:30 analysis.R\ndrwxr-xr-x  3 wcresko staff  96B Nov 15 10:00 data\n-rw-r--r--  1 wcresko staff  512 Nov 14 09:15 README.md\ndrwxr-xr-x  2 wcresko staff  64B Nov 15 14:00 results\nThe long format shows: permissions, links, owner, group, size, modification date, and name.\nMoving Around: cd\n\nThe cd command (change directory) moves you to a different location:\n# Go to a specific directory\n$ cd Documents\n\n# Go up one directory\n$ cd ..\n\n# Go to home directory (three equivalent ways)\n$ cd ~\n$ cd $HOME\n$ cd\n\n# Go to the previous directory (like \"back\" in a browser)\n$ cd -\n\n# Go to an absolute path\n$ cd /Users/wcresko/projects\n\n\n\n\n\n\nCaution\n\n\n\nUnlike GUIs, cd gives no feedback when successful. If you don’t see an error message, the command worked! Use pwd to verify where you ended up.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-working-with-files",
    "href": "chapters/03-unix-fundamentals.html#sec-working-with-files",
    "title": "3  Unix Fundamentals",
    "section": "\n3.6 Working with Files and Directories",
    "text": "3.6 Working with Files and Directories\nNow that you can navigate, let’s learn to manipulate files and directories.\nCreating Directories: mkdir\n\n# Create a single directory\n$ mkdir analysis\n\n# Create nested directories (use -p for parents)\n$ mkdir -p project/data/raw\n\n# Create multiple directories at once\n$ mkdir results figures tables\nCreating Files: touch and nano\n\nThe touch command creates an empty file (or updates the timestamp of an existing file):\n$ touch notes.txt\nFor files with content, use a text editor. nano is a simple terminal-based editor:\n$ nano notes.txt\nIn nano: - Type your content normally - Ctrl+O saves the file (press Enter to confirm) - Ctrl+X exits nano\nCopying Files: cp\n\n# Copy a file\n$ cp original.txt backup.txt\n\n# Copy a file to a directory\n$ cp analysis.R scripts/\n\n# Copy a directory (requires -r for recursive)\n$ cp -r data/ data_backup/\n\n# Copy multiple files to a directory\n$ cp *.txt documents/\nMoving and Renaming: mv\n\nThe mv command both moves and renames files:\n# Rename a file\n$ mv oldname.txt newname.txt\n\n# Move a file to a directory\n$ mv analysis.R scripts/\n\n# Move and rename simultaneously\n$ mv data/raw.csv backup/raw_2024.csv\n\n# Move multiple files\n$ mv *.R scripts/\nDeleting Files: rm and rmdir\n\n\n\n\n\n\n\nDanger Zone\n\n\n\nThe shell has no trash can! Deleted files are gone forever. Always double-check before using rm, especially with wildcards.\n\n\n# Remove a file\n$ rm unnecessary.txt\n\n# Remove multiple files\n$ rm *.tmp\n\n# Remove a directory and all contents (DANGEROUS!)\n$ rm -r old_project/\n\n# Interactive mode (asks for confirmation)\n$ rm -i important.txt\n\n# Remove empty directories only\n$ rmdir empty_folder/\nThe -r (recursive) option with rm is powerful but dangerous. Consider using -i (interactive) to confirm each deletion when learning.\nViewing Files: cat, head, tail, less\n\n# Display entire file\n$ cat README.md\n\n# First 10 lines (or specify -n NUMBER)\n$ head data.csv\n$ head -n 20 data.csv\n\n# Last 10 lines\n$ tail data.csv\n$ tail -n 5 data.csv\n\n# Follow a file as it grows (great for logs)\n$ tail -f analysis.log\n\n# Page through a large file\n$ less huge_data.txt\nIn less: - Space or f — next page - b — previous page - /pattern — search forward - n — next search result - q — quit\nFile Information: wc\n\nThe wc (word count) command provides file statistics:\n$ wc document.txt\n  100   850  5420 document.txt\nThis shows: lines, words, and bytes. Useful options:\n$ wc -l data.csv   # Lines only\n$ wc -w essay.txt  # Words only\n$ wc -c file.bin   # Bytes only\nFinding Files: find\n\nThe find command searches for files and directories based on various criteria:\n# Find by name (case-insensitive with -iname)\n$ find . -iname \"*.csv\"\n\n# Find files modified in the last 7 days\n$ find ~/projects -mtime -7\n\n# Find files larger than 100MB\n$ find . -size +100M\n\n# Find and do something with results\n$ find . -name \"*.tmp\" -delete\nThe find command is powerful for locating files across complex directory structures. Common options:\n\n\nTable 3.3: Common find options\n\n\n\nOption\nDescription\n\n\n\n-name\nMatch filename (case-sensitive)\n\n\n-iname\nMatch filename (case-insensitive)\n\n\n-type f\nFind files only\n\n\n-type d\nFind directories only\n\n\n-size +10M\nLarger than 10 megabytes\n\n\n-mtime -7\nModified within 7 days",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-naming-conventions",
    "href": "chapters/03-unix-fundamentals.html#sec-naming-conventions",
    "title": "3  Unix Fundamentals",
    "section": "\n3.7 Naming Conventions",
    "text": "3.7 Naming Conventions\nGood naming practices prevent countless problems:\n\n\n\n\n\n\nFile Naming Rules\n\n\n\nDO: - Use lowercase letters, numbers, underscores (_), and hyphens (-) - Use meaningful, descriptive names - Include dates in ISO format if relevant: data_2024-01-15.csv - Use appropriate file extensions\nDON’T: - Use spaces (use _ or - instead) - Start names with - (confused with options) - Use special characters: * ? [ ] ' \" \\ / &lt; &gt; | - Use extremely long names\n\n\nExamples:\n\n\nBad\nGood\n\n\n\nmy data.csv\nmy_data.csv\n\n\n-results.txt\nresults.txt\n\n\ndata (copy).csv\ndata_copy.csv\n\n\nJoe's file.txt\njoes_file.txt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-unix-getting-help",
    "href": "chapters/03-unix-fundamentals.html#sec-unix-getting-help",
    "title": "3  Unix Fundamentals",
    "section": "\n3.8 Getting Help",
    "text": "3.8 Getting Help\nManual Pages\nEvery Unix command has a manual page accessible via man:\n$ man ls\nManual pages can be dense. Look for: - NAME: What the command does - SYNOPSIS: How to use it - DESCRIPTION: Detailed explanation - OPTIONS: Available flags - EXAMPLES: If you’re lucky!\nPress q to exit the manual.\nQuick Help\nMany commands support a --help option:\n$ ls --help\nSearching for Commands\nIf you know what you want to do but not the command:\n# Search man page descriptions\n$ man -k \"search term\"\n$ apropos \"search term\"\n\n# Find where a command is located\n$ which python",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-file-permissions",
    "href": "chapters/03-unix-fundamentals.html#sec-file-permissions",
    "title": "3  Unix Fundamentals",
    "section": "\n3.9 File Permissions and Ownership",
    "text": "3.9 File Permissions and Ownership\nUnix is a multi-user system, and every file has permissions controlling who can read, write, or execute it. Understanding permissions is essential for security and for running scripts.\nUnderstanding Permission Strings\nWhen you run ls -l, you see permission strings like:\n-rwxr-xr-x 1 wcresko staff 2048 Jan 15 10:30 analysis.sh\ndrwxr-xr-x 3 wcresko staff   96 Jan 15 09:00 data\nLet’s decode -rwxr-xr-x:\n\n\nPosition\nMeaning\n\n\n\n1st character\nType: - (file), d (directory), l (link)\n\n\nCharacters 2-4\n\nOwner permissions: rwx\n\n\n\nCharacters 5-7\n\nGroup permissions: r-x\n\n\n\nCharacters 8-10\n\nOthers permissions: r-x\n\n\n\n\nThe three permission types are:\n\n\nr (read): View file contents or list directory\n\nw (write): Modify file or add/remove files in directory\n\nx (execute): Run file as program or enter directory\n\n-: Permission denied for that operation\nChanging Permissions: chmod\n\nThe chmod command changes file permissions. There are two notations:\nSymbolic notation:\n# Add execute permission for owner\n$ chmod u+x script.sh\n\n# Remove write permission for group and others\n$ chmod go-w data.csv\n\n# Set exact permissions (owner: rwx, group: rx, others: rx)\n$ chmod u=rwx,g=rx,o=rx script.sh\nOctal notation (uses numbers 0-7):\n\n\nNumber\nPermission\n\n\n\n7\nrwx (read + write + execute)\n\n\n6\nrw- (read + write)\n\n\n5\nr-x (read + execute)\n\n\n4\nr– (read only)\n\n\n0\n— (no permissions)\n\n\n\n# Common permission sets\n$ chmod 755 script.sh    # rwxr-xr-x (executable script)\n$ chmod 644 data.csv     # rw-r--r-- (readable file)\n$ chmod 700 private/     # rwx------ (private directory)\nChanging Ownership: chown\n\nThe chown command changes file ownership:\n# Change owner\n$ chown alice myfile.txt\n\n# Change owner and group\n$ chown alice:researchers myfile.txt\n\n# Recursively change directory ownership\n$ chown -R alice:researchers project/\nSuperuser Access: sudo\n\nSome operations require administrator (root) privileges. The sudo command temporarily elevates your permissions:\n# Install software (requires admin rights)\n$ sudo apt install cowsay\n\n# Edit a system file\n$ sudo nano /etc/hosts\n\n\n\n\n\n\nWarning\n\n\n\nUse sudo carefully! With great power comes great responsibility. Commands run with sudo can modify or delete system files, potentially breaking your computer. Always double-check commands before running them with sudo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#sec-unix-summary",
    "href": "chapters/03-unix-fundamentals.html#sec-unix-summary",
    "title": "3  Unix Fundamentals",
    "section": "\n3.10 Summary",
    "text": "3.10 Summary\nThis chapter covered the fundamentals of Unix navigation and file manipulation:\n\nThe shell interprets commands and interfaces with the operating system\nCommands follow the pattern: command -options arguments\n\nThe file system is a hierarchical tree starting at /\n\nNavigation uses pwd, ls, and cd\n\nFile operations include cp, mv, rm, mkdir, and touch\n\nGood naming conventions prevent problems\nManual pages (man) provide detailed help\n\nThese commands form the foundation for everything else in Unix. Practice them until they become automatic!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#additional-reading",
    "href": "chapters/03-unix-fundamentals.html#additional-reading",
    "title": "3  Unix Fundamentals",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo further develop your Unix skills:\n\n\nThe Linux Command Line by William Shotts — A comprehensive, free book on Linux command-line fundamentals\n\nSoftware Carpentry: The Unix Shell — An excellent interactive tutorial designed for scientists\n\n“The Unix Programming Environment” by Kernighan and Pike — A classic text on Unix philosophy and tools\n\nExplain Shell — An interactive tool that breaks down complex shell commands into understandable parts\n\nLinux Pocket Guide by Daniel Barrett — A handy quick reference for essential commands\n\nFor macOS-specific Unix guidance:\n\n\nmacOS Terminal User Guide — Apple’s official documentation for the Terminal application",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#exercises",
    "href": "chapters/03-unix-fundamentals.html#exercises",
    "title": "3  Unix Fundamentals",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Navigation Practice\nWithout using a GUI file browser:\n\nOpen your terminal\nPrint your current directory\nNavigate to your home directory\nList all files including hidden ones\nNavigate to your Documents folder (or create it if needed)\nReturn to your home directory using the shortcut\n\nExercise 2: File Operations\n\nCreate a new directory called practice\n\nNavigate into it\nCreate an empty file called notes.txt\n\nCreate a subdirectory called backup\n\nCopy notes.txt to the backup directory\nRename the original notes.txt to my_notes.txt\n\nList the contents of both directories to verify your work\nClean up by removing the entire practice directory\n\nExercise 3: Exploration\nUse manual pages to answer:\n\nWhat does the ls -R option do?\nHow can you create a directory and all necessary parent directories in one command?\nWhat command shows disk usage, and what option makes the output human-readable?\n\n\n\n\n\n\n\nKernighan, B. W., & Pike, R. (1984). The UNIX programming environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html",
    "href": "chapters/04-files-pipes.html",
    "title": "4  Files, Pipes, and Redirection",
    "section": "",
    "text": "4.1 The Unix Philosophy\nUnix tools are designed around a simple but powerful philosophy (Kernighan & Pike, 1984):\nThis philosophy has profound implications for how you work:\nUnderstanding this philosophy transforms how you approach computational problems. Instead of seeking one program that does everything, you learn to construct pipelines from simple components.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-unix-philosophy",
    "href": "chapters/04-files-pipes.html#sec-unix-philosophy",
    "title": "4  Files, Pipes, and Redirection",
    "section": "",
    "text": "Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.\n\n\n\n\nSmall, focused tools: Each command does one specific task excellently\n\nComposition: Complex operations are built by combining simple tools\n\nText as interface: Programs communicate through text, making them compatible",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-standard-streams",
    "href": "chapters/04-files-pipes.html#sec-standard-streams",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.2 Standard Streams: The Foundation",
    "text": "4.2 Standard Streams: The Foundation\nEvery Unix program has three standard communication channels, called streams:\n         ┌──────────────┐\nstdin ───&gt;│              │───&gt; stdout\n    (0)   │   Program    │      (1)\n          │              │───&gt; stderr\n          └──────────────┘      (2)\nStandard Input (stdin). Where programs read input. File descriptor 0. Default: keyboard.\nStandard Output (stdout). Where programs write normal output. File descriptor 1. Default: terminal screen.\nStandard Error (stderr). Where programs write error messages. File descriptor 2. Default: terminal screen.\nThese streams can be redirected—connected to files or other programs—which is the key to building powerful pipelines.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-reading-files",
    "href": "chapters/04-files-pipes.html#sec-reading-files",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.3 Reading Files",
    "text": "4.3 Reading Files\nChoosing the Right Tool\nDifferent situations call for different tools:\n\n\nTable 4.1: File reading commands and their uses\n\n\n\nCommand\nBest For\n\n\n\ncat\nSmall files, concatenating files\n\n\nless\nLarge files, browsing, searching\n\n\nhead\nFirst few lines of a file\n\n\ntail\nLast few lines, monitoring logs\n\n\nwc\nCounting lines, words, characters\n\n\n\n\n\n\n\ncat: Concatenate and Display\n# Display a file\n$ cat data.txt\n\n# Display multiple files (concatenated)\n$ cat header.txt data.txt footer.txt\n\n# Number the output lines\n$ cat -n data.txt\n\n# Show non-printing characters (debugging whitespace)\n$ cat -A data.txt\n\n\n\n\n\n\nWarning\n\n\n\nNever use cat on large files! It will flood your terminal with text. For genomic data files, always use head, less, or process through pipes.\n\n\n\nless: The Pager\nFor large files, less lets you navigate page by page:\n$ less huge_file.txt\nKey navigation commands in less:\n\n\nTable 4.2: Navigation keys in less\n\n\n\nKey\nAction\n\n\n\nSpace, f\nForward one page\n\n\nb\nBack one page\n\n\ng\nGo to beginning\n\n\nG\nGo to end\n\n\n/pattern\nSearch forward\n\n\n?pattern\nSearch backward\n\n\nn\nNext search match\n\n\nN\nPrevious search match\n\n\nq\nQuit\n\n\n\n\n\n\n\nhead and tail: Peek at Extremes\n# First 10 lines (default)\n$ head data.csv\n\n# First 20 lines\n$ head -n 20 data.csv\n\n# All but the last 5 lines\n$ head -n -5 data.csv\n\n# Last 10 lines (default)\n$ tail data.csv\n\n# Last 50 lines\n$ tail -n 50 data.csv\n\n# Everything after line 100\n$ tail -n +100 data.csv\nA special use of tail is monitoring files that are being written:\n# Follow a file as it grows (Ctrl+C to stop)\n$ tail -f analysis.log\nThis is invaluable for watching job progress on computing clusters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-output-redirection",
    "href": "chapters/04-files-pipes.html#sec-output-redirection",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.4 Output Redirection",
    "text": "4.4 Output Redirection\nRedirection connects a program’s output to a file instead of the terminal.\nOverwrite: &gt;\n\n# Save command output to a file\n$ ls -l &gt; file_list.txt\n\n# Save date to a file\n$ date &gt; timestamp.txt\n\n# Redirect output from any command\n$ echo \"Hello, World!\" &gt; greeting.txt\n\n\n\n\n\n\nCaution\n\n\n\nThe &gt; operator overwrites existing files without warning! Double-check your filenames before running commands with &gt;.\n\n\nAppend: &gt;&gt;\n\n# Add to an existing file\n$ echo \"New line\" &gt;&gt; notes.txt\n\n# Append today's entries to a log\n$ date &gt;&gt; daily_log.txt\n$ echo \"Analysis completed\" &gt;&gt; daily_log.txt\nRedirecting Errors: 2&gt;\n\nSometimes you want to separate normal output from error messages:\n# Send errors to a file, output to screen\n$ ls /nonexistent 2&gt; errors.txt\n\n# Save output and errors to different files\n$ command &gt; output.txt 2&gt; errors.txt\n\n# Discard errors (send to /dev/null)\n$ command 2&gt; /dev/null\n\n# Combine stdout and stderr to one file\n$ command &&gt; all_output.txt",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-input-redirection",
    "href": "chapters/04-files-pipes.html#sec-input-redirection",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.5 Input Redirection",
    "text": "4.5 Input Redirection\nInput redirection feeds file contents to a program’s stdin:\n# Count lines in a file (two equivalent ways)\n$ wc -l &lt; data.txt\n$ wc -l data.txt\n\n# Sort input from a file\n$ sort &lt; names.txt\nWhile many commands can read files directly as arguments, input redirection is essential for programs that only read from stdin.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-pipes",
    "href": "chapters/04-files-pipes.html#sec-pipes",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.6 Pipes: Connecting Commands",
    "text": "4.6 Pipes: Connecting Commands\nPipes (|) are the secret weapon of Unix. They connect the stdout of one command to the stdin of another, creating a processing pipeline:\ncommand1 | command2 | command3\nData flows through the pipeline, transformed at each step, with no intermediate files created.\nSimple Pipe Examples\n# Count files in a directory\n$ ls | wc -l\n\n# Sort and remove duplicates\n$ cat names.txt | sort | uniq\n\n# Find large files\n$ ls -l | grep \"\\.csv\"\nBuilding Complex Pipelines\nStart simple and build up:\n# Step 1: Look at raw data\n$ cat data.csv | head\n\n# Step 2: Extract a column (field 2, comma-delimited)\n$ cat data.csv | cut -d',' -f2 | head\n\n# Step 3: Remove header, sort\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort\n\n# Step 4: Count unique values\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c\n\n# Step 5: Sort by frequency\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c | sort -rn\n\n\n\n\n\n\nPipeline Development Strategy\n\n\n\n\nStart with head to see sample data\nAdd one command at a time\nVerify output at each step\nBuild up to the complete pipeline\nSave complex pipelines in scripts\n\n\n\nThe tee Command\nSometimes you want to save intermediate results while continuing the pipeline. The tee command splits output:\n# Save to file AND continue pipeline\n$ cat data.txt | sort | tee sorted.txt | uniq\nThis saves the sorted data to sorted.txt while simultaneously passing it to uniq.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-pipeline-tools",
    "href": "chapters/04-files-pipes.html#sec-pipeline-tools",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.7 Essential Pipeline Tools",
    "text": "4.7 Essential Pipeline Tools\nSeveral commands are designed to work in pipelines:\n\nsort: Order Lines\n# Alphabetical sort\n$ sort names.txt\n\n# Numeric sort\n$ sort -n numbers.txt\n\n# Reverse sort\n$ sort -r data.txt\n\n# Sort by specific column (field 3, tab-delimited)\n$ sort -t$'\\t' -k3 data.tsv\n\n# Sort numerically by field 2\n$ sort -t',' -k2 -n data.csv\n\nuniq: Remove Duplicates\n# Remove adjacent duplicates (requires sorted input!)\n$ sort names.txt | uniq\n\n# Count occurrences\n$ sort names.txt | uniq -c\n\n# Show only duplicated lines\n$ sort names.txt | uniq -d\n\n# Show only unique lines\n$ sort names.txt | uniq -u\n\ncut: Extract Columns\n# Extract column 2 (tab-delimited default)\n$ cut -f2 data.tsv\n\n# Extract columns 1 and 3\n$ cut -f1,3 data.tsv\n\n# Comma-delimited\n$ cut -d',' -f2 data.csv\n\n# Character positions 1-10\n$ cut -c1-10 data.txt\n\ntr: Translate Characters\n# Convert lowercase to uppercase\n$ echo \"hello\" | tr 'a-z' 'A-Z'\n\n# Delete characters\n$ echo \"hello123\" | tr -d '0-9'\n\n# Squeeze repeated characters\n$ echo \"hello    world\" | tr -s ' '\n\n# Replace characters\n$ cat data.csv | tr ',' '\\t'  # CSV to TSV\n\ngrep: Search for Patterns\nWe’ll cover grep extensively in Chapter 5, but here are basic uses:\n# Find lines containing \"error\"\n$ grep \"error\" log.txt\n\n# Case-insensitive search\n$ grep -i \"warning\" log.txt\n\n# Invert match (lines NOT containing pattern)\n$ grep -v \"^#\" data.txt  # Remove comment lines\n\n# Count matches\n$ grep -c \"pattern\" file.txt",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-genomic-data",
    "href": "chapters/04-files-pipes.html#sec-genomic-data",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.8 Working with Large Genomic Data",
    "text": "4.8 Working with Large Genomic Data\nLet’s apply these tools to real bioinformatics scenarios. Genomic data files are often massive—the human genome reference is over 3 GB. You must process them efficiently.\nFASTA Format Basics\nFASTA files store sequences with headers starting with &gt;:\n&gt;sequence1 description\nATGCGATCGATCGATCGATCG\nATCGATCGATCGATCGATCGA\n&gt;sequence2 another description\nGCTAGCTAGCTAGCTAGCTAG\nDownloading Genomic Data\n# Download human genome from NCBI using wget\n$ wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\\\nGCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n\n# Or using curl\n$ curl -O https://example.com/genome.fa.gz\nWorking with Compressed Files\nMost genomic data is compressed. Unix tools handle this seamlessly:\n# View compressed file without extracting\n$ zcat genome.fa.gz | head\n\n# Search compressed file\n$ zgrep \"&gt;\" genome.fa.gz | wc -l\n\n# Decompress and recompress\n$ gunzip genome.fa.gz         # Creates genome.fa\n$ gzip genome.fa              # Creates genome.fa.gz\n$ gzip -k genome.fa           # Keep original file\nAnalyzing Sequence Data\n# Count sequences (headers start with &gt;)\n$ grep -c \"^&gt;\" sequences.fa\n\n# List all sequence headers\n$ grep \"^&gt;\" sequences.fa\n\n# Calculate total sequence length (remove headers and newlines)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | wc -c\n\n# Count nucleotides\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | fold -w1 | sort | uniq -c\nExample Pipeline: GC Content\nLet’s calculate the GC content (percentage of G and C bases) of a genome:\n# Extract sequence, convert to single line, count G and C\n$ grep -v \"^&gt;\" genome.fa | \\\n  tr -d '\\n' | \\\n  tr -cd 'GCgc' | \\\n  wc -c\nThis pipeline:\n\n\ngrep -v \"^&gt;\" — removes header lines\n\ntr -d '\\n' — removes newlines to get one long sequence\n\ntr -cd 'GCgc' — deletes everything except G and C\n\nwc -c — counts remaining characters\n\n\n\n\n\n\n\nNote\n\n\n\nThe backslash \\ at the end of a line continues the command on the next line, making complex pipelines more readable.\n\n\nFinding Restriction Sites\n# Count EcoRI sites (GAATTC)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -o \"GAATTC\" | wc -l\n\n# Find all start codons (ATG) and their positions\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -b -o \"ATG\" | head -20\nStream Processing: Download and Analyze\nYou can process data without saving intermediate files:\n# Download, decompress, and analyze in one pipeline\n$ curl -s https://url/to/genome.fa.gz | \\\n  gunzip -c | \\\n  grep -v \"^&gt;\" | \\\n  tr -d '\\n' | \\\n  wc -c\nThis streams data through memory, never writing to disk—essential when working with files larger than your available storage.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-pipes-best-practices",
    "href": "chapters/04-files-pipes.html#sec-pipes-best-practices",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.9 Best Practices",
    "text": "4.9 Best Practices\nPipeline Efficiency\n\n\nFilter early: Reduce data volume as early as possible in the pipeline\n\nTest incrementally: Build and verify step by step\n\nUse appropriate tools: less for viewing, head for sampling\n\nSave complex pipelines: Put them in shell scripts\nDefensive Practices\n# Check file exists before processing\n$ test -f data.txt && grep \"pattern\" data.txt\n\n# Preview destructive operations\n$ ls *.tmp                    # See what will be deleted\n$ rm *.tmp                    # Then delete\n\n# Use tee to save intermediate results\n$ complex_pipeline | tee checkpoint.txt | further_processing",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#sec-pipes-summary",
    "href": "chapters/04-files-pipes.html#sec-pipes-summary",
    "title": "4  Files, Pipes, and Redirection",
    "section": "\n4.10 Summary",
    "text": "4.10 Summary\nThis chapter covered the tools for reading, writing, and connecting Unix commands:\n\nStandard streams (stdin, stdout, stderr) enable program communication\nOutput redirection (&gt;, &gt;&gt;) saves results to files\nInput redirection (&lt;) feeds file contents to commands\nPipes (|) connect commands into powerful pipelines\nTools like sort, uniq, cut, tr, and grep transform data\nGenomic data analysis leverages these tools at scale\n\nThese concepts form the foundation of Unix data processing. Master them, and you can construct solutions to virtually any text-processing challenge.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#additional-reading",
    "href": "chapters/04-files-pipes.html#additional-reading",
    "title": "4  Files, Pipes, and Redirection",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo explore pipes, redirection, and text processing further:\n\n\nData Science at the Command Line by Jeroen Janssens — A comprehensive guide to using command-line tools for data analysis\n\nThe Art of Command Line — A curated collection of tips and techniques for command-line mastery\n\nAWK Programming Language — The GNU Awk manual for advanced text processing\n\nsed & awk by Dale Dougherty and Arnold Robbins — A comprehensive guide to these powerful text manipulation tools\n\nFor bioinformatics-specific applications:\n\n\nBioinformatics Data Skills by Vince Buffalo — An excellent resource covering Unix tools in a genomics context\n\nNCBI FTP Guide — How to download genomic data from NCBI",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#exercises",
    "href": "chapters/04-files-pipes.html#exercises",
    "title": "4  Files, Pipes, and Redirection",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Redirection Practice\n\nUse ls -la to list your home directory and save the output to home_contents.txt\n\nAppend the current date and time to the end of home_contents.txt\n\nCount how many lines are in the file\nDisplay only the last 5 lines\n\nExercise 2: Pipeline Construction\nCreate a file numbers.txt with one number per line (include some duplicates):\necho -e \"5\\n3\\n8\\n3\\n1\\n5\\n9\\n3\\n7\" &gt; numbers.txt\nUsing pipes: 1. Sort the numbers numerically 2. Remove duplicates 3. Show only the top 3 numbers 4. Save the result to top_three.txt\nExercise 3: Text Processing\nGiven a CSV file with format name,score,grade, write a pipeline to: 1. Extract just the scores (column 2) 2. Sort them numerically in descending order 3. Calculate how many scores there are 4. Find the top 5 scores\nExercise 4: Genomic Data\nDownload a small bacterial genome and: 1. Count the number of sequences 2. Find the total sequence length 3. Count each nucleotide (A, T, G, C) 4. Search for a specific restriction enzyme site\n\n\n\n\n\n\nKernighan, B. W., & Pike, R. (1984). The UNIX programming environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html",
    "href": "chapters/05-grep-regex.html",
    "title": "5  GREP and Regular Expressions",
    "section": "",
    "text": "5.1 Introduction to Pattern Matching\nWhile the tools in Chapter 4 can filter and transform data, they work on whole lines or fixed positions. Pattern matching lets you search based on the content of text—finding lines that contain specific sequences of characters.\nGREP (Global Regular Expression Print) is the Unix standard for pattern matching. Originally developed for the ed text editor in the 1970s, grep remains essential for bioinformatics, log analysis, and text processing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-basic-grep",
    "href": "chapters/05-grep-regex.html#sec-basic-grep",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.2 Basic GREP Usage",
    "text": "5.2 Basic GREP Usage\nAt its simplest, grep finds lines containing a specific string:\n# Find lines containing \"error\"\n$ grep \"error\" logfile.txt\n\n# Search multiple files\n$ grep \"mutation\" *.txt\n\n# Search recursively in directories\n$ grep -r \"BRCA1\" data/\nEssential GREP Options\n\n\nTable 5.1: Common grep options\n\n\n\nOption\nDescription\n\n\n\n-i\nCase-insensitive search\n\n\n-v\nInvert match (lines NOT containing pattern)\n\n\n-c\nCount matching lines\n\n\n-n\nShow line numbers\n\n\n-l\nList only filenames with matches\n\n\n-o\nPrint only the matched part\n\n\n-w\nMatch whole words only\n\n\n-A n\nShow n lines after match\n\n\n-B n\nShow n lines before match\n\n\n-C n\nShow n lines before and after match\n\n\n\n\n\n\nExamples\n# Case-insensitive search\n$ grep -i \"gene\" annotations.txt\n\n# Count matches (not lines, just the count)\n$ grep -c \"&gt;\" sequences.fasta\n\n# Show line numbers\n$ grep -n \"ERROR\" application.log\n\n# Find files containing pattern\n$ grep -l \"experimental\" *.md\n\n# Remove comment lines (inverted match)\n$ grep -v \"^#\" config.txt\n\n# Show context around matches\n$ grep -C 2 \"significant\" results.txt",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-regex-intro",
    "href": "chapters/05-grep-regex.html#sec-regex-intro",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.3 Introduction to Regular Expressions",
    "text": "5.3 Introduction to Regular Expressions\nSo far we’ve searched for literal text. Regular expressions (regex) let you describe patterns that match multiple strings.\nFor example, the pattern [ACGT]+ matches any sequence of DNA bases, while colou?r matches both “color” and “colour”.\nMetacharacters: Characters with Special Meaning\nRegular expressions use certain characters as pattern-building blocks:\n\n\nTable 5.2: Basic regular expression metacharacters\n\n\n\nCharacter\nMeaning\nExample\n\n\n\n.\nAny single character\n\na.c matches “abc”, “a1c”, “a-c”\n\n\n*\nZero or more of preceding\n\nab*c matches “ac”, “abc”, “abbc”\n\n\n^\nStart of line\n\n^Hello matches lines starting with “Hello”\n\n\n$\nEnd of line\n\nend$ matches lines ending with “end”\n\n\n[]\nCharacter class\n\n[aeiou] matches any vowel\n\n\n[^]\nNegated class\n\n[^0-9] matches non-digits\n\n\n\\\nEscape special characters\n\n\\. matches literal period\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe * in regular expressions is different from the * wildcard in the shell. In regex, * means “zero or more of the preceding character.” In shell globbing, * means “any characters.”\n\n\nAnchors: Position Matching\nAnchors don’t match characters—they match positions:\n# Lines starting with \"Chr\"\n$ grep \"^Chr\" genome.gff\n\n# Lines ending with \"protein\"\n$ grep \"protein$\" annotations.txt\n\n# Empty lines\n$ grep \"^$\" file.txt\n\n# Lines with exactly \"gene\"\n$ grep \"^gene$\" list.txt\nCharacter Classes\nCharacter classes match any single character from a set:\n# Match any vowel\n$ grep \"[aeiou]\" words.txt\n\n# Match any digit\n$ grep \"[0-9]\" data.txt\n\n# Match any uppercase letter\n$ grep \"[A-Z]\" text.txt\n\n# Match DNA bases\n$ grep \"[ACGT]\" sequence.txt\n\n# Match non-DNA characters (errors in sequence)\n$ grep \"[^ACGTN]\" sequence.txt\nPredefined character classes:\n\n\n[[:alpha:]] — letters\n\n[[:digit:]] — digits (same as [0-9])\n\n[[:alnum:]] — letters and digits\n\n[[:space:]] — whitespace\n\n[[:upper:]] — uppercase letters\n\n[[:lower:]] — lowercase letters\nQuantifiers\nQuantifiers specify how many times a pattern should repeat:\n# Match \"color\" or \"colour\" (zero or one 'u')\n$ grep \"colou\\?r\" text.txt\n\n# Three consecutive A's\n$ grep \"A\\{3\\}\" sequence.txt\n\n# Two to four T's\n$ grep \"T\\{2,4\\}\" sequence.txt\n\n# At least three G's\n$ grep \"G\\{3,\\}\" sequence.txt\n\n\n\n\n\n\nWarning\n\n\n\nIn basic grep, quantifiers like ?, +, and {} must be escaped with \\. Use grep -E or egrep to avoid escaping.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-extended-regex",
    "href": "chapters/05-grep-regex.html#sec-extended-regex",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.4 Extended Regular Expressions",
    "text": "5.4 Extended Regular Expressions\nExtended regular expressions (ERE) add more metacharacters and don’t require escaping common quantifiers. Use grep -E or egrep:\n\n\nTable 5.3: Extended regular expression metacharacters\n\n\n\nCharacter\nMeaning\n\n\n\n+\nOne or more of preceding\n\n\n?\nZero or one of preceding\n\n\n{n}\nExactly n of preceding\n\n\n{n,m}\nBetween n and m of preceding\n\n\n\\|\nAlternation (OR)\n\n\n()\nGrouping\n\n\n\n\n\n\nAlternation: Matching Multiple Patterns\n# Match \"start\" OR \"stop\" OR \"pause\"\n$ grep -E \"start|stop|pause\" commands.txt\n\n# Match any start codon\n$ grep -E \"ATG|GTG|TTG\" sequences.fa\n\n# Match file extensions\n$ grep -E \"\\.(txt|csv|tsv)$\" filelist.txt\nGrouping\nParentheses group patterns together:\n# Match \"gray\" or \"grey\"\n$ grep -E \"gr(a|e)y\" text.txt\n\n# Match \"ab\", \"abab\", \"ababab\", etc.\n$ grep -E \"(ab)+\" data.txt\n\n# Match repeated dinucleotide\n$ grep -E \"(AT){3,}\" sequence.txt\nThe Plus Quantifier\nUnlike * (zero or more), + requires at least one match:\n# One or more A's followed by one or more T's\n$ grep -E \"A+T+\" sequence.txt\n\n# One or more digits\n$ grep -E \"[0-9]+\" data.txt",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-regex-bioinformatics",
    "href": "chapters/05-grep-regex.html#sec-regex-bioinformatics",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.5 Practical Bioinformatics Examples",
    "text": "5.5 Practical Bioinformatics Examples\nWorking with FASTA Files\n# Count sequences (lines starting with &gt;)\n$ grep -c \"^&gt;\" sequences.fa\n\n# Extract all headers\n$ grep \"^&gt;\" sequences.fa\n\n# Find specific gene headers\n$ grep -i \"kinase\" sequences.fa\n\n# Find sequences by organism\n$ grep -E \"^&gt;.*Homo sapiens\" proteins.fa\nSearching for Biological Motifs\n# Find restriction enzyme sites\n$ grep -E \"GAATTC\" genome.fa         # EcoRI\n$ grep -E \"GGATCC\" genome.fa         # BamHI\n$ grep -E \"G[ACGT]ANTC\" genome.fa    # Degenerate site\n\n# Find potential ORFs (start followed by bases, then stop)\n$ grep -oE \"ATG([ACGT]{3})*?(TAA|TAG|TGA)\" cds.fa\n\n# Find poly-A tails\n$ grep -E \"A{10,}\" transcripts.fa\n\n# Find microsatellites (dinucleotide repeats)\n$ grep -E \"(AT){5,}\" genome.fa\n$ grep -E \"(CA){5,}\" genome.fa\nAnalyzing GFF/GTF Files\n# Find all genes\n$ grep -P \"\\tgene\\t\" annotations.gff\n\n# Find genes on chromosome 1\n$ grep \"^chr1\" genes.gff | grep -P \"\\tgene\\t\"\n\n# Extract gene IDs\n$ grep -oE \"gene_id \\\"[^\\\"]+\\\"\" annotations.gtf\n\n# Find protein-coding genes\n$ grep \"protein_coding\" annotations.gtf\nLog File Analysis\n# Find errors in logs\n$ grep -i \"error\" application.log\n\n# Find specific error codes\n$ grep -E \"error\\s*[0-9]{3}\" application.log\n\n# Extract timestamps of errors\n$ grep -E \"^[0-9]{4}-[0-9]{2}-[0-9]{2}.*error\" log.txt\n\n# Count errors per type\n$ grep -oE \"ERROR: [A-Za-z]+\" log.txt | sort | uniq -c",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-grep-pipes",
    "href": "chapters/05-grep-regex.html#sec-grep-pipes",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.6 Combining GREP with Pipes",
    "text": "5.6 Combining GREP with Pipes\nThe real power of grep emerges when combined with other tools:\n# Count unique patterns\n$ grep \"^&gt;\" sequences.fa | sort | uniq | wc -l\n\n# Extract sequence names without &gt; symbol\n$ grep \"^&gt;\" sequences.fa | cut -c2-\n\n# Find headers, then get organism names\n$ grep \"^&gt;\" proteins.fa | grep -oE \"\\[.*\\]\" | sort | uniq -c | sort -rn\n\n# Process FASTA: count bases per sequence\n$ grep -v \"^&gt;\" sequence.fa | while read line; do\n    echo \"$line\" | wc -c\n  done\nExtracting Matching Regions\nThe -o flag prints only the matched portion:\n# Extract all email addresses\n$ grep -oE \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\" contacts.txt\n\n# Extract all URLs\n$ grep -oE \"https?://[a-zA-Z0-9./?=_-]+\" webpage.html\n\n# Extract gene names from headers\n$ grep \"^&gt;\" genes.fa | grep -oE \"gene=[^;]+\"\nContext Searches\nWhen you need to see what surrounds a match:\n# Show 3 lines after each match (including the sequence)\n$ grep -A 3 \"^&gt;interesting_gene\" sequences.fa\n\n# Show lines before and after\n$ grep -C 5 \"error\" logfile.txt\n\n# Extract sequence following a header\n$ grep -A 1 \"^&gt;target_gene\" sequences.fa | grep -v \"^&gt;\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-genomic-processing",
    "href": "chapters/05-grep-regex.html#sec-genomic-processing",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.7 Processing Genomic Data: A Complete Example",
    "text": "5.7 Processing Genomic Data: A Complete Example\nLet’s work through a realistic analysis pipeline for human chromosome data.\nDownloading the Data\n# Download human chromosome 21 (smaller for practice)\n$ wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\\\nGCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n\n# Or work with a subset\n$ zcat genome.fa.gz | head -1000 &gt; sample.fa\nAnalyzing Sequence Content\n# Count sequences in the file\n$ grep -c \"^&gt;\" genome.fa\n\n# View headers\n$ grep \"^&gt;\" genome.fa | head\n\n# Total bases (excluding headers and newlines)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | wc -c\n\n# Nucleotide frequencies\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | fold -w1 | sort | uniq -c\nFinding Restriction Enzyme Sites\n# Count EcoRI sites (GAATTC)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -o \"GAATTC\" | wc -l\n\n# Count multiple restriction sites\n$ for site in GAATTC GGATCC AAGCTT CTCGAG; do\n    count=$(grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -o \"$site\" | wc -l)\n    echo \"$site: $count\"\n  done\n\n# Find context around restriction sites (50 bases each side)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \".{50}GAATTC.{50}\" | head\nFinding Simple Sequence Repeats\n# Dinucleotide repeats\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"(AT){5,}\" | wc -l\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"(CA){5,}\" | wc -l\n\n# Homopolymer runs (consecutive same base)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"A{10,}\" | wc -l\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"T{10,}\" | wc -l\nStream Processing Large Files\nProcess data without loading it entirely into memory:\n# Download, decompress, and analyze in one pipeline\n$ curl -s ftp://example.com/genome.fa.gz | \\\n  gunzip -c | \\\n  grep -v \"^&gt;\" | \\\n  tr -d '\\n' | \\\n  grep -o \"GAATTC\" | \\\n  wc -l\n\n# Process compressed file directly\n$ zgrep -c \"^&gt;\" genome.fa.gz",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-common-patterns",
    "href": "chapters/05-grep-regex.html#sec-common-patterns",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.8 Common Patterns Reference",
    "text": "5.8 Common Patterns Reference\nHere’s a quick reference for frequently needed patterns:\n\n\nTable 5.4: Common regular expression patterns for bioinformatics\n\n\n\nPattern\nWhat It Matches\n\n\n\n^&gt;\nFASTA headers\n\n\n^#\nComment lines\n\n\n^$\nEmpty lines\n\n\n[ACGT]+\nDNA sequence\n\n\n[ACGU]+\nRNA sequence\n\n\n[A-Z]{3}\nUppercase triplet (codon)\n\n\nATG([ACGT]{3})*\nPotential ORF\n\n\n(TAA\\|TAG\\|TGA)\nStop codons\n\n\nA{n,}\nPoly-A of at least n\n\n\n\\t\nTab character\n\n\n\\s+\nOne or more whitespace\n\n\n[0-9]+\\.[0-9]+\nDecimal number",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#sec-regex-summary",
    "href": "chapters/05-grep-regex.html#sec-regex-summary",
    "title": "5  GREP and Regular Expressions",
    "section": "\n5.9 Summary",
    "text": "5.9 Summary\nThis chapter covered pattern matching with grep and regular expressions:\n\n\ngrep finds lines matching patterns in files or streams\nBasic regex uses metacharacters: ., *, ^, $, []\n\nExtended regex (grep -E) adds: +, ?, |, {}\n\nCharacter classes match sets of characters\nAnchors match positions (start/end of line)\nCombining grep with pipes enables powerful analysis\nGenomic data analysis uses these tools for motif searching\n\nRegular expressions are a foundational skill. While the syntax can seem cryptic initially, investment in learning regex pays off enormously across all aspects of computational biology.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#additional-reading",
    "href": "chapters/05-grep-regex.html#additional-reading",
    "title": "5  GREP and Regular Expressions",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo deepen your understanding of regular expressions:\n\n\nRegular-Expressions.info — A comprehensive tutorial and reference site for regex\n\nRegexOne — An interactive tutorial for learning regular expressions step by step\n\nregex101.com — An online regex tester with explanations of each component\n\n“Mastering Regular Expressions” by Jeffrey Friedl — The definitive book on regex theory and practice\n\nFor bioinformatics-specific pattern matching:\n\n\nBioStars: Regular Expressions in Bioinformatics — Community Q&A with many regex examples\n\nRosalind.info — Bioinformatics problem sets that often require regex solutions\n\nGNU Grep Manual — Official documentation with advanced options: man grep or online at gnu.org",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#exercises",
    "href": "chapters/05-grep-regex.html#exercises",
    "title": "5  GREP and Regular Expressions",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Pattern Matching\nCreate a file with various text and practice: 1. Find all lines containing “the” (case-insensitive) 2. Find lines starting with a capital letter 3. Find lines ending with a period 4. Count empty lines\nExercise 2: Biological Patterns\nUsing a FASTA file: 1. Count the number of sequences 2. Extract headers for sequences longer than 1000 bp (hint: you’ll need multiple commands) 3. Find all sequences containing the pattern “TATAAA” (TATA box) 4. Count how many sequences have “chromosome” in their header\nExercise 3: Regular Expression Practice\nWrite grep commands to match: 1. Phone numbers in format: (555) 555-5555 2. Email addresses 3. Dates in YYYY-MM-DD format 4. DNA sequences that are exactly 20 bases long\nExercise 4: Genomic Analysis Pipeline\nDownload a bacterial genome and: 1. Calculate the genome size 2. Count all restriction sites for at least 3 different enzymes 3. Find the longest homopolymer run 4. Calculate GC content using only Unix tools 5. Document your pipeline in a shell script",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html",
    "href": "chapters/06-shell-scripting.html",
    "title": "6  Shell Scripting",
    "section": "",
    "text": "6.1 Why Shell Scripting?\nThroughout the previous chapters, you’ve been typing commands interactively. While this is great for exploration, real-world analyses require something more:\nShell scripts transform a series of commands into a reusable, documented program.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-why-shell-scripting",
    "href": "chapters/06-shell-scripting.html#sec-why-shell-scripting",
    "title": "6  Shell Scripting",
    "section": "",
    "text": "Reproducibility: Record exactly what you did\n\nAutomation: Run the same analysis on many files\n\nEfficiency: Avoid repetitive typing\n\nDocumentation: Explain why you did each step\n\nSharing: Let others reproduce your work",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-first-script",
    "href": "chapters/06-shell-scripting.html#sec-first-script",
    "title": "6  Shell Scripting",
    "section": "\n6.2 Your First Shell Script",
    "text": "6.2 Your First Shell Script\nA shell script is simply a text file containing commands. Let’s create one:\n#!/bin/bash\n# first_script.sh - My first shell script\n# Author: Your Name\n# Date: 2025-01-15\n\n# Display a greeting\necho \"Hello, Bioengineering!\"\n\n# Show the current date\necho \"Today is: $(date +%Y-%m-%d)\"\n\n# List files in current directory\necho \"Files in this directory:\"\nls -la\nThe Shebang Line\nThe first line #!/bin/bash is called a shebang (or hashbang). It tells the operating system which interpreter should execute this script. Always include it.\nOther common shebangs:\n\n\n#!/bin/bash — Bash shell (most common)\n\n#!/bin/sh — POSIX shell (more portable)\n\n#!/usr/bin/env python3 — Python 3\n\n#!/usr/bin/env Rscript — R\nRunning Scripts\nThere are several ways to run a script:\n# Method 1: Invoke the interpreter explicitly\n$ bash first_script.sh\n\n# Method 2: Make executable and run directly\n$ chmod +x first_script.sh\n$ ./first_script.sh\nThe chmod +x command adds execute permission to the file. After that, you can run it directly with ./. The ./ is necessary because your current directory typically isn’t in your system’s PATH.\n\n\n\n\n\n\nTip\n\n\n\nAlways use chmod +x on your scripts. It signals that the file is meant to be executed and allows direct invocation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-shell-variables",
    "href": "chapters/06-shell-scripting.html#sec-shell-variables",
    "title": "6  Shell Scripting",
    "section": "\n6.3 Variables",
    "text": "6.3 Variables\nVariables store values for later use. In Bash, variable assignment has strict syntax rules.\nAssigning Variables\n#!/bin/bash\n\n# Assign variables (NO spaces around the =)\nNAME=\"DNA Analysis\"\nCOUNT=100\nINPUT_FILE=\"sequences.fa\"\n\n# Use variables (with $ prefix)\necho \"Running $NAME\"\necho \"Processing $COUNT sequences from $INPUT_FILE\"\n\n# Curly braces for clarity\necho \"File: ${INPUT_FILE}\"\necho \"Creating ${INPUT_FILE}.backup\"\n\n\n\n\n\n\nCommon Mistake\n\n\n\nSpaces around = will cause errors!\n# WRONG - causes errors\nNAME = \"value\"\nCOUNT= 100\n\n# RIGHT - no spaces\nNAME=\"value\"\nCOUNT=100\n\n\nCommand Substitution\nCapture command output in a variable:\n#!/bin/bash\n\n# Old syntax (backticks)\nTODAY=`date +%Y-%m-%d`\n\n# Modern syntax (preferred)\nTODAY=$(date +%Y-%m-%d)\nFILECOUNT=$(ls *.fa | wc -l)\nGENOME_SIZE=$(grep -v \"^&gt;\" genome.fa | tr -d '\\n' | wc -c)\n\necho \"Analysis date: $TODAY\"\necho \"Processing $FILECOUNT FASTA files\"\necho \"Genome size: $GENOME_SIZE bp\"\nArithmetic Operations\nBash arithmetic uses $(( )):\n#!/bin/bash\n\nCOUNT=10\nDOUBLE=$((COUNT * 2))\nINCREMENT=$((COUNT + 1))\n\n# Calculate percentage\nTOTAL=1000\nMATCHED=350\nPERCENT=$((MATCHED * 100 / TOTAL))\n\necho \"Doubled: $DOUBLE\"\necho \"Match rate: $PERCENT%\"\nFor floating-point math, use external tools like bc:\nGC_CONTENT=$(echo \"scale=2; 450 / 1000 * 100\" | bc)\necho \"GC content: $GC_CONTENT%\"\nString Operations\n#!/bin/bash\n\nFILE=\"sample_001.fastq.gz\"\n\n# Remove suffix\nNAME=\"${FILE%.fastq.gz}\"    # sample_001\n\n# Remove prefix  \nNUMBER=\"${FILE#sample_}\"    # 001.fastq.gz\n\n# Remove longest match from end\nBASENAME=\"${FILE%%.*}\"      # sample_001\n\n# String length\nLENGTH=${#FILE}             # 19\n\necho \"Original: $FILE\"\necho \"Name without extension: $NAME\"\necho \"Length: $LENGTH characters\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-shell-arrays",
    "href": "chapters/06-shell-scripting.html#sec-shell-arrays",
    "title": "6  Shell Scripting",
    "section": "\n6.4 Arrays",
    "text": "6.4 Arrays\nArrays store multiple values:\n#!/bin/bash\n\n# Declare an array\nBASES=(A C G T)\nSAMPLES=(\"control\" \"treatment_1\" \"treatment_2\")\n\n# Access elements (0-indexed)\necho \"First base: ${BASES[0]}\"        # A\necho \"Second sample: ${SAMPLES[1]}\"   # treatment_1\n\n# All elements\necho \"All bases: ${BASES[@]}\"\n\n# Array length\necho \"Number of samples: ${#SAMPLES[@]}\"\n\n# Add element\nBASES+=(N)\n\n# Loop through array\nfor sample in \"${SAMPLES[@]}\"; do\n    echo \"Processing: $sample\"\ndone",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-shell-conditionals",
    "href": "chapters/06-shell-scripting.html#sec-shell-conditionals",
    "title": "6  Shell Scripting",
    "section": "\n6.5 Conditional Statements",
    "text": "6.5 Conditional Statements\nConditionals control program flow based on tests.\nBasic if/then/else\n#!/bin/bash\n\nCOUNT=100\n\nif [ $COUNT -gt 50 ]; then\n    echo \"High count\"\nelif [ $COUNT -gt 10 ]; then\n    echo \"Medium count\"\nelse\n    echo \"Low count\"\nfi\nTest Operators\nNumeric comparisons:\n\n\nOperator\nMeaning\n\n\n\n-eq\nEqual\n\n\n-ne\nNot equal\n\n\n-gt\nGreater than\n\n\n-ge\nGreater than or equal\n\n\n-lt\nLess than\n\n\n-le\nLess than or equal\n\n\n\nString comparisons:\n\n\nOperator\nMeaning\n\n\n\n=\nEqual\n\n\n!=\nNot equal\n\n\n-z\nZero length (empty)\n\n\n-n\nNon-zero length\n\n\n\nFile tests:\n\n\nOperator\nMeaning\n\n\n\n-f\nFile exists and is regular file\n\n\n-d\nDirectory exists\n\n\n-e\nFile exists (any type)\n\n\n-r\nFile is readable\n\n\n-w\nFile is writable\n\n\n-x\nFile is executable\n\n\n-s\nFile exists and is not empty\n\n\nExamples\n#!/bin/bash\n\n# File existence check\nif [ -f \"data.txt\" ]; then\n    echo \"File exists\"\nelse\n    echo \"File not found\"\nfi\n\n# Directory check with creation\nif [ ! -d \"results\" ]; then\n    mkdir results\n    echo \"Created results directory\"\nfi\n\n# String comparison\nFILETYPE=\"fasta\"\nif [ \"$FILETYPE\" = \"fasta\" ]; then\n    echo \"Processing FASTA file\"\nfi\n\n# Combining conditions\nif [ -f \"$FILE\" ] && [ -r \"$FILE\" ]; then\n    echo \"File exists and is readable\"\nfi\n\n\n\n\n\n\nImportant\n\n\n\nAlways quote variables in tests: [ \"$VAR\" = \"value\" ]. Without quotes, empty variables cause syntax errors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-shell-loops",
    "href": "chapters/06-shell-scripting.html#sec-shell-loops",
    "title": "6  Shell Scripting",
    "section": "\n6.6 Loops",
    "text": "6.6 Loops\nLoops repeat actions multiple times.\nFor Loops\n#!/bin/bash\n\n# Loop over a list\nfor base in A C G T; do\n    echo \"Base: $base\"\ndone\n\n# Loop over files\nfor file in *.fastq; do\n    echo \"Processing: $file\"\n    gzip \"$file\"\ndone\n\n# Loop with counter\nfor i in {1..10}; do\n    echo \"Iteration $i\"\ndone\n\n# C-style loop\nfor ((i=0; i&lt;10; i++)); do\n    echo \"Index: $i\"\ndone\n\n# Loop over array\nFILES=(\"sample1.fa\" \"sample2.fa\" \"sample3.fa\")\nfor file in \"${FILES[@]}\"; do\n    echo \"Analyzing $file\"\ndone\nWhile Loops\n#!/bin/bash\n\n# Count down\nCOUNT=5\nwhile [ $COUNT -gt 0 ]; do\n    echo \"Count: $COUNT\"\n    COUNT=$((COUNT - 1))\ndone\n\n# Read file line by line\nwhile read line; do\n    echo \"Processing: $line\"\ndone &lt; input.txt\n\n# Process command output\nls *.fa | while read file; do\n    echo \"Found: $file\"\ndone\nLoop Control\n#!/bin/bash\n\nfor file in *.fa; do\n    # Skip files starting with \"test\"\n    if [[ \"$file\" == test* ]]; then\n        continue\n    fi\n    \n    # Stop if we find \"final.fa\"\n    if [ \"$file\" = \"final.fa\" ]; then\n        echo \"Found final file, stopping\"\n        break\n    fi\n    \n    echo \"Processing: $file\"\ndone",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-cli-arguments",
    "href": "chapters/06-shell-scripting.html#sec-cli-arguments",
    "title": "6  Shell Scripting",
    "section": "\n6.7 Command-Line Arguments",
    "text": "6.7 Command-Line Arguments\nScripts become more flexible when they accept arguments:\n#!/bin/bash\n# process_fasta.sh - Process a FASTA file\n\n# $0 is the script name\necho \"Script: $0\"\n\n# $# is the number of arguments\necho \"Arguments received: $#\"\n\n# $1, $2, etc. are positional arguments\nINPUT=$1\nOUTPUT=$2\n\n# $@ is all arguments\necho \"All arguments: $@\"\n\n# Check for required arguments\nif [ $# -lt 2 ]; then\n    echo \"Usage: $0 input.fasta output.txt\"\n    exit 1\nfi\n\n# Now process the files\necho \"Input: $INPUT\"\necho \"Output: $OUTPUT\"\nUsage:\n$ ./process_fasta.sh sequences.fa results.txt\nArgument Validation\n#!/bin/bash\n\n# Check argument count\nif [ $# -ne 2 ]; then\n    echo \"Error: Expected 2 arguments, got $#\" &gt;&2\n    echo \"Usage: $0 input.fa output.txt\" &gt;&2\n    exit 1\nfi\n\nINPUT=$1\nOUTPUT=$2\n\n# Validate input file\nif [ ! -f \"$INPUT\" ]; then\n    echo \"Error: Input file '$INPUT' not found\" &gt;&2\n    exit 1\nfi\n\n# Check if output would overwrite\nif [ -f \"$OUTPUT\" ]; then\n    echo \"Warning: Output file exists. Overwrite? (y/n)\"\n    read answer\n    if [ \"$answer\" != \"y\" ]; then\n        exit 0\n    fi\nfi\n\n# Continue with processing...",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-shell-functions",
    "href": "chapters/06-shell-scripting.html#sec-shell-functions",
    "title": "6  Shell Scripting",
    "section": "\n6.8 Functions",
    "text": "6.8 Functions\nFunctions make scripts modular and reusable:\n#!/bin/bash\n\n# Define a function\ncount_sequences() {\n    local FILE=$1  # Local variable\n    local COUNT=$(grep -c \"^&gt;\" \"$FILE\")\n    echo $COUNT\n}\n\n# Function with multiple parameters\nanalyze_fasta() {\n    local INPUT=$1\n    local OUTPUT=$2\n    \n    echo \"Analyzing $INPUT...\"\n    \n    local SEQ_COUNT=$(count_sequences \"$INPUT\")\n    local TOTAL_LENGTH=$(grep -v \"^&gt;\" \"$INPUT\" | tr -d '\\n' | wc -c)\n    \n    echo \"Sequences: $SEQ_COUNT\" &gt; \"$OUTPUT\"\n    echo \"Total bases: $TOTAL_LENGTH\" &gt;&gt; \"$OUTPUT\"\n}\n\n# Call functions\nSEQ_NUM=$(count_sequences \"proteins.fa\")\necho \"Found $SEQ_NUM sequences\"\n\nanalyze_fasta \"input.fa\" \"statistics.txt\"\nReturn Values\nFunctions return exit status (0-255), not strings. To return data:\n#!/bin/bash\n\n# Return via stdout\nget_gc_content() {\n    local FILE=$1\n    local GC=$(grep -v \"^&gt;\" \"$FILE\" | tr -d '\\n' | grep -o \"[GC]\" | wc -l)\n    local TOTAL=$(grep -v \"^&gt;\" \"$FILE\" | tr -d '\\n' | wc -c)\n    echo \"$((GC * 100 / TOTAL))\"\n}\n\n# Capture output\nGC_PERCENT=$(get_gc_content \"genome.fa\")\necho \"GC content: $GC_PERCENT%\"\n\n# Return status for success/failure\ncheck_file() {\n    if [ -f \"$1\" ] && [ -r \"$1\" ]; then\n        return 0  # Success\n    else\n        return 1  # Failure\n    fi\n}\n\nif check_file \"data.txt\"; then\n    echo \"File is ready\"\nelse\n    echo \"File not accessible\"\nfi",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-error-handling",
    "href": "chapters/06-shell-scripting.html#sec-error-handling",
    "title": "6  Shell Scripting",
    "section": "\n6.9 Error Handling",
    "text": "6.9 Error Handling\nRobust scripts handle errors gracefully.\nExit Status\nEvery command returns an exit status: 0 for success, non-zero for failure.\n#!/bin/bash\n\n# Check last command's status\ngrep \"pattern\" file.txt\nif [ $? -ne 0 ]; then\n    echo \"Pattern not found\"\nfi\n\n# Conditional execution\ngrep \"pattern\" file.txt && echo \"Found!\"\ngrep \"pattern\" file.txt || echo \"Not found\"\nStrict Mode\nStart scripts with these settings for better error detection:\n#!/bin/bash\nset -e          # Exit on any error\nset -u          # Error on undefined variables\nset -o pipefail # Catch errors in pipes\n\n# Or combine: set -euo pipefail\nCustom Error Handling\n#!/bin/bash\nset -euo pipefail\n\n# Define error handler\nerror_exit() {\n    echo \"Error on line $1\" &gt;&2\n    exit 1\n}\n\n# Set trap to call handler on error\ntrap 'error_exit $LINENO' ERR\n\n# Now errors are caught\ncheck_file() {\n    if [ ! -f \"$1\" ]; then\n        echo \"Error: File $1 not found!\" &gt;&2\n        exit 1\n    fi\n}\n\n# Validate inputs\ncheck_file \"$1\"\n\n# Continue processing...\nCleanup on Exit\nEnsure temporary files are removed even if script fails:\n#!/bin/bash\n\n# Create temp file\nTMPFILE=$(mktemp)\n\n# Remove temp file on exit (normal or error)\ntrap \"rm -f $TMPFILE\" EXIT\n\n# Use temp file...\necho \"working data\" &gt; \"$TMPFILE\"\n# ... do processing ...\n\n# Cleanup happens automatically",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-rscript",
    "href": "chapters/06-shell-scripting.html#sec-rscript",
    "title": "6  Shell Scripting",
    "section": "\n6.10 Running R Scripts from the Shell",
    "text": "6.10 Running R Scripts from the Shell\nWhile shell scripts are powerful, sometimes you need R’s statistical capabilities. The Rscript command lets you run R code directly from the shell.\nBasic Usage\n# Run an R script\n$ Rscript my_analysis.R\n\n# Execute a single R expression\n$ Rscript -e \"print('Hello from R!')\"\n\n# Quick calculation\n$ Rscript -e \"mean(c(1, 2, 3, 4, 5))\"\nPassing Arguments to R Scripts\nYou can pass command-line arguments to R scripts, making them flexible and reusable:\n#!/usr/bin/env Rscript\n# process_data.R - Process a data file with custom parameters\n\n# Capture command-line arguments\nargs &lt;- commandArgs(trailingOnly = TRUE)\n\n# Check for required arguments\nif (length(args) &lt; 2) {\n  stop(\"Usage: Rscript process_data.R &lt;input_file&gt; &lt;output_file&gt;\")\n}\n\ninput_file &lt;- args[1]\noutput_file &lt;- args[2]\n\n# Now use these in your analysis\ndata &lt;- read.csv(input_file)\n# ... process data ...\nwrite.csv(data, output_file)\n\ncat(\"Processed\", input_file, \"-&gt; \", output_file, \"\\n\")\nRun it from the shell:\n$ Rscript process_data.R raw_data.csv cleaned_data.csv\nCombining Shell and R\nA common pattern is using shell scripts to orchestrate R analyses:\n#!/bin/bash\n# run_analysis.sh - Process all CSV files in a directory\n\nfor file in data/*.csv; do\n    output=\"results/$(basename \"$file\" .csv)_processed.csv\"\n    echo \"Processing $file...\"\n    Rscript process_data.R \"$file\" \"$output\"\ndone\n\necho \"All files processed!\"\n\n\n\n\n\n\nTip\n\n\n\nUsing Rscript is particularly valuable for batch processing, scheduled tasks (cron jobs), and integration with high-performance computing clusters where you submit shell scripts as jobs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-complete-script",
    "href": "chapters/06-shell-scripting.html#sec-complete-script",
    "title": "6  Shell Scripting",
    "section": "\n6.11 A Complete Bioinformatics Script",
    "text": "6.11 A Complete Bioinformatics Script\nLet’s put it all together with a realistic example:\n#!/bin/bash\n#===============================================================================\n# fasta_stats.sh - Calculate statistics for FASTA files\n# \n# Usage: ./fasta_stats.sh input.fasta [output.txt]\n#\n# Author: Your Name\n# Date: 2025-01-15\n#===============================================================================\n\nset -euo pipefail\n\n#-------------------------------------------------------------------------------\n# Configuration\n#-------------------------------------------------------------------------------\nVERSION=\"1.0.0\"\n\n#-------------------------------------------------------------------------------\n# Functions\n#-------------------------------------------------------------------------------\n\nusage() {\n    cat &lt;&lt; EOF\nUsage: $(basename \"$0\") [OPTIONS] input.fasta [output.txt]\n\nCalculate basic statistics for FASTA files.\n\nOptions:\n    -h, --help      Show this help message\n    -v, --version   Show version information\n    -q, --quiet     Suppress progress messages\n\nArguments:\n    input.fasta     Input FASTA file (required)\n    output.txt      Output file (optional, defaults to stdout)\n\nExamples:\n    $(basename \"$0\") sequences.fa\n    $(basename \"$0\") genome.fa stats.txt\n    $(basename \"$0\") -q input.fa &gt; results.txt\nEOF\n}\n\nlog() {\n    if [ \"$QUIET\" = false ]; then\n        echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" &gt;&2\n    fi\n}\n\nerror() {\n    echo \"Error: $1\" &gt;&2\n    exit 1\n}\n\ncount_sequences() {\n    grep -c \"^&gt;\" \"$1\"\n}\n\ntotal_length() {\n    grep -v \"^&gt;\" \"$1\" | tr -d '\\n' | wc -c | tr -d ' '\n}\n\ngc_content() {\n    local gc=$(grep -v \"^&gt;\" \"$1\" | tr -d '\\n' | grep -o \"[GC]\" | wc -l | tr -d ' ')\n    local total=$(total_length \"$1\")\n    if [ \"$total\" -gt 0 ]; then\n        echo \"$((gc * 100 / total))\"\n    else\n        echo \"0\"\n    fi\n}\n\n#-------------------------------------------------------------------------------\n# Parse Arguments\n#-------------------------------------------------------------------------------\n\nQUIET=false\n\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        -h|--help)\n            usage\n            exit 0\n            ;;\n        -v|--version)\n            echo \"$(basename \"$0\") version $VERSION\"\n            exit 0\n            ;;\n        -q|--quiet)\n            QUIET=true\n            shift\n            ;;\n        -*)\n            error \"Unknown option: $1\"\n            ;;\n        *)\n            break\n            ;;\n    esac\ndone\n\n# Check required arguments\nif [ $# -lt 1 ]; then\n    usage &gt;&2\n    exit 1\nfi\n\nINPUT=\"$1\"\nOUTPUT=\"${2:-/dev/stdout}\"\n\n#-------------------------------------------------------------------------------\n# Validate Input\n#-------------------------------------------------------------------------------\n\nif [ ! -f \"$INPUT\" ]; then\n    error \"Input file not found: $INPUT\"\nfi\n\nif [ ! -r \"$INPUT\" ]; then\n    error \"Cannot read input file: $INPUT\"\nfi\n\n#-------------------------------------------------------------------------------\n# Main Analysis\n#-------------------------------------------------------------------------------\n\nlog \"Starting analysis of $INPUT\"\n\nlog \"Counting sequences...\"\nSEQ_COUNT=$(count_sequences \"$INPUT\")\n\nlog \"Calculating total length...\"\nTOTAL_LEN=$(total_length \"$INPUT\")\n\nlog \"Computing GC content...\"\nGC_PCT=$(gc_content \"$INPUT\")\n\n# Calculate average length\nif [ \"$SEQ_COUNT\" -gt 0 ]; then\n    AVG_LEN=$((TOTAL_LEN / SEQ_COUNT))\nelse\n    AVG_LEN=0\nfi\n\n#-------------------------------------------------------------------------------\n# Output Results\n#-------------------------------------------------------------------------------\n\n{\n    echo \"=== FASTA Statistics ===\"\n    echo \"File: $INPUT\"\n    echo \"Date: $(date '+%Y-%m-%d %H:%M:%S')\"\n    echo \"\"\n    echo \"Number of sequences: $SEQ_COUNT\"\n    echo \"Total length: $TOTAL_LEN bp\"\n    echo \"Average length: $AVG_LEN bp\"\n    echo \"GC content: $GC_PCT%\"\n} &gt; \"$OUTPUT\"\n\nlog \"Analysis complete. Results written to $OUTPUT\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-scripting-best-practices",
    "href": "chapters/06-shell-scripting.html#sec-scripting-best-practices",
    "title": "6  Shell Scripting",
    "section": "\n6.12 Best Practices Summary",
    "text": "6.12 Best Practices Summary\n\n\n\n\n\n\nShell Scripting Best Practices\n\n\n\n\n\nAlways include a shebang: #!/bin/bash\n\n\nUse strict mode: set -euo pipefail\n\n\nComment your code: Explain what and why\n\nUse meaningful names: SEQ_COUNT not SC\n\n\nValidate inputs: Check files exist, arguments are provided\n\nHandle errors: Provide useful error messages\n\nMake it portable: Don’t hardcode paths\n\nTest incrementally: Build and test piece by piece\n\nVersion control: Track changes with git\n\nDocument usage: Include help messages",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#sec-scripting-summary",
    "href": "chapters/06-shell-scripting.html#sec-scripting-summary",
    "title": "6  Shell Scripting",
    "section": "\n6.13 Summary",
    "text": "6.13 Summary\nThis chapter covered shell scripting essentials:\n\nScripts are text files containing commands with a shebang line\nVariables store values; arrays store multiple values\nConditionals (if/elif/else) control flow based on tests\nLoops (for, while) repeat actions\nFunctions make code modular and reusable\nCommand-line arguments make scripts flexible\nError handling ensures robustness\nCombining these elements creates powerful analysis pipelines\n\nShell scripting is a foundational skill for computational biology. Even as you learn other languages like R and Python, shell scripts remain invaluable for automation, file management, and orchestrating complex workflows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#additional-reading",
    "href": "chapters/06-shell-scripting.html#additional-reading",
    "title": "6  Shell Scripting",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo advance your shell scripting skills:\n\n\nBash Guide for Beginners — A comprehensive introduction to Bash scripting\n\nAdvanced Bash-Scripting Guide — An in-depth exploration of shell scripting techniques\n\nShellCheck — An online tool for finding bugs in your shell scripts\n\n“Learning the bash Shell” by Cameron Newham — A thorough guide to Bash programming\n\nGoogle Shell Style Guide — Best practices for writing maintainable scripts\n\nFor bioinformatics workflow development:\n\n\nSnakemake — A workflow management system that builds on shell scripting concepts\n\nNextflow — A domain-specific language for data-driven pipelines\n\nGNU Make Tutorial — Build automation that complements shell scripts",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#exercises",
    "href": "chapters/06-shell-scripting.html#exercises",
    "title": "6  Shell Scripting",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Script\nCreate a script that: 1. Accepts a directory as an argument 2. Counts files of each type (.txt, .csv, .fa, etc.) 3. Reports the results\nExercise 2: File Processing Loop\nWrite a script that: 1. Loops through all .fastq files in a directory 2. Counts the number of reads in each (hint: lines/4 in FASTQ) 3. Saves results to a summary file\nExercise 3: FASTA Validator\nCreate a script that validates FASTA files: 1. Check that every header line starts with &gt; 2. Check that sequence lines only contain valid characters 3. Report any errors found\nExercise 4: Analysis Pipeline\nBuild a complete analysis script that: 1. Accepts input/output arguments with validation 2. Downloads a bacterial genome if not present 3. Calculates genome statistics 4. Finds all occurrences of a user-specified motif 5. Generates a summary report 6. Includes proper error handling and logging",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html",
    "href": "chapters/07-r-programming.html",
    "title": "7  R Programming Fundamentals",
    "section": "",
    "text": "7.1 Why R for Scientific Computing?\nR is a programming language designed specifically for statistical computing and graphics (R Core Team, 2024). It is an offshoot of a language called S, developed in 1976 at Bell Laboratories by John Chambers and colleagues. R was created in 1991 by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand (hence the name “R”), and has since become one of the most widely used tools in data science, statistics, and bioinformatics.\nAs an interpreted language, R code is translated to machine language by the R interpreter each time it runs, as opposed to being compiled beforehand. This makes R highly interactive and excellent for exploratory data analysis.\nR offers several advantages for researchers:\nStatistical Power. R was built by statisticians for statistics. It includes comprehensive implementations of classical and modern statistical methods.\nGraphics Excellence. R produces publication-quality graphics with fine-grained control over every aspect of visualization.\nPackage Ecosystem. Over 20,000 packages extend R’s capabilities for virtually every analytical need, from genomics to machine learning.\nReproducibility. R scripts document your analysis completely. Combined with Quarto (which produced this book!), you can create reproducible documents that integrate code, results, and narrative.\nActive Community. A large, welcoming community provides abundant resources, tutorials, and support.\nCost. R is free and open-source, removing financial barriers to sophisticated analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-rstudio",
    "href": "chapters/07-r-programming.html#sec-rstudio",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.2 RStudio: Your R Environment",
    "text": "7.2 RStudio: Your R Environment\nWhile R can run from the command line, most users work in RStudio, an integrated development environment (IDE) that makes R more accessible and productive.\nThe RStudio Interface\nRStudio organizes your workspace into four panes:\nSource (top-left). Where you write and edit scripts. Code here isn’t executed until you explicitly run it.\nConsole (bottom-left). The interactive R session. Commands typed here execute immediately. Output appears here.\nEnvironment/History (top-right). Shows variables you’ve created (Environment tab) and commands you’ve run (History tab).\nFiles/Plots/Help (bottom-right). File browser, plot viewer, and help documentation.\n\n\n\n\n\nFigure 7.1: The RStudio interface showing all four panes: Source (top-left) with R code, Console (bottom-left) showing command output, Environment/Workspace (top-right) displaying variables and data, and Plots (bottom-right) showing a visualization.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearn keyboard shortcuts! Ctrl+Enter (Windows/Linux) or Cmd+Enter (Mac) runs the current line or selection. Ctrl+Shift+S runs the entire script.\n\n\nRStudio Projects\nRStudio Projects organize your work with self-contained directories:\n\nGo to File → New Project\nChoose New Directory → New Project\nName your project and select a location\nRStudio creates a .Rproj file\n\nProjects set your working directory automatically and keep related files together—essential for reproducible research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-r-objects",
    "href": "chapters/07-r-programming.html#sec-r-objects",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.3 Understanding R: Everything is an Object",
    "text": "7.3 Understanding R: Everything is an Object\nBefore diving into R syntax, it helps to understand R’s fundamental design philosophy: everything in R is an object. This object-oriented approach means that every piece of data you work with—numbers, text, datasets, even functions themselves—is stored as an object with specific properties.\n\n# Numbers are objects\nx &lt;- 42\nclass(x)\n#&gt; [1] \"numeric\"\ntypeof(x)\n#&gt; [1] \"double\"\n\n# Text strings are objects\nname &lt;- \"Gene Expression\"\nclass(name)\n#&gt; [1] \"character\"\ntypeof(name)\n#&gt; [1] \"character\"\n\n# Even functions are objects!\nclass(mean)\n#&gt; [1] \"function\"\ntypeof(mean)\n#&gt; [1] \"closure\"\n\n# And data frames are objects\ndf &lt;- data.frame(a = 1:3, b = c(\"x\", \"y\", \"z\"))\nclass(df)\n#&gt; [1] \"data.frame\"\ntypeof(df)\n#&gt; [1] \"list\"\n\nEvery object has:\n\n\nA class: Determines how the object behaves with functions (what class() returns)\n\nA type: The underlying storage mode (what typeof() returns)\n\nAttributes: Optional metadata like names, dimensions, or custom properties\n\nThe str() function provides a compact display of an object’s structure:\n\n# Examine structure of different objects\nstr(x)\n#&gt;  num 42\nstr(name)\n#&gt;  chr \"Gene Expression\"\nstr(df)\n#&gt; 'data.frame':    3 obs. of  2 variables:\n#&gt;  $ a: int  1 2 3\n#&gt;  $ b: chr  \"x\" \"y\" \"z\"\n\nUnderstanding this object-oriented nature helps you:\n\n\nDebug errors: When functions fail, check what class of object you’re passing\n\nUse generic functions: Functions like print(), summary(), and plot() behave differently based on object class\n\nExtend R: You can create custom classes with specialized behaviors",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-r-basics",
    "href": "chapters/07-r-programming.html#sec-r-basics",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.4 R Basics: Arithmetic and Variables",
    "text": "7.4 R Basics: Arithmetic and Variables\nR as a Calculator\nThe simplest use of R is arithmetic:\n\n# Basic operations\n4 * 4\n#&gt; [1] 16\n\n# Order of operations applies\n(4 + 3 * 2^2)\n#&gt; [1] 16\n\n# More complex expressions\nsqrt(144) + log(100)\n#&gt; [1] 16.60517\n\n# Integer division and modulo\n17 %/% 5    # Integer division: how many times 5 goes into 17\n#&gt; [1] 3\n17 %% 5     # Modulo: remainder after division\n#&gt; [1] 2\n\nR follows standard mathematical order of operations (PEMDAS). The modulo operator (%%) is particularly useful for tasks like determining if a number is even or odd, cycling through indices, or extracting digits from numbers:\n\n# Check if numbers are even (remainder 0 when divided by 2)\nnumbers &lt;- 1:10\nnumbers %% 2 == 0\n#&gt;  [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE\n\n# Extract the last digit of a number\n12345 %% 10\n#&gt; [1] 5\n\nCreating Variables\nVariables store values for later use. In R, the assignment operator is &lt;-:\n\n# Assign values to variables\nx &lt;- 2\ny &lt;- 5\n\n# Use variables in calculations\nx * 3\n#&gt; [1] 6\ny + x\n#&gt; [1] 7\n\n# Store results in new variables\nz &lt;- x * y\nz\n#&gt; [1] 10\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also use = for assignment in R, but &lt;- is the traditional and preferred style. It makes assignment visually distinct from function arguments, which use =.\n\n\nVariable Naming Rules\n\nNames must start with a letter\nCan contain letters, numbers, periods, and underscores\nR is case-sensitive: Gene and gene are different variables\nUse descriptive names: sample_count is better than sc\n\n\n\n# Valid names\ngene_count &lt;- 100\nSample.1 &lt;- \"control\"\nmyData2 &lt;- 42\n\n# Invalid (would cause errors)\n# 2samples &lt;- 5      # Can't start with number\n# my-data &lt;- 10      # Hyphens not allowed\n\nReserved Words\nR has reserved words that cannot be used as variable names because they have special meaning in the language:\n\n\nTable 7.1: R reserved words\n\n\n\nReserved Words\nDescription\n\n\n\n\nif, else\n\nConditional statements\n\n\n\nfor, while, repeat\n\nLoop constructs\n\n\nfunction\nFunction definition\n\n\nin\nUsed in for loops\n\n\n\nnext, break\n\nLoop control\n\n\n\nTRUE, FALSE\n\nLogical constants\n\n\nNULL\nNull object\n\n\n\nInf, NaN, NA\n\nSpecial numeric values\n\n\n\n\n\n\nR also has semi-reserved words—names of built-in constants and functions that you can overwrite but generally shouldn’t:\n\n# These work but are dangerous:\nT &lt;- 5       # Overwrites TRUE abbreviation\nF &lt;- 10      # Overwrites FALSE abbreviation\nc &lt;- \"text\"  # Shadows the c() function\nmean &lt;- 42   # Shadows the mean() function\n\n# If you accidentally overwrite something, use rm() to remove it\nrm(c)        # Removes your 'c' variable, restoring access to c()\n\n\n\n\n\n\n\nWarning\n\n\n\nAvoid using T and F as variable names—they are common abbreviations for TRUE and FALSE. Similarly, never name variables c, t, mean, sum, or other common function names.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-r-terminology",
    "href": "chapters/07-r-programming.html#sec-r-terminology",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.5 Key R Terminology",
    "text": "7.5 Key R Terminology\nBefore diving deeper into R, let’s establish some fundamental terminology that you’ll encounter throughout your R programming journey:\n\n\n\n\n\nFigure 7.2: Core R concepts: Objects, Vectors, Functions, Parameters, and Arguments are the building blocks of R programming.\n\n\nUnderstanding R’s operators is equally important. The table below shows the main operators you’ll use, listed in order of precedence (operations higher in the table are evaluated first):\n\n\n\n\n\nFigure 7.3: R operators reference: This table shows common R operators for indexing, arithmetic, comparison, logical operations, and assignment.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that R uses &lt;- for assignment (right to left), while = is typically used for argument assignment within function calls. Both work for variable assignment, but &lt;- is the conventional R style.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-vectors",
    "href": "chapters/07-r-programming.html#sec-vectors",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.6 Vectors: R’s Fundamental Data Structure",
    "text": "7.6 Vectors: R’s Fundamental Data Structure\nR thinks in terms of vectors—ordered collections of values of the same type. Even a single number is a vector of length 1.\nCreating Vectors\nThe c() function (for “combine” or “concatenate”) creates vectors:\n\n# Numeric vector\nmeasurements &lt;- c(2, 3, 4, 2, 1, 2, 4, 5, 10, 8, 9)\nmeasurements\n#&gt;  [1]  2  3  4  2  1  2  4  5 10  8  9\n\n# Character vector\nsamples &lt;- c(\"control\", \"treatment_A\", \"treatment_B\")\nsamples\n#&gt; [1] \"control\"     \"treatment_A\" \"treatment_B\"\n\n# Logical vector\npassed &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\npassed\n#&gt; [1]  TRUE FALSE  TRUE  TRUE FALSE\n\nSequence Generation\nFor regular sequences, use shortcuts:\n\n# Integer sequence\n1:10\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Sequence with step\nseq(0, 10, by = 0.5)\n#&gt;  [1]  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0\n#&gt; [16]  7.5  8.0  8.5  9.0  9.5 10.0\n\n# Specified length\nseq(0, 1, length.out = 5)\n#&gt; [1] 0.00 0.25 0.50 0.75 1.00\n\n# Repeated values\nrep(\"A\", 5)\n#&gt; [1] \"A\" \"A\" \"A\" \"A\" \"A\"\nrep(c(1, 2), times = 3)\n#&gt; [1] 1 2 1 2 1 2\nrep(c(1, 2), each = 3)\n#&gt; [1] 1 1 1 2 2 2\n\nSorting Vectors\nThe sort() function returns a sorted vector:\n\n# Sort in ascending order (default)\nx &lt;- c(5, 2, 8, 1, 9, 3)\nsort(x)\n#&gt; [1] 1 2 3 5 8 9\n\n# Sort in descending order\nsort(x, decreasing = TRUE)\n#&gt; [1] 9 8 5 3 2 1\n\n# Sorting character vectors (alphabetical)\ngenes &lt;- c(\"BRCA1\", \"TP53\", \"EGFR\", \"KRAS\")\nsort(genes)\n#&gt; [1] \"BRCA1\" \"EGFR\"  \"KRAS\"  \"TP53\"\n\nTo get the indices that would sort a vector (useful for reordering related data), use order():\n\nx &lt;- c(5, 2, 8, 1, 9, 3)\norder(x)  # Indices: 4th element is smallest, 2nd is next, etc.\n#&gt; [1] 4 2 6 1 3 5\n\n# Use order() to sort one vector by another\nnames &lt;- c(\"Sample_A\", \"Sample_B\", \"Sample_C\")\nvalues &lt;- c(30, 10, 20)\nnames[order(values)]  # Names sorted by values\n#&gt; [1] \"Sample_B\" \"Sample_C\" \"Sample_A\"\n\nVectorized Operations\nR’s power comes from vectorized operations—operations that apply to entire vectors at once:\n\nx &lt;- c(2, 3, 4, 2, 1, 2, 4, 5, 10, 8, 9)\n\n# Operations apply to each element\nx * 2\n#&gt;  [1]  4  6  8  4  2  4  8 10 20 16 18\nx^2\n#&gt;  [1]   4   9  16   4   1   4  16  25 100  64  81\nlog(x)\n#&gt;  [1] 0.6931472 1.0986123 1.3862944 0.6931472 0.0000000 0.6931472 1.3862944\n#&gt;  [8] 1.6094379 2.3025851 2.0794415 2.1972246\nsqrt(x)\n#&gt;  [1] 1.414214 1.732051 2.000000 1.414214 1.000000 1.414214 2.000000 2.236068\n#&gt;  [9] 3.162278 2.828427 3.000000\n\n# Operations between vectors (element-wise)\ny &lt;- 1:11\nx + y\n#&gt;  [1]  3  5  7  6  6  8 11 13 19 18 20\nx * y\n#&gt;  [1]  2  6 12  8  5 12 28 40 90 80 99\n\nThis is much faster and cleaner than writing loops!\nVector Indexing\nAccess specific elements using square brackets:\n\nmeasurements &lt;- c(2, 3, 4, 2, 1, 2, 4, 5, 10, 8, 9)\n\n# Single element (1-indexed!)\nmeasurements[1]\n#&gt; [1] 2\nmeasurements[5]\n#&gt; [1] 1\n\n# Multiple elements\nmeasurements[c(1, 3, 5)]\n#&gt; [1] 2 4 1\n\n# Range\nmeasurements[2:5]\n#&gt; [1] 3 4 2 1\n\n# Negative indices exclude\nmeasurements[-1]          # All but first\n#&gt;  [1]  3  4  2  1  2  4  5 10  8  9\nmeasurements[-(1:3)]      # All but first three\n#&gt; [1]  2  1  2  4  5 10  8  9\n\n# Logical indexing\nmeasurements[measurements &gt; 5]\n#&gt; [1] 10  8  9\n\n\n\n\n\n\n\nImportant\n\n\n\nR uses 1-based indexing! The first element is [1], not [0] as in Python and most other languages.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-data-types",
    "href": "chapters/07-r-programming.html#sec-data-types",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.7 Data Types in R",
    "text": "7.7 Data Types in R\nR has several fundamental data types:\n\n\nTable 7.2: R data types\n\n\n\nType\nDescription\nExample\n\n\n\nnumeric\nReal numbers (default)\n\n3.14, 42\n\n\n\ninteger\nWhole numbers\n\n1L, 100L\n\n\n\ncharacter\nText strings\n\n\"hello\", \"gene1\"\n\n\n\nlogical\nBoolean values\n\nTRUE, FALSE\n\n\n\nfactor\nCategorical variables\nTreatment levels\n\n\n\n\n\n\n\n# Check types with class()\nclass(3.14)\n#&gt; [1] \"numeric\"\nclass(\"hello\")\n#&gt; [1] \"character\"\nclass(TRUE)\n#&gt; [1] \"logical\"\n\n# Type coercion\nas.numeric(\"42\")\n#&gt; [1] 42\nas.character(123)\n#&gt; [1] \"123\"\nas.logical(0)    # FALSE\n#&gt; [1] FALSE\nas.logical(1)    # TRUE\n#&gt; [1] TRUE\n\nFactors\nFactors are special vectors for representing categorical data with a fixed set of possible values called levels. They’re essential for statistical modeling and data analysis in R:\n\n# Create a factor from a character vector\ntreatment &lt;- c(\"control\", \"drug_A\", \"drug_A\", \"control\", \"drug_B\", \"drug_B\")\ntreatment_factor &lt;- as.factor(treatment)\ntreatment_factor\n#&gt; [1] control drug_A  drug_A  control drug_B  drug_B \n#&gt; Levels: control drug_A drug_B\n\n# Check the levels\nlevels(treatment_factor)\n#&gt; [1] \"control\" \"drug_A\"  \"drug_B\"\n\n\n\n\n\n\n\nNote\n\n\n\nFactor levels are stored alphabetically by default, regardless of the order they appear in your data. This affects how results are displayed and can impact statistical analyses (e.g., the reference level in regression models).\n\n\nTo specify a custom level order, use the levels argument:\n\n# Custom level order\ntreatment_ordered &lt;- factor(treatment,\n                            levels = c(\"control\", \"drug_A\", \"drug_B\"))\nlevels(treatment_ordered)\n#&gt; [1] \"control\" \"drug_A\"  \"drug_B\"\n\n# This matters for plotting and statistical models\n# where \"control\" should be the reference level\n\nFactors are memory-efficient because they store unique levels once and use integer indices to reference them. Use str() to see this internal representation:\n\nstr(treatment_factor)\n#&gt;  Factor w/ 3 levels \"control\",\"drug_A\",..: 1 2 2 1 3 3\n\nSpecial Values\nR has special values for missing data and undefined results:\n\n# Missing value\nNA\n#&gt; [1] NA\n\n# Not a number (undefined)\n0/0\n#&gt; [1] NaN\nlog(-1)\n#&gt; [1] NaN\n\n# Infinity\n1/0\n#&gt; [1] Inf\n-1/0\n#&gt; [1] -Inf\n\nHandling NA values is a common task in data analysis.\nLogical Operators and Comparisons\nR provides a rich set of operators for logical comparisons and Boolean operations:\n\n# Comparison operators\n5 &gt; 3     # Greater than\n#&gt; [1] TRUE\n5 &lt; 3     # Less than\n#&gt; [1] FALSE\n5 &gt;= 5    # Greater than or equal\n#&gt; [1] TRUE\n5 &lt;= 5    # Less than or equal\n#&gt; [1] TRUE\n5 == 5    # Equal (note: two equals signs!)\n#&gt; [1] TRUE\n5 != 3    # Not equal\n#&gt; [1] TRUE\n\n# Logical operators\nTRUE & FALSE   # AND\n#&gt; [1] FALSE\nTRUE | FALSE   # OR\n#&gt; [1] TRUE\n!TRUE          # NOT\n#&gt; [1] FALSE\n\n# Vectorized logical operations\nx &lt;- c(1, 5, 10, 15, 20)\nx &gt; 5 & x &lt; 18   # Both conditions must be true\n#&gt; [1] FALSE FALSE  TRUE  TRUE FALSE\nx &lt; 5 | x &gt; 15   # Either condition can be true\n#&gt; [1]  TRUE FALSE FALSE FALSE  TRUE\n\nThe %in% operator is particularly useful for checking membership in a set:\n\n# Check if values are in a set\ngenes &lt;- c(\"BRCA1\", \"TP53\", \"EGFR\", \"KRAS\")\ntarget_genes &lt;- c(\"TP53\", \"EGFR\")\n\ngenes %in% target_genes\n#&gt; [1] FALSE  TRUE  TRUE FALSE\n\n# Useful for subsetting\ngenes[genes %in% target_genes]\n#&gt; [1] \"TP53\" \"EGFR\"\n\nFinding Indices with which()\n\nThe which() function returns the indices where a logical condition is TRUE—useful when you need positions rather than values:\n\nx &lt;- c(10, 5, 20, 15, 8, 25)\n\n# Find indices where condition is TRUE\nwhich(x &gt; 12)\n#&gt; [1] 3 4 6\n\n# Get the values at those positions\nx[which(x &gt; 12)]\n#&gt; [1] 20 15 25\n\n# Find the index of the maximum value\nwhich.max(x)\n#&gt; [1] 6\n\n# Find the index of the minimum value\nwhich.min(x)\n#&gt; [1] 2\n\n# Find indices of specific values\nwhich(x == 15)\n#&gt; [1] 4\n\n# Practical example: find which samples exceed a threshold\nexpression_levels &lt;- c(2.5, 8.1, 4.2, 12.3, 6.7, 15.2)\nhigh_expression &lt;- which(expression_levels &gt; 10)\nhigh_expression  # Indices 4 and 6\n#&gt; [1] 4 6\n\n\n\n\n\n\n\nTip\n\n\n\nWhile which() is useful, you often don’t need it—logical vectors work directly for subsetting:\n\n# These are equivalent:\nx[which(x &gt; 12)]\n#&gt; [1] 20 15 25\nx[x &gt; 12]\n#&gt; [1] 20 15 25\n\nUse which() when you specifically need the index positions (e.g., to report which samples, to use in loops, or for which.max()/which.min()).\n\n\n\n\n\n\n\n\nOperator Precedence\n\n\n\nR evaluates operators in a specific order. From highest to lowest precedence:\n\n\n^ (exponentiation)\n\n- (unary minus, for negation)\n\n: (sequence)\n\n%any% (special operators like %%, %/%, %in%)\n\n*, / (multiplication, division)\n\n+, - (addition, subtraction)\n\n&lt;, &gt;, &lt;=, &gt;=, ==, != (comparisons)\n\n! (logical NOT)\n\n&, && (logical AND)\n\n|, || (logical OR)\n\n&lt;-, = (assignment)\n\nUse parentheses to make your intentions clear and avoid unexpected results.\n\n\nFloating-Point Precision\nA common source of confusion involves floating-point arithmetic. Computers represent decimal numbers with limited precision, which can lead to unexpected results:\n\n# This seems wrong, but is due to floating-point representation\n0.1 + 0.2 == 0.3\n#&gt; [1] FALSE\n\n# See the actual values\nprint(0.1 + 0.2, digits = 20)\n#&gt; [1] 0.30000000000000004441\nprint(0.3, digits = 20)\n#&gt; [1] 0.2999999999999999889\n\nInstead of using == for floating-point comparisons, use all.equal():\n\n# Safe comparison for floating-point numbers\nall.equal(0.1 + 0.2, 0.3)\n#&gt; [1] TRUE\n\n# Returns TRUE if values are nearly equal (within tolerance)\nisTRUE(all.equal(0.1 + 0.2, 0.3))\n#&gt; [1] TRUE\n\n# You can also specify a tolerance\nall.equal(1.0, 1.0001, tolerance = 0.001)\n#&gt; [1] TRUE\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways use all.equal() or check if the difference is within an acceptable tolerance when comparing floating-point numbers. Never rely on == for exact equality of decimal calculations.\n\n\nUsing dplyr::near() for Comparisons\nThe tidyverse provides dplyr::near() as a convenient alternative for checking near-equality, especially when working with data frames:\n\nlibrary(dplyr)\n\n# near() returns TRUE for values within a small tolerance\nnear(0.1 + 0.2, 0.3)\n#&gt; [1] TRUE\n\n# Works well in filter() and mutate()\ndf &lt;- tibble(x = c(0.1 + 0.2, 0.5, 0.3))\ndf |&gt; filter(near(x, 0.3))\n\n\n\n\nx\n\n\n\n0.3\n\n\n0.3\n\n\n\n\n\n\n# You can specify a custom tolerance\nnear(1.0, 1.001, tol = 0.01)\n#&gt; [1] TRUE\n\nUse near() when filtering or comparing numeric columns in data frames, and all.equal() when you need detailed comparison information.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-r-functions",
    "href": "chapters/07-r-programming.html#sec-r-functions",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.8 Functions",
    "text": "7.8 Functions\nFunctions perform operations on inputs and return outputs. R has thousands of built-in functions.\nUsing Functions\n\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Basic statistical functions\nmean(x)\n#&gt; [1] 5.5\nmedian(x)\n#&gt; [1] 5.5\nsd(x)        # Standard deviation\n#&gt; [1] 3.02765\nvar(x)       # Variance\n#&gt; [1] 9.166667\nsum(x)\n#&gt; [1] 55\nlength(x)\n#&gt; [1] 10\nmin(x)\n#&gt; [1] 1\nmax(x)\n#&gt; [1] 10\nrange(x)     # Returns min and max\n#&gt; [1]  1 10\n\nFunction Arguments\nFunctions take arguments that modify their behavior:\n\n# Round to 2 decimal places\nround(3.14159, digits = 2)\n#&gt; [1] 3.14\n\n# Sample with replacement\nsample(1:10, size = 5, replace = TRUE)\n#&gt; [1] 9 4 4 2 7\n\n# Mean with NA handling\nvalues &lt;- c(1, 2, NA, 4, 5)\nmean(values)              # Returns NA\n#&gt; [1] NA\nmean(values, na.rm = TRUE)  # Removes NA first\n#&gt; [1] 3\n\nArgument Order and Naming\nR functions expect arguments in a specific order. When you don’t name arguments, R matches them by position:\n\n# These are equivalent - arguments matched by position\nset.seed(42)\nx1 &lt;- rnorm(5, 0, 10)  # n, mean, sd\nx1\n#&gt; [1] 13.709584 -5.646982  3.631284  6.328626  4.042683\n\n# Named arguments can be in any order\nset.seed(42)\nx2 &lt;- rnorm(sd = 10, n = 5, mean = 0)\nx2\n#&gt; [1] 13.709584 -5.646982  3.631284  6.328626  4.042683\n\n\n\n\n\n\n\nWarning\n\n\n\nRelying on position order can lead to subtle bugs. If you accidentally swap arguments, you may get unexpected results without any error message:\n\n# Oops! We meant 1000 samples with mean=0 and sd=10\n# But we got 10 samples with mean=1000 and sd=0\nwrong &lt;- rnorm(10, 1000, 0)\nwrong  # All identical values!\n#&gt;  [1] 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000\n\nWhen in doubt, name your arguments explicitly for clarity and safety.\n\n\nNesting Functions\nOne powerful feature of R is the ability to nest function calls—using the output of one function as the input to another without creating intermediate variables:\n\n# Without nesting: create intermediate objects\nz &lt;- c(10, 20, 30)\nresult1 &lt;- mean(z)\nresult1\n#&gt; [1] 20\n\n# With nesting: more concise\nresult2 &lt;- mean(c(10, 20, 30))\nresult2\n#&gt; [1] 20\n\n# More complex nesting\n# Calculate the mean of the square roots of 1 through 10\nmean(sqrt(1:10))\n#&gt; [1] 2.246828\n\n# Nested functions are evaluated from inside out\nround(mean(sqrt(c(4, 9, 16, 25))), digits = 2)\n#&gt; [1] 3.5\n\nNesting makes code more compact, but deeply nested expressions can become hard to read. As a rule of thumb, if you find yourself nesting more than 2-3 functions, consider breaking the expression into steps or using the pipe operator (covered in the tidyverse chapter).\nGetting Help\nR has excellent built-in documentation:\n\n# Get help on a function\n?mean\nhelp(mean)\n\n# Search help\nhelp.search(\"correlation\")\n??correlation\n\n# See function arguments\nargs(mean)\n\n# See examples\nexample(mean)\n\n# Find functions containing a string in their name\napropos(\"mean\")  # Returns: \"colMeans\", \"mean\", \"mean.Date\", etc.\n\n# Find functions by partial name matching\napropos(\"cor\")   # Returns all functions containing \"cor\"\n\nVignettes and Demos\nBeyond function-level help, packages often include vignettes—comprehensive tutorials that demonstrate how to use the package:\n\n# List all available vignettes\nvignette()\n\n# List vignettes for a specific package\nvignette(package = \"dplyr\")\n\n# Open a specific vignette\nvignette(\"dplyr\")\n\n# Browse vignettes in your browser\nbrowseVignettes(\"ggplot2\")\n\nSome packages also include demos—interactive demonstrations of functionality:\n\n# List all available demos\ndemo()\n\n# Run a specific demo\ndemo(graphics)    # Base R graphics demonstration\n\n\n\n\n\n\n\nTip\n\n\n\nVignettes are often the best place to start when learning a new package. They provide context and workflows that function documentation alone cannot convey.\n\n\nThe help page structure:\n\n\nDescription: What the function does\n\nUsage: Function syntax\n\nArguments: What inputs it accepts\n\nValue: What it returns\n\nExamples: Working code examples",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-packages",
    "href": "chapters/07-r-programming.html#sec-packages",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.9 Installing and Loading Packages",
    "text": "7.9 Installing and Loading Packages\nBase R includes dozens of useful functions, but as you become a more advanced R user, you’ll need functions for specialized analyses. Fortunately, thousands of additional functions are distributed in the form of R packages.\nInstalling Packages\nPackages from the Comprehensive R Archive Network (CRAN) are easy to install:\n\n# Install a package from CRAN\ninstall.packages(\"dplyr\")\n\n# Install multiple packages at once\ninstall.packages(c(\"ggplot2\", \"readr\", \"tidyr\"))\n\n\n\n\n\n\n\nNote\n\n\n\nPackage names are case-sensitive and must be in quotation marks when installing. You only need to install a package once on your system.\n\n\nLoading Packages\nTo use functions from an installed package, you must load it into your current session:\n\n# Load a package\nlibrary(dplyr)\n\n# Check if a package is installed\ninstalled.packages(\"dplyr\")\n\nUnlike installation, you need to call library() every time you start a new R session. Note that you don’t need quotation marks with library().\n\n\n\n\n\n\nTip\n\n\n\nIt’s good practice to load all required packages at the beginning of your script. This makes dependencies clear and helps others reproduce your work.\n\n\nNamespace Conflicts\nWhen you load multiple packages, function names can collide. The most recently loaded package “wins,” masking functions from earlier packages:\n\n# Loading dplyr after another package\nlibrary(MASS)\nlibrary(dplyr)\n# Warning: The following object is masked from 'package:MASS': select\n\nWhen conflicts occur, you have several options:\n1. Use the package prefix (recommended)\n\n# Explicitly specify which package's function to use\ndplyr::select(data, column1, column2)\nMASS::select(object)\n\n# This works even without loading the package\nstats::filter(x, method = \"convolution\")\n\n2. Control loading order\nLoad packages with conflicting names in the order that gives you the default behavior you want.\n3. Use the conflicted package\n\n# Install and load conflicted\ninstall.packages(\"conflicted\")\nlibrary(conflicted)\n\n# Now conflicts cause errors instead of silent masking\n# You must explicitly choose which function to use:\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"select\", \"dplyr\")\n\n\n\n\n\n\n\nWarning\n\n\n\nThe filter() and lag() functions from dplyr commonly conflict with base R’s stats::filter() and stats::lag(). If your filtering code suddenly stops working, namespace conflicts are often the culprit.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-data-frames",
    "href": "chapters/07-r-programming.html#sec-data-frames",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.10 Data Frames",
    "text": "7.10 Data Frames\nData frames are R’s workhorse for tabular data—think spreadsheets or database tables. Each column is a vector, and columns can have different types.\nCreating Data Frames\n\n# Create vectors\nsample_id &lt;- c(\"S1\", \"S2\", \"S3\", \"S4\", \"S5\")\ntreatment &lt;- c(\"control\", \"drug_A\", \"drug_A\", \"drug_B\", \"drug_B\")\nconcentration &lt;- c(0, 10, 20, 10, 20)\nresponse &lt;- c(1.2, 3.4, 5.6, 2.8, 4.1)\n\n# Combine into data frame\nexperiment &lt;- data.frame(\n  sample_id = sample_id,\n  treatment = treatment,\n  concentration = concentration,\n  response = response\n)\n\nexperiment\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\nS1\ncontrol\n0\n1.2\n\n\nS2\ndrug_A\n10\n3.4\n\n\nS3\ndrug_A\n20\n5.6\n\n\nS4\ndrug_B\n10\n2.8\n\n\nS5\ndrug_B\n20\n4.1\n\n\n\n\n\n\nExamining Data Frames\n\n# Structure\nstr(experiment)\n#&gt; 'data.frame':    5 obs. of  4 variables:\n#&gt;  $ sample_id    : chr  \"S1\" \"S2\" \"S3\" \"S4\" ...\n#&gt;  $ treatment    : chr  \"control\" \"drug_A\" \"drug_A\" \"drug_B\" ...\n#&gt;  $ concentration: num  0 10 20 10 20\n#&gt;  $ response     : num  1.2 3.4 5.6 2.8 4.1\n\n# Dimensions\ndim(experiment)\n#&gt; [1] 5 4\nnrow(experiment)\n#&gt; [1] 5\nncol(experiment)\n#&gt; [1] 4\n\n# Preview\nhead(experiment, 3)\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\nS1\ncontrol\n0\n1.2\n\n\nS2\ndrug_A\n10\n3.4\n\n\nS3\ndrug_A\n20\n5.6\n\n\n\n\n\ntail(experiment, 2)\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n4\nS4\ndrug_B\n10\n2.8\n\n\n5\nS5\ndrug_B\n20\n4.1\n\n\n\n\n\n\n# Summary statistics\nsummary(experiment)\n#&gt;   sample_id          treatment         concentration    response   \n#&gt;  Length:5           Length:5           Min.   : 0    Min.   :1.20  \n#&gt;  Class :character   Class :character   1st Qu.:10    1st Qu.:2.80  \n#&gt;  Mode  :character   Mode  :character   Median :10    Median :3.40  \n#&gt;                                        Mean   :12    Mean   :3.42  \n#&gt;                                        3rd Qu.:20    3rd Qu.:4.10  \n#&gt;                                        Max.   :20    Max.   :5.60\n\n# Column names\nnames(experiment)\n#&gt; [1] \"sample_id\"     \"treatment\"     \"concentration\" \"response\"\ncolnames(experiment)\n#&gt; [1] \"sample_id\"     \"treatment\"     \"concentration\" \"response\"\n\nIn RStudio, you can also use View() to open a spreadsheet-like viewer for interactive exploration:\n\n# Opens data in a viewer pane (RStudio only)\nView(experiment)\n\n\n\n\n\n\n\nTip\n\n\n\nView() is useful for quick inspection, but avoid using it in scripts meant for non-interactive execution. For large datasets, View() can be slow—use head() instead.\n\n\nAccessing Data Frame Elements\n\n# Single column (returns vector)\nexperiment$response\n#&gt; [1] 1.2 3.4 5.6 2.8 4.1\nexperiment[[\"response\"]]\n#&gt; [1] 1.2 3.4 5.6 2.8 4.1\nexperiment[, \"response\"]\n#&gt; [1] 1.2 3.4 5.6 2.8 4.1\n\n# Multiple columns\nexperiment[, c(\"sample_id\", \"response\")]\n\n\n\n\nsample_id\nresponse\n\n\n\nS1\n1.2\n\n\nS2\n3.4\n\n\nS3\n5.6\n\n\nS4\n2.8\n\n\nS5\n4.1\n\n\n\n\n\n\n# Rows by index\nexperiment[1, ]\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\nS1\ncontrol\n0\n1.2\n\n\n\n\nexperiment[1:3, ]\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\nS1\ncontrol\n0\n1.2\n\n\nS2\ndrug_A\n10\n3.4\n\n\nS3\ndrug_A\n20\n5.6\n\n\n\n\n\n\n# Combination\nexperiment[1:3, c(\"treatment\", \"response\")]\n\n\n\n\ntreatment\nresponse\n\n\n\ncontrol\n1.2\n\n\ndrug_A\n3.4\n\n\ndrug_A\n5.6\n\n\n\n\n\n\n# Conditional selection\nexperiment[experiment$treatment == \"drug_A\", ]\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n2\nS2\ndrug_A\n10\n3.4\n\n\n3\nS3\ndrug_A\n20\n5.6\n\n\n\n\n\nexperiment[experiment$response &gt; 3, ]\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n2\nS2\ndrug_A\n10\n3.4\n\n\n3\nS3\ndrug_A\n20\n5.6\n\n\n5\nS5\ndrug_B\n20\n4.1\n\n\n\n\n\n\nAdding and Modifying Columns\n\n# Add a new column\nexperiment$replicate &lt;- c(1, 1, 2, 1, 2)\n\n# Calculated column\nexperiment$log_response &lt;- log(experiment$response)\n\nexperiment\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\nreplicate\nlog_response\n\n\n\nS1\ncontrol\n0\n1.2\n1\n0.1823216\n\n\nS2\ndrug_A\n10\n3.4\n1\n1.2237754\n\n\nS3\ndrug_A\n20\n5.6\n2\n1.7227666\n\n\nS4\ndrug_B\n10\n2.8\n1\n1.0296194\n\n\nS5\ndrug_B\n20\n4.1\n2\n1.4109870",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-lists",
    "href": "chapters/07-r-programming.html#sec-lists",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.11 Lists",
    "text": "7.11 Lists\nLists are flexible containers that can hold objects of different types and lengths. Unlike vectors, which require all elements to be the same type, lists can contain a mix of vectors, data frames, or even other lists.\nCreating Lists\n\n# Create individual vectors\nmeasurements &lt;- c(10, 20, 30, 40, 50)\ncategories &lt;- c(\"high\", \"medium\", \"low\")\nstatus &lt;- factor(c(\"active\", \"inactive\"))\n\n# Combine into a list\nmy_list &lt;- list(\n  values = measurements,\n  labels = categories,\n  status = status\n)\n\nmy_list\n#&gt; $values\n#&gt; [1] 10 20 30 40 50\n#&gt; \n#&gt; $labels\n#&gt; [1] \"high\"   \"medium\" \"low\"   \n#&gt; \n#&gt; $status\n#&gt; [1] active   inactive\n#&gt; Levels: active inactive\n\n# Check the structure\nstr(my_list)\n#&gt; List of 3\n#&gt;  $ values: num [1:5] 10 20 30 40 50\n#&gt;  $ labels: chr [1:3] \"high\" \"medium\" \"low\"\n#&gt;  $ status: Factor w/ 2 levels \"active\",\"inactive\": 1 2\n\nIndexing Lists\nLists use double square brackets [[]] to access individual elements:\n\n# Access by position\nmy_list[[1]]\n#&gt; [1] 10 20 30 40 50\n\n# Access by name\nmy_list[[\"labels\"]]\n#&gt; [1] \"high\"   \"medium\" \"low\"\n\n# Access using $ notation\nmy_list$values\n#&gt; [1] 10 20 30 40 50\n\n# Access element within a list component\nmy_list[[1]][3]  # Third element of first component\n#&gt; [1] 30\n\n\n\n\n\n\n\nNote\n\n\n\nSingle brackets [] return a sublist, while double brackets [[]] extract the actual element. This distinction matters when working with list components.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-matrices",
    "href": "chapters/07-r-programming.html#sec-matrices",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.12 Matrices",
    "text": "7.12 Matrices\nMatrices are two-dimensional arrays where all elements must be the same type. Unlike data frames, matrices don’t have named columns by default and are optimized for mathematical operations.\nCreating Matrices\n\n# Create a matrix from a vector\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\nmat\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    4    7   10\n#&gt; [2,]    2    5    8   11\n#&gt; [3,]    3    6    9   12\n\n# Create with row-wise filling\nmatrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\n#&gt;      [,1] [,2] [,3] [,4]\n#&gt; [1,]    1    2    3    4\n#&gt; [2,]    5    6    7    8\n#&gt; [3,]    9   10   11   12\n\n# From vectors using cbind (column bind) or rbind (row bind)\ncol1 &lt;- c(1, 2, 3)\ncol2 &lt;- c(4, 5, 6)\ncbind(col1, col2)\n#&gt;      col1 col2\n#&gt; [1,]    1    4\n#&gt; [2,]    2    5\n#&gt; [3,]    3    6\n\nrbind(col1, col2)\n#&gt;      [,1] [,2] [,3]\n#&gt; col1    1    2    3\n#&gt; col2    4    5    6\n\nMatrix Operations\n\n# Dimensions\ndim(mat)\n#&gt; [1] 3 4\nnrow(mat)\n#&gt; [1] 3\nncol(mat)\n#&gt; [1] 4\n\n# Transpose (swap rows and columns)\nt(mat)\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,]    1    2    3\n#&gt; [2,]    4    5    6\n#&gt; [3,]    7    8    9\n#&gt; [4,]   10   11   12\n\n# Indexing: [row, column]\nmat[1, 2]      # First row, second column\n#&gt; [1] 4\nmat[1, ]       # Entire first row\n#&gt; [1]  1  4  7 10\nmat[, 2]       # Entire second column\n#&gt; [1] 4 5 6\nmat[1:2, 2:3]  # Submatrix\n#&gt;      [,1] [,2]\n#&gt; [1,]    4    7\n#&gt; [2,]    5    8\n\nMatrices are particularly useful for linear algebra operations and when you need efficient numerical computations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-reading-writing",
    "href": "chapters/07-r-programming.html#sec-reading-writing",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.13 Reading and Writing Data",
    "text": "7.13 Reading and Writing Data\nReading Files\n\n# CSV files (comma-separated)\ndata &lt;- read.csv(\"data.csv\")\n\n# Tab-separated files\ndata &lt;- read.table(\"data.tsv\", header = TRUE, sep = \"\\t\")\n\n# Excel files (requires readxl package)\nlibrary(readxl)\ndata &lt;- read_excel(\"data.xlsx\")\n\n# Specify options\ndata &lt;- read.csv(\"data.csv\",\n                 header = TRUE,\n                 stringsAsFactors = FALSE,\n                 na.strings = c(\"\", \"NA\", \"N/A\"))\n\nWriting Files\n\n# CSV output\nwrite.csv(experiment, \"experiment_results.csv\", row.names = FALSE)\n\n# Tab-separated\nwrite.table(experiment, \"results.tsv\", \n            sep = \"\\t\", \n            quote = FALSE, \n            row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-basic-viz",
    "href": "chapters/07-r-programming.html#sec-basic-viz",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.14 Basic Visualization",
    "text": "7.14 Basic Visualization\nR excels at creating graphics. Here’s a quick introduction to base R plotting:\nScatter Plots\n\nx &lt;- 1:10\ny &lt;- x^2\n\nplot(x, y, \n     main = \"Quadratic Relationship\",\n     xlab = \"X values\",\n     ylab = \"Y values\",\n     col = \"darkblue\",\n     pch = 16)  # Filled circles\n\n\n\n\n\n\nFigure 7.4: Basic scatter plot showing quadratic relationship\n\n\n\n\nHistograms\n\n# Generate random data\ndata &lt;- rnorm(1000, mean = 50, sd = 10)\n\nhist(data,\n     main = \"Distribution of Values\",\n     xlab = \"Value\",\n     col = \"steelblue\",\n     breaks = 30)\n\n\n\n\n\n\nFigure 7.5: Histogram of normally distributed data\n\n\n\n\nBox Plots\n\n# Create sample data\nset.seed(42)\ncontrol &lt;- rnorm(30, mean = 10, sd = 2)\ntreatment &lt;- rnorm(30, mean = 15, sd = 3)\n\nboxplot(control, treatment,\n        names = c(\"Control\", \"Treatment\"),\n        main = \"Treatment Effect\",\n        ylab = \"Response\",\n        col = c(\"lightblue\", \"lightcoral\"))\n\n\n\n\n\n\nFigure 7.6: Box plot comparing treatment groups\n\n\n\n\nMultiple Plots\n\n# Create 2x2 layout\npar(mfrow = c(2, 2))\n\n# Four different plots\nplot(1:10, (1:10)^2, type = \"l\", main = \"Line Plot\")\nhist(rnorm(100), main = \"Histogram\")\nboxplot(rnorm(50), main = \"Box Plot\")\nbarplot(c(3, 5, 2, 7), names.arg = c(\"A\", \"B\", \"C\", \"D\"), main = \"Bar Plot\")\n\n# Reset to single plot\npar(mfrow = c(1, 1))\n\n\n\n\n\n\nFigure 7.7: Multiple plots in a single figure\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor publication-quality graphics, explore the ggplot2 package (Wickham et al., 2023), which provides a powerful and consistent grammar of graphics.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-split-apply-combine",
    "href": "chapters/07-r-programming.html#sec-split-apply-combine",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.15 The Split-Apply-Combine Approach",
    "text": "7.15 The Split-Apply-Combine Approach\nA common pattern in data analysis is to split data by groups, apply a function to each group, and combine the results. This split-apply-combine workflow appears repeatedly in scientific computing.\nUsing tapply() for Grouped Operations\nThe tapply() function applies a function to subsets of a vector, split by a factor:\n\n# Sample data\nvalues &lt;- c(23, 45, 67, 34, 56, 78, 12, 89)\ngroups &lt;- factor(c(\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"C\", \"C\"))\n\n# Mean by group\ntapply(values, groups, mean)\n#&gt;    A    B    C \n#&gt; 45.0 56.0 50.5\n\n# Standard deviation by group\ntapply(values, groups, sd)\n#&gt;        A        B        C \n#&gt; 22.00000 22.00000 54.44722\n\n# Custom summary: range\ntapply(values, groups, function(x) max(x) - min(x))\n#&gt;  A  B  C \n#&gt; 44 44 77\n\nUsing aggregate() for Data Frames\nFor data frames, aggregate() summarizes multiple columns at once:\n\n# Using built-in iris data\nhead(iris)\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n# Mean of all numeric columns by Species\naggregate(. ~ Species, data = iris, FUN = mean)\n\n\n\n\nSpecies\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n# Multiple statistics (result contains a matrix column)\nagg_result &lt;- aggregate(Sepal.Length ~ Species, data = iris,\n                        FUN = function(x) c(mean = mean(x), sd = sd(x)))\n\n# Convert to a regular data frame for display\ndo.call(data.frame, agg_result)\n\n\n\n\nSpecies\nSepal.Length.mean\nSepal.Length.sd\n\n\n\nsetosa\n5.006\n0.3524897\n\n\nversicolor\n5.936\n0.5161711\n\n\nvirginica\n6.588\n0.6358796\n\n\n\n\n\n\nThe apply() Family\nR provides several related functions for different data structures:\n\n\nTable 7.3: The apply family of functions\n\n\n\nFunction\nInput\nApplies Over\n\n\n\napply()\nMatrix/data frame\nRows or columns\n\n\nlapply()\nList\nEach element, returns list\n\n\nsapply()\nList\nEach element, returns vector\n\n\ntapply()\nVector + factor\nGroups defined by factor\n\n\nmapply()\nMultiple vectors\nCorresponding elements\n\n\n\n\n\n\n\n# Apply to matrix columns\nmat &lt;- matrix(1:12, nrow = 3)\napply(mat, 2, sum)  # Column sums (MARGIN = 2)\n#&gt; [1]  6 15 24 33\napply(mat, 1, sum)  # Row sums (MARGIN = 1)\n#&gt; [1] 22 26 30\n\n# Apply to each element of a list\nmy_list &lt;- list(a = 1:5, b = 10:15, c = 100:110)\nlapply(my_list, mean)\n#&gt; $a\n#&gt; [1] 3\n#&gt; \n#&gt; $b\n#&gt; [1] 12.5\n#&gt; \n#&gt; $c\n#&gt; [1] 105\nsapply(my_list, mean)  # Simplified output\n#&gt;     a     b     c \n#&gt;   3.0  12.5 105.0\n\n\n\n\n\n\n\nTip\n\n\n\nThe tidyverse packages (covered in the next chapter) provide more intuitive alternatives like group_by() and summarize() for this workflow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-programming-constructs",
    "href": "chapters/07-r-programming.html#sec-programming-constructs",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.16 Basic Programming Constructs",
    "text": "7.16 Basic Programming Constructs\nAs you develop your R skills, you’ll encounter situations requiring conditional logic and iteration.\nConditional Statements with ifelse()\n\nThe ifelse() function provides vectorized conditional logic:\n\n# Basic ifelse\nscores &lt;- c(85, 72, 91, 68, 79)\nifelse(scores &gt;= 80, \"Pass\", \"Fail\")\n#&gt; [1] \"Pass\" \"Fail\" \"Pass\" \"Fail\" \"Fail\"\n\n# Nested conditions\ngrades &lt;- ifelse(scores &gt;= 90, \"A\",\n           ifelse(scores &gt;= 80, \"B\",\n             ifelse(scores &gt;= 70, \"C\", \"F\")))\ngrades\n#&gt; [1] \"B\" \"C\" \"A\" \"F\" \"C\"\n\n# Useful for creating indicator variables\ntreatment &lt;- c(\"control\", \"drug\", \"drug\", \"control\", \"drug\")\ncolors &lt;- ifelse(treatment == \"drug\", \"red\", \"blue\")\n\nType-Safe Conditionals with if_else()\n\nThe tidyverse provides dplyr::if_else(), which is stricter than base R’s ifelse():\n\nlibrary(dplyr)\n\nx &lt;- c(-2, -1, 0, 1, 2)\n\n# if_else() requires matching types for true/false\nif_else(x &gt; 0, \"positive\", \"non-positive\")\n#&gt; [1] \"non-positive\" \"non-positive\" \"non-positive\" \"positive\"     \"positive\"\n\n# Handles NA explicitly with the 'missing' argument\ny &lt;- c(1, NA, 3, NA, 5)\nif_else(y &gt; 2, \"high\", \"low\", missing = \"unknown\")\n#&gt; [1] \"low\"     \"unknown\" \"high\"    \"unknown\" \"high\"\n\n# Creates a simple absolute value implementation\nif_else(x &lt; 0, -x, x)\n#&gt; [1] 2 1 0 1 2\n\n\n\n\n\n\n\nTip\n\n\n\nUse dplyr::if_else() over base ifelse() when:\n\nYou want type checking (prevents accidentally mixing numbers and strings)\nYou need explicit control over NA handling\nYou’re already using tidyverse functions\n\n\n\nFor Loops\nWhile R is optimized for vectorized operations, loops are sometimes necessary:\n\n# Simple loop\nfor (i in 1:5) {\n  print(i^2)\n}\n#&gt; [1] 1\n#&gt; [1] 4\n#&gt; [1] 9\n#&gt; [1] 16\n#&gt; [1] 25\n\n# Loop with pre-allocated output (recommended for efficiency)\nn &lt;- 10\nresults &lt;- numeric(n)  # Pre-allocate\nfor (i in 1:n) {\n  results[i] &lt;- i * 2\n}\nresults\n#&gt;  [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\n\n\n\n\nWarning\n\n\n\nIn R, loops are often slower than vectorized alternatives. Before writing a loop, consider whether a vectorized function (apply(), tapply(), etc.) or built-in operation would work instead.\n\n\nWriting Simple Functions\nYou can create your own functions for repeated tasks:\n\n# Define a function\ncalculate_cv &lt;- function(x) {\n  # Coefficient of variation: SD / mean * 100\n  cv &lt;- sd(x) / mean(x) * 100\n  return(cv)\n}\n\n# Use the function\ndata &lt;- c(10, 12, 11, 13, 9, 14)\ncalculate_cv(data)\n#&gt; [1] 16.26808\n\n# Function with multiple arguments and default values\nsummarize_data &lt;- function(x, digits = 2) {\n  result &lt;- c(\n    mean = round(mean(x), digits),\n    sd = round(sd(x), digits),\n    n = length(x)\n  )\n  return(result)\n}\n\nsummarize_data(data)\n#&gt;  mean    sd     n \n#&gt; 11.50  1.87  6.00\nsummarize_data(data, digits = 3)\n#&gt;   mean     sd      n \n#&gt; 11.500  1.871  6.000",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-random-sampling",
    "href": "chapters/07-r-programming.html#sec-random-sampling",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.17 Random Sampling and Simulation",
    "text": "7.17 Random Sampling and Simulation\nR makes it easy to generate random data and run simulations:\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Random samples from distributions\nuniform_sample &lt;- runif(100, min = 0, max = 1)\nnormal_sample &lt;- rnorm(1000, mean = 0, sd = 1)\npoisson_sample &lt;- rpois(100, lambda = 5)\n\n# Visualize normal distribution\nhist(normal_sample, \n     probability = TRUE,  # Density instead of counts\n     main = \"Sample vs. Theoretical Distribution\",\n     col = \"lightblue\")\n\n# Add theoretical curve\ncurve(dnorm(x, mean = 0, sd = 1), \n      add = TRUE, \n      col = \"red\", \n      lwd = 2)\n\n\n\n\n\n\nFigure 7.8: Simulated data from a normal distribution with theoretical curve overlay\n\n\n\n\nCommon Distribution Functions\nFor distribution xxx (e.g., norm, unif, pois, binom):\n\n\nrxxx() — Random samples\n\ndxxx() — Density/probability\n\npxxx() — Cumulative distribution function\n\nqxxx() — Quantile function\n\n\n# Normal distribution\nrnorm(5)        # 5 random values\n#&gt; [1]  0.9159921  0.8006224 -0.9365690 -1.4007874  0.1602775\ndnorm(0)        # Density at x=0\n#&gt; [1] 0.3989423\npnorm(1.96)     # P(X ≤ 1.96)\n#&gt; [1] 0.9750021\nqnorm(0.975)    # Value where P(X ≤ x) = 0.975\n#&gt; [1] 1.959964\n\nRepeated Simulations with replicate()\n\nThe replicate() function repeats an expression multiple times and collects the results—perfect for simulations:\n\n# Shuffle integers 1-10 five times\nset.seed(42)\nreplicate(5, sample(1:10, size = 10, replace = FALSE))\n#&gt;       [,1] [,2] [,3] [,4] [,5]\n#&gt;  [1,]    1    8    9    3    5\n#&gt;  [2,]    5    7   10    1    4\n#&gt;  [3,]   10    4    3    2    2\n#&gt;  [4,]    8    1    4    6    8\n#&gt;  [5,]    2    5    5   10    3\n#&gt;  [6,]    4   10    6    8    1\n#&gt;  [7,]    6    2    1    4   10\n#&gt;  [8,]    9    6    2    5    7\n#&gt;  [9,]    7    9    8    7    6\n#&gt; [10,]    3    3    7    9    9\n\n# Simulate sampling distributions\n# Take 1000 samples of size 30 and calculate mean of each\nset.seed(123)\nsample_means &lt;- replicate(1000, mean(rnorm(30, mean = 100, sd = 15)))\nhist(sample_means, main = \"Distribution of Sample Means\",\n     col = \"lightgreen\", xlab = \"Sample Mean\")\n\n\n\n\n\n\n\nThe replicate() function belongs to the “apply” family and returns a matrix (if results are vectors of equal length) or a list (if results vary in length).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-numeric-transformations",
    "href": "chapters/07-r-programming.html#sec-numeric-transformations",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.18 Useful Numeric Transformations",
    "text": "7.18 Useful Numeric Transformations\nR and the tidyverse provide many functions for transforming numeric data. These are particularly useful within mutate() operations.\nCumulative and Running Aggregates\nBase R provides functions for running (cumulative) calculations:\n\nx &lt;- c(1, 2, 3, 4, 5)\n\ncumsum(x)   # Running sum: 1, 3, 6, 10, 15\n#&gt; [1]  1  3  6 10 15\ncumprod(x)  # Running product: 1, 2, 6, 24, 120\n#&gt; [1]   1   2   6  24 120\ncummax(x)   # Running maximum\n#&gt; [1] 1 2 3 4 5\ncummin(x)   # Running minimum\n#&gt; [1] 1 1 1 1 1\n\n# Useful for time series data\ndaily_cases &lt;- c(10, 15, 8, 22, 18)\ncumsum(daily_cases)  # Total cases over time\n#&gt; [1] 10 25 33 55 73\n\nOffsets with lead() and lag()\n\nThe dplyr::lead() and dplyr::lag() functions let you reference values before or after the current position:\n\nlibrary(dplyr)\n\nx &lt;- c(10, 20, 30, 40, 50)\n\nlag(x)   # Previous value: NA, 10, 20, 30, 40\n#&gt; [1] NA 10 20 30 40\nlead(x)  # Next value: 20, 30, 40, 50, NA\n#&gt; [1] 20 30 40 50 NA\n\n# Calculate differences from previous value\nx - lag(x)\n#&gt; [1] NA 10 10 10 10\n\n# Detect changes\nx != lag(x)\n#&gt; [1]   NA TRUE TRUE TRUE TRUE\n\n# Lag by more than one position\nlag(x, n = 2)\n#&gt; [1] NA NA 10 20 30\n\nThis is invaluable for time series analysis, detecting changes, and calculating growth rates.\nRanking Values\ndplyr provides several ranking functions:\n\nx &lt;- c(5, 1, 3, 2, 2, NA)\n\nmin_rank(x)      # Standard competition ranking (1, 2, 2, 4)\n#&gt; [1]  5  1  4  2  2 NA\ndense_rank(x)    # No gaps after ties (1, 2, 2, 3)\n#&gt; [1]  4  1  3  2  2 NA\nrow_number(x)    # Unique ranks (ties broken by position)\n#&gt; [1]  5  1  4  2  3 NA\n\n# Rank in descending order\nmin_rank(desc(x))\n#&gt; [1]  1  5  2  3  3 NA\n\n# Practical example: find top 3 values\ndf &lt;- tibble(gene = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n             expression = c(5.2, 8.1, 3.4, 9.7, 6.5))\ndf |&gt; filter(min_rank(desc(expression)) &lt;= 3)\n\n\n\n\ngene\nexpression\n\n\n\nB\n8.1\n\n\nD\n9.7\n\n\nE\n6.5\n\n\n\n\n\n\nBinning Numbers with cut()\n\nThe cut() function divides continuous data into discrete bins:\n\nages &lt;- c(15, 25, 35, 45, 55, 65, 75)\n\n# Create age groups\ncut(ages, breaks = c(0, 18, 35, 55, 100))\n#&gt; [1] (0,18]   (18,35]  (18,35]  (35,55]  (35,55]  (55,100] (55,100]\n#&gt; Levels: (0,18] (18,35] (35,55] (55,100]\n\n# Custom labels\ncut(ages,\n    breaks = c(0, 18, 35, 55, 100),\n    labels = c(\"minor\", \"young_adult\", \"middle_aged\", \"senior\"))\n#&gt; [1] minor       young_adult young_adult middle_aged middle_aged senior     \n#&gt; [7] senior     \n#&gt; Levels: minor young_adult middle_aged senior\n\n# Include lowest value and control interval direction\ncut(ages, breaks = c(0, 18, 35, 55, 100),\n    include.lowest = TRUE, right = FALSE)\n#&gt; [1] [0,18)   [18,35)  [35,55)  [35,55)  [55,100] [55,100] [55,100]\n#&gt; Levels: [0,18) [18,35) [35,55) [55,100]\n\nThis is particularly useful for creating categorical variables from continuous measurements for analysis or visualization.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-cleanup",
    "href": "chapters/07-r-programming.html#sec-cleanup",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.19 Cleaning Up Your Environment",
    "text": "7.19 Cleaning Up Your Environment\nAs you work in R, your environment accumulates objects. Periodically cleaning up helps manage memory and avoid confusion.\nRemoving Objects\nUse rm() to remove objects from your environment:\n\n# Remove a single object\nrm(x)\n\n# Remove multiple objects\nrm(x, y, z)\n\n# Remove all objects (use with caution!)\nrm(list = ls())\n\n\n\n\n\n\n\nWarning\n\n\n\nrm(list = ls()) removes everything in your environment. Use it intentionally, typically at the start of a script to ensure a clean slate, but never in a shared function or package.\n\n\nClosing Graphics Devices\nWhen working with plots, graphics devices can accumulate. Use dev.off() to close them:\n\n# Close the current graphics device\ndev.off()\n\n# Close all graphics devices\ngraphics.off()\n\n# List open devices\ndev.list()\n\nThis is especially important when saving plots to files—if you don’t close the device, the file may not be properly saved:\n\n# Save a plot to PDF\npdf(\"my_plot.pdf\")\nplot(1:10, (1:10)^2)\ndev.off()  # Essential! Closes the file and finalizes it\n\n# Similarly for PNG\npng(\"my_plot.png\", width = 800, height = 600)\nplot(1:10, (1:10)^2)\ndev.off()\n\nChecking Your Environment\n\n# List all objects in the environment\nls()\n\n# List objects matching a pattern\nls(pattern = \"data\")\n\n# Check if an object exists\nexists(\"my_variable\")\n\n# See environment size\nobject.size(x)  # Size of one object",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#sec-r-summary",
    "href": "chapters/07-r-programming.html#sec-r-summary",
    "title": "7  R Programming Fundamentals",
    "section": "\n7.20 Summary",
    "text": "7.20 Summary\nThis chapter introduced R programming fundamentals:\n\nR is designed for statistical computing with excellent graphics\nEverything in R is an object with a class and type; use class(), typeof(), and str() to explore objects\nRStudio provides an integrated development environment\nVariables are assigned with &lt;-; R is case-sensitive; avoid reserved words and shadowing built-in functions\nVectors are the fundamental data structure; operations are vectorized\nLogical operators (&, |, !, %in%) and comparison operators enable conditional logic\nFloating-point comparisons require all.equal() instead of ==\n\nData frames hold tabular data with columns of different types\nLists can contain elements of different types and lengths\nMatrices are optimized for numerical computations\nFunctions perform operations; use ?function, vignettes, and demos for help\nPackages extend R’s capabilities; watch for namespace conflicts when loading multiple packages\nThe split-apply-combine pattern (tapply(), aggregate(), apply()) is fundamental to data analysis\nConditional logic (ifelse()) and loops enable programmatic control\nWriting custom functions allows you to encapsulate repeated operations\nR reads/writes CSV, TSV, and Excel files easily\nBasic plotting is built-in; ggplot2 offers advanced graphics\nUse rm() and dev.off() to clean up your environment and graphics devices\n\nThese fundamentals prepare you for more advanced R programming, including the tidyverse packages covered in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#exercises",
    "href": "chapters/07-r-programming.html#exercises",
    "title": "7  R Programming Fundamentals",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Vector Operations\n\nCreate a vector of the first 20 integers\nCalculate the mean, median, and standard deviation\nCreate a new vector containing only the even numbers\nCalculate the sum of squares\n\nExercise 2: Data Frame Practice\nCreate a data frame representing an experiment: 1. Include columns for: sample_id, group (control/treatment), measurement 2. Add 10 rows of sample data 3. Calculate the mean measurement for each group 4. Add a new column with log-transformed measurements\nExercise 3: Lists and Matrices\n\nCreate a list containing three components: a numeric vector, a character vector, and a data frame\nUse indexing to extract the second element of the first component\nCreate a 4x4 matrix filled with the numbers 1-16\nCalculate row sums and column means using apply()\n\n\nExercise 4: Split-Apply-Combine\nUsing the built-in mtcars dataset: 1. Calculate the mean mpg for each number of cylinders (cyl) using tapply() 2. Use aggregate() to find the mean and standard deviation of hp by cyl 3. Write a custom function that returns the range (max - min) of a vector 4. Apply your function to find the range of mpg for each gear type\nExercise 5: Programming Practice\n\nUse ifelse() to create a new vector that categorizes mtcars$mpg as “efficient” (≥25) or “inefficient” (&lt;25)\nWrite a for loop that calculates the cumulative sum of the first 10 integers\nCreate a function called standardize() that takes a vector and returns z-scores: (x - mean) / sd\nTest your function on a vector of your choice\n\nExercise 6: Random Simulation\n\nGenerate 1000 random samples from a normal distribution with mean=100 and sd=15\nCreate a histogram of the data\nCalculate what proportion of values fall between 85 and 115\nCompare to the theoretical proportion for a normal distribution\n\nExercise 7: File I/O\n\nCreate a data frame with experimental data\nSave it as a CSV file\nRead it back into a new variable\nVerify the data matches the original",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#additional-resources",
    "href": "chapters/07-r-programming.html#additional-resources",
    "title": "7  R Programming Fundamentals",
    "section": "Additional Resources",
    "text": "Additional Resources\nOfficial Documentation and Tutorials\n\n\nThe R Project Homepage — Official R website with downloads and documentation\n\nAn Introduction to R — Comprehensive official manual\n\nR for Data Science — Free online book covering the tidyverse approach\n\nQuick References\n\n\nQuick-R — Concise reference for common R tasks\n\nRStudio Cheat Sheets — One-page references for popular packages\n\nBioinformatics-Specific\n\n\nBioconductor — R packages for bioinformatics and computational biology\n\nA Primer for Computational Biology — O’Neil, S.T. 2017\n\nRecommended Books\n\nLogan, M. 2010. Biostatistical Design and Analysis Using R — Excellent introduction to R for statistical analysis in the life sciences\n\n\n\n\n\nR Core Team (2024). R: A language and environment for statistical computing.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html",
    "href": "chapters/08-tidy-data.html",
    "title": "8  Tidy Data Principles",
    "section": "",
    "text": "8.1 Why Data Organization Matters\nBefore you can analyze data, you must organize it. The structure of your data profoundly affects how easily you can work with it. A well-organized dataset can be analyzed in minutes; a poorly organized one might require hours of preprocessing.\nConsider this scenario: you’ve completed an experiment and collected data in a spreadsheet. Now you need to analyze it. If your data is organized consistently and logically, analysis is straightforward. If it’s scattered across multiple tabs with inconsistent formatting, merged cells, and color-coded values, you’re in for a frustrating experience.\nTidy data provides a standard way to organize data that makes analysis easier. It’s not the only valid way to structure data, but it’s particularly well-suited for analysis in R and Python.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tidy-principles",
    "href": "chapters/08-tidy-data.html#sec-tidy-principles",
    "title": "8  Tidy Data Principles",
    "section": "\n8.2 The Three Principles of Tidy Data",
    "text": "8.2 The Three Principles of Tidy Data\nHadley Wickham formalized the concept of tidy data in a influential paper (Wickham, 2014). Tidy data follows three rules:\n\n\n\n\n\n\nThe Three Rules of Tidy Data\n\n\n\n\nEach variable forms a column\nEach observation forms a row\nEach value has its own cell\n\n\n\nLet’s unpack what these mean.\nVariables as Columns\nA variable is a characteristic that you measure, observe, or categorize. Examples:\n\nGene expression level\nTreatment group\nPatient age\nSample ID\nMeasurement date\n\nEach variable should have its own column with a descriptive header.\nObservations as Rows\nAn observation is a single unit of data collection—typically one measurement from one subject at one time point. Each observation gets its own row.\nValues in Cells\nEach cell contains exactly one value. No merged cells, no multiple values crammed together, no implicit values indicated by color or formatting.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tidy-examples",
    "href": "chapters/08-tidy-data.html#sec-tidy-examples",
    "title": "8  Tidy Data Principles",
    "section": "\n8.3 Tidy vs. Messy Data: Examples",
    "text": "8.3 Tidy vs. Messy Data: Examples\nA Tidy Dataset\n\ntidy_data &lt;- tibble(\n  sample_id = c(\"S001\", \"S002\", \"S003\", \"S001\", \"S002\", \"S003\"),\n  timepoint = c(\"0h\", \"0h\", \"0h\", \"24h\", \"24h\", \"24h\"),\n  gene = rep(\"BRCA1\", 6),\n  expression = c(4.2, 3.8, 4.5, 8.1, 7.9, 9.2)\n)\n\ntidy_data |&gt;\n  gt() |&gt;\n  tab_header(title = \"Gene Expression Data (Tidy Format)\")\n\n\nTable 8.1: Example of tidy data organization\n\n\n\n\n\n\n\nGene Expression Data (Tidy Format)\n\n\nsample_id\ntimepoint\ngene\nexpression\n\n\n\n\nS001\n0h\nBRCA1\n4.2\n\n\nS002\n0h\nBRCA1\n3.8\n\n\nS003\n0h\nBRCA1\n4.5\n\n\nS001\n24h\nBRCA1\n8.1\n\n\nS002\n24h\nBRCA1\n7.9\n\n\nS003\n24h\nBRCA1\n9.2\n\n\n\n\n\n\n\n\n\nThis is tidy because:\n\nEach column is a variable (sample_id, timepoint, gene, expression)\nEach row is an observation (one expression measurement)\nEach cell has one value\nA Messy Dataset (Same Data)\nA common messy format spreads measurements across columns:\n\nmessy_data &lt;- tibble(\n  sample_id = c(\"S001\", \"S002\", \"S003\"),\n  `BRCA1_0h` = c(4.2, 3.8, 4.5),\n  `BRCA1_24h` = c(8.1, 7.9, 9.2)\n)\n\nmessy_data |&gt;\n  gt() |&gt;\n  tab_header(title = \"Gene Expression Data (Messy Format)\")\n\n\nTable 8.2: The same data in messy (wide) format\n\n\n\n\n\n\n\nGene Expression Data (Messy Format)\n\n\nsample_id\nBRCA1_0h\nBRCA1_24h\n\n\n\n\nS001\n4.2\n8.1\n\n\nS002\n3.8\n7.9\n\n\nS003\n4.5\n9.2\n\n\n\n\n\n\n\n\n\nThis is messy because:\n\nTime point and gene are embedded in column names\nEach row represents multiple observations\nAdding genes or timepoints requires adding columns\nWhy It Matters\nWith tidy data, operations are straightforward:\n\n# Calculate mean expression by timepoint\ntidy_data |&gt;\n  group_by(timepoint) |&gt;\n  summarize(mean_expression = mean(expression))\n\n\n\n\ntimepoint\nmean_expression\n\n\n\n0h\n4.166667\n\n\n24h\n8.400000\n\n\n\n\n\n\n# Easy to filter and plot\ntidy_data |&gt;\n  filter(timepoint == \"24h\")\n\n\n\n\nsample_id\ntimepoint\ngene\nexpression\n\n\n\nS001\n24h\nBRCA1\n8.1\n\n\nS002\n24h\nBRCA1\n7.9\n\n\nS003\n24h\nBRCA1\n9.2\n\n\n\n\n\n\nWith messy data, you must first reshape it—or write complex code to work around the structure.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-data-problems",
    "href": "chapters/08-tidy-data.html#sec-data-problems",
    "title": "8  Tidy Data Principles",
    "section": "\n8.4 Common Data Problems",
    "text": "8.4 Common Data Problems\nProblem 1: Column Headers Are Values\nOften, column names contain data values rather than variable names:\n\n# Messy: years in column headers\npopulation_messy &lt;- tibble(\n  country = c(\"USA\", \"Canada\", \"Mexico\"),\n  `2020` = c(331, 38, 129),\n  `2021` = c(332, 38, 130),\n  `2022` = c(333, 39, 131)\n)\n\npopulation_messy |&gt; gt()\n\n\n\n\n\ncountry\n2020\n2021\n2022\n\n\n\nUSA\n331\n332\n333\n\n\nCanada\n38\n38\n39\n\n\nMexico\n129\n130\n131\n\n\n\n\n\n\nSolution: “Pivot” the data to create a year column:\n\npopulation_tidy &lt;- population_messy |&gt;\n  pivot_longer(\n    cols = `2020`:`2022`,\n    names_to = \"year\",\n    values_to = \"population\"\n  )\n\npopulation_tidy |&gt; gt()\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\nUSA\n2020\n331\n\n\nUSA\n2021\n332\n\n\nUSA\n2022\n333\n\n\nCanada\n2020\n38\n\n\nCanada\n2021\n38\n\n\nCanada\n2022\n39\n\n\nMexico\n2020\n129\n\n\nMexico\n2021\n130\n\n\nMexico\n2022\n131\n\n\n\n\n\n\nProblem 2: Multiple Variables in One Column\nSometimes multiple pieces of information are crammed into one cell:\n\n# Messy: rate contains both values\nmessy_rates &lt;- tibble(\n  country = c(\"USA\", \"Canada\", \"Mexico\"),\n  rate = c(\"1000/50000\", \"200/10000\", \"500/25000\")\n)\n\nmessy_rates |&gt; gt()\n\n\n\n\n\ncountry\nrate\n\n\n\nUSA\n1000/50000\n\n\nCanada\n200/10000\n\n\nMexico\n500/25000\n\n\n\n\n\n\nSolution: Separate into distinct columns:\n\ntidy_rates &lt;- messy_rates |&gt;\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\", convert = TRUE)\n\ntidy_rates |&gt; gt()\n\n\n\n\n\ncountry\ncases\npopulation\n\n\n\nUSA\n1000\n50000\n\n\nCanada\n200\n10000\n\n\nMexico\n500\n25000\n\n\n\n\n\n\nProblem 3: Variables in Both Rows and Columns\nComplex tables sometimes mix variables across dimensions:\n\n# Messy: measurement type in rows, conditions in columns\nmessy_experiment &lt;- tibble(\n  measurement = c(\"weight\", \"height\", \"weight\", \"height\"),\n  subject = c(\"A\", \"A\", \"B\", \"B\"),\n  control = c(70, 170, 65, 165),\n  treatment = c(68, 170, 64, 166)\n)\n\nmessy_experiment |&gt; gt()\n\n\n\n\n\nmeasurement\nsubject\ncontrol\ntreatment\n\n\n\nweight\nA\n70\n68\n\n\nheight\nA\n170\n170\n\n\nweight\nB\n65\n64\n\n\nheight\nB\n165\n166\n\n\n\n\n\n\nThis requires multiple steps to tidy: pivot longer, then pivot wider.\nProblem 4: Multiple Observational Units\nWhen a single table contains data about different types of things:\n\n# Messy: patient and hospital info in same table\ncombined_data &lt;- tibble(\n  patient_id = c(\"P001\", \"P002\"),\n  patient_name = c(\"Alice\", \"Bob\"),\n  hospital_id = c(\"H1\", \"H1\"),\n  hospital_name = c(\"General\", \"General\"),\n  hospital_beds = c(500, 500),  # Repeated!\n  diagnosis = c(\"Flu\", \"Cold\")\n)\n\ncombined_data |&gt; gt()\n\n\n\n\n\npatient_id\npatient_name\nhospital_id\nhospital_name\nhospital_beds\ndiagnosis\n\n\n\nP001\nAlice\nH1\nGeneral\n500\nFlu\n\n\nP002\nBob\nH1\nGeneral\n500\nCold\n\n\n\n\n\n\nSolution: Normalize into separate tables linked by keys.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-data-best-practices",
    "href": "chapters/08-tidy-data.html#sec-data-best-practices",
    "title": "8  Tidy Data Principles",
    "section": "\n8.5 Best Practices for Data Organization",
    "text": "8.5 Best Practices for Data Organization\nNaming Conventions\n\n\n\n\n\n\nData File Naming Rules\n\n\n\nFile names:\n\nUse descriptive names: patient_outcomes_2024.csv\n\nInclude dates in ISO format: YYYY-MM-DD\n\nAvoid spaces (use underscores or hyphens)\nKeep names concise but informative\n\nColumn names:\n\nUse lowercase with underscores: sample_id, gene_expression\n\nMake names meaningful: treatment_group not tg\n\nAvoid special characters and spaces\nStart with a letter, not a number\n\n\n\nFile Formats\n\n\nTable 8.3: Common data file formats\n\n\n\nFormat\nExtension\nBest For\n\n\n\nComma-separated\n.csv\nGeneral tabular data\n\n\nTab-separated\n.tsv\nData containing commas\n\n\nPlain text\n.txt\nSimple data, scripts\n\n\nExcel\n.xlsx\nSharing with non-programmers\n\n\nRDS\n.rds\nR-specific data with types preserved\n\n\nParquet\n.parquet\nLarge datasets, fast reading\n\n\n\n\n\n\nFor long-term storage and sharing, prefer plain text formats (CSV, TSV) over proprietary formats. They’re human-readable and don’t require specific software.\nDocumentation\nAlways create a data dictionary (also called a codebook) documenting:\n\nVariable names and descriptions\nUnits of measurement\nAllowable values or categories\nCoding of missing values\nSource of the data\nDate created/modified\nPreserving Raw Data\n\n\n\n\n\n\nNever Modify Raw Data\n\n\n\nAlways keep an unchanged copy of your original data. Create a separate processed version for analysis. This allows you to:\n\nRetrace your steps if something goes wrong\nRerun analysis with different preprocessing\nShare the original data with collaborators",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tidy-data-types",
    "href": "chapters/08-tidy-data.html#sec-tidy-data-types",
    "title": "8  Tidy Data Principles",
    "section": "\n8.6 Data Types",
    "text": "8.6 Data Types\nUnderstanding data types helps you organize and analyze correctly.\nCategorical vs. Quantitative\n\ndata_types &lt;- tibble(\n  Category = c(\"Categorical\", \"Categorical\", \"Quantitative\", \"Quantitative\"),\n  Type = c(\"Ordinal\", \"Nominal\", \"Ratio\", \"Interval\"),\n  Description = c(\n    \"Ordered categories\",\n    \"Unordered categories\",\n    \"True zero, ratios meaningful\",\n    \"No true zero\"\n  ),\n  Examples = c(\n    \"small, medium, large\",\n    \"red, blue, green\",\n    \"height, weight, concentration\",\n    \"temperature (°C), calendar year\"\n  )\n)\n\ndata_types |&gt;\n  gt() |&gt;\n  tab_header(title = \"Data Type Classification\")\n\n\nTable 8.4: Classification of data types\n\n\n\n\n\n\n\nData Type Classification\n\n\nCategory\nType\nDescription\nExamples\n\n\n\n\nCategorical\nOrdinal\nOrdered categories\nsmall, medium, large\n\n\nCategorical\nNominal\nUnordered categories\nred, blue, green\n\n\nQuantitative\nRatio\nTrue zero, ratios meaningful\nheight, weight, concentration\n\n\nQuantitative\nInterval\nNo true zero\ntemperature (°C), calendar year\n\n\n\n\n\n\n\n\n\nR Data Types\nR has specific data types for these categories:\n\n# Numeric (continuous)\ntemperature &lt;- c(37.2, 36.8, 38.1)\nclass(temperature)\n#&gt; [1] \"numeric\"\n\n# Integer\ncounts &lt;- c(1L, 2L, 3L)\nclass(counts)\n#&gt; [1] \"integer\"\n\n# Character\nsample_names &lt;- c(\"control\", \"treatment\", \"treatment\")\nclass(sample_names)\n#&gt; [1] \"character\"\n\n# Factor (categorical)\ntreatment &lt;- factor(c(\"low\", \"medium\", \"high\"), \n                   levels = c(\"low\", \"medium\", \"high\"),\n                   ordered = TRUE)\nclass(treatment)\n#&gt; [1] \"ordered\" \"factor\"\ntreatment\n#&gt; [1] low    medium high  \n#&gt; Levels: low &lt; medium &lt; high\n\n# Logical\npassed_qc &lt;- c(TRUE, TRUE, FALSE)\nclass(passed_qc)\n#&gt; [1] \"logical\"\n\nWhen to Use Factors\nFactors are R’s way of handling categorical variables. Use them when:\n\nYou have a limited set of possible values\nThe order matters (ordinal data)\nYou want specific grouping in analyses and plots\n\n\n# Character vector sorts alphabetically\ntreatments &lt;- c(\"High\", \"Low\", \"Medium\", \"Low\", \"High\")\nsort(treatments)\n#&gt; [1] \"High\"   \"High\"   \"Low\"    \"Low\"    \"Medium\"\n\n# Factor maintains meaningful order\ntreatments_factor &lt;- factor(treatments, \n                           levels = c(\"Low\", \"Medium\", \"High\"))\nsort(treatments_factor)\n#&gt; [1] Low    Low    Medium High   High  \n#&gt; Levels: Low Medium High",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-spreadsheets",
    "href": "chapters/08-tidy-data.html#sec-spreadsheets",
    "title": "8  Tidy Data Principles",
    "section": "\n8.7 Structuring Spreadsheets for Analysis",
    "text": "8.7 Structuring Spreadsheets for Analysis\nIf you must use spreadsheets for data entry, follow these guidelines:\n\n\n\n\n\n\nSpreadsheet Best Practices\n\n\n\n\n\nOne header row only — Put variable names in row 1\n\nNo merged cells — They break data structure\n\nNo color-coding as data — Use a column instead\n\nConsistent missing values — Use NA, not blank, -, or N/A\n\n\nOne sheet per dataset — Don’t spread data across tabs\n\nExport as CSV — Preserve as plain text\n\nNo calculations in raw data — Keep raw and calculated data separate\n\n\n\nBefore and After\nBad spreadsheet practices:\n\nMerged cells for visual grouping\nColor-coded cells indicating categories\nMultiple header rows\nNotes in random cells\nMixed data types in columns\nEmbedded calculations\n\nGood spreadsheet practices:\n\nClean rectangular data\nSingle header row\nConsistent formatting throughout\nSeparate documentation sheet\nExplicit coding of all information",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tidyverse",
    "href": "chapters/08-tidy-data.html#sec-tidyverse",
    "title": "8  Tidy Data Principles",
    "section": "\n8.8 The Tidyverse: A Collection of Data Science Packages",
    "text": "8.8 The Tidyverse: A Collection of Data Science Packages\nThe tidyverse is a collection of R packages designed for data science that share a common philosophy and work seamlessly together. These packages make data manipulation, visualization, and analysis more intuitive and readable.\nCore Tidyverse Packages\n\n\nTable 8.5: Core tidyverse packages\n\n\n\nPackage\nPurpose\n\n\n\ndplyr\nData manipulation (filter, select, mutate, etc.)\n\n\nggplot2\nData visualization\n\n\ntidyr\nData tidying (pivot, separate, etc.)\n\n\nreadr\nFast data import\n\n\npurrr\nFunctional programming\n\n\nstringr\nString manipulation\n\n\nforcats\nWorking with factors\n\n\ntibble\nModern data frames\n\n\n\n\n\n\nLoad all core packages at once:\n\nlibrary(tidyverse)\n\nData Import with readr\nThe readr package provides fast, friendly functions for reading rectangular data. It’s more consistent and feature-rich than base R’s read.csv().\n\n# Basic CSV import (tidyverse alternative to read.csv)\ndata &lt;- read_csv(\"experiment_results.csv\")\n\n# Tab-separated files\ndata &lt;- read_tsv(\"data.tsv\")\n\n# Semicolon-separated (common in European locales)\ndata &lt;- read_csv2(\"data.csv\")\n\n# Fixed-width files\ndata &lt;- read_fwf(\"data.txt\", fwf_widths(c(10, 5, 8)))\n\nControlling Column Types\nBy default, read_csv() guesses column types from the first 1000 rows. You can override this with col_types:\n\n# Specify column types explicitly\ndata &lt;- read_csv(\n  \"samples.csv\",\n  col_types = cols(\n    sample_id = col_character(),    # Force as character\n    count = col_integer(),          # Integer values\n    concentration = col_double(),   # Decimal numbers\n    date = col_date(),              # Parse as date\n    treatment = col_factor(levels = c(\"control\", \"drug_A\", \"drug_B\"))\n  )\n)\n\n# Shorthand specification (c = character, i = integer, d = double, D = date)\ndata &lt;- read_csv(\"samples.csv\", col_types = \"ciidD\")\n\n# Only override specific columns, guess the rest\ndata &lt;- read_csv(\n  \"samples.csv\",\n  col_types = cols(\n    sample_id = col_character(),\n    .default = col_guess()\n  )\n)\n\n\n\n\n\n\n\nTip\n\n\n\nAlways check problems(data) after reading to see if any values couldn’t be parsed as expected. This catches data quality issues early!\n\n\nHandling Messy Column Names\nReal-world data often has problematic column names (spaces, special characters, inconsistent capitalization). The janitor package provides clean_names() to standardize them:\n\nlibrary(janitor)\n\n# Before: \"Sample ID\", \"Gene Expression (log2)\", \"P-Value\"\n# After:  \"sample_id\", \"gene_expression_log2\",  \"p_value\"\ndata &lt;- read_csv(\"messy_data.csv\") |&gt;\n  clean_names()\n\nThis converts all names to lowercase snake_case, making them valid R identifiers and consistent to work with.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-pipe",
    "href": "chapters/08-tidy-data.html#sec-pipe",
    "title": "8  Tidy Data Principles",
    "section": "\n8.9 The Pipe Operator",
    "text": "8.9 The Pipe Operator\nOne of the most powerful features of the tidyverse is the pipe operator, which allows you to chain operations together in a readable way.\nThe Native Pipe: |&gt;\n\nR 4.1+ includes a native pipe operator |&gt;:\n\n# Without pipe: nested, hard to read\nsqrt(sum(abs(c(-4, 9, -16))))\n#&gt; [1] 5.385165\n\n# With pipe: reads left to right\nc(-4, 9, -16) |&gt;\n  abs() |&gt;\n  sum() |&gt;\n  sqrt()\n#&gt; [1] 5.385165\n\nThe magrittr Pipe: %&gt;%\n\nThe tidyverse also provides %&gt;% from the magrittr package, which has additional features:\n\nlibrary(dplyr)\n\n# Both work similarly for most cases\nc(1, 2, 3, 4, 5) |&gt; mean()\n#&gt; [1] 3\nc(1, 2, 3, 4, 5) %&gt;% mean()\n#&gt; [1] 3\n\n\n\n\n\n\n\nTip\n\n\n\nThe pipe transforms x |&gt; f(y) into f(x, y). Think of it as “take this, then do that.” This makes code read like a recipe: take data, then filter, then select, then summarize.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-dplyr",
    "href": "chapters/08-tidy-data.html#sec-dplyr",
    "title": "8  Tidy Data Principles",
    "section": "\n8.10 Data Manipulation with dplyr",
    "text": "8.10 Data Manipulation with dplyr\nThe dplyr package provides a consistent set of “verbs” for data manipulation. These functions are designed to be intuitive and compose well together.\nThe Five Core dplyr Verbs\n\n\nTable 8.6: The five core dplyr verbs\n\n\n\nVerb\nAction\n\n\n\nfilter()\nPick rows based on conditions\n\n\nselect()\nPick columns by name\n\n\narrange()\nReorder rows\n\n\nmutate()\nCreate or modify columns\n\n\nsummarize()\nReduce to summary statistics\n\n\n\n\n\n\nLet’s create sample data to demonstrate these:\n\n# Sample experimental data\nexperiments &lt;- tibble(\n  sample_id = paste0(\"S\", 1:12),\n  treatment = rep(c(\"control\", \"drug_A\", \"drug_B\"), each = 4),\n  replicate = rep(1:4, 3),\n  expression = c(10.2, 11.1, 9.8, 10.5,   # control\n                 15.3, 14.8, 16.2, 15.0,   # drug_A\n                 18.1, 19.2, 17.5, 18.8),  # drug_B\n  cell_count = c(1000, 1050, 980, 1020,\n                 1100, 1080, 1150, 1090,\n                 1200, 1250, 1180, 1220)\n)\n\nexperiments\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS2\ncontrol\n2\n11.1\n1050\n\n\nS3\ncontrol\n3\n9.8\n980\n\n\nS4\ncontrol\n4\n10.5\n1020\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\n\n\n\n\nFiltering Rows with filter()\n\nSelect rows that meet specific conditions:\n\n# Single condition\nexperiments |&gt;\n  filter(treatment == \"drug_A\")\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\n\n\n\n\n# Multiple conditions (AND)\nexperiments |&gt;\n  filter(treatment == \"drug_A\", expression &gt; 15)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\n\n\n\n\n# OR conditions\nexperiments |&gt;\n  filter(treatment == \"drug_A\" | treatment == \"drug_B\")\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\n\n\n\n\n# Using %in% for multiple values\nexperiments |&gt;\n  filter(treatment %in% c(\"drug_A\", \"drug_B\"))\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\n\n\n\n\nSelecting Columns with select()\n\nChoose which columns to keep:\n\n# Select specific columns\nexperiments |&gt;\n  select(sample_id, treatment, expression)\n\n\n\n\nsample_id\ntreatment\nexpression\n\n\n\nS1\ncontrol\n10.2\n\n\nS2\ncontrol\n11.1\n\n\nS3\ncontrol\n9.8\n\n\nS4\ncontrol\n10.5\n\n\nS5\ndrug_A\n15.3\n\n\nS6\ndrug_A\n14.8\n\n\nS7\ndrug_A\n16.2\n\n\nS8\ndrug_A\n15.0\n\n\nS9\ndrug_B\n18.1\n\n\nS10\ndrug_B\n19.2\n\n\nS11\ndrug_B\n17.5\n\n\nS12\ndrug_B\n18.8\n\n\n\n\n\n\n# Select range of columns\nexperiments |&gt;\n  select(sample_id:replicate)\n\n\n\n\nsample_id\ntreatment\nreplicate\n\n\n\nS1\ncontrol\n1\n\n\nS2\ncontrol\n2\n\n\nS3\ncontrol\n3\n\n\nS4\ncontrol\n4\n\n\nS5\ndrug_A\n1\n\n\nS6\ndrug_A\n2\n\n\nS7\ndrug_A\n3\n\n\nS8\ndrug_A\n4\n\n\nS9\ndrug_B\n1\n\n\nS10\ndrug_B\n2\n\n\nS11\ndrug_B\n3\n\n\nS12\ndrug_B\n4\n\n\n\n\n\n\n# Exclude columns with minus\nexperiments |&gt;\n  select(-cell_count)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\n\n\n\nS1\ncontrol\n1\n10.2\n\n\nS2\ncontrol\n2\n11.1\n\n\nS3\ncontrol\n3\n9.8\n\n\nS4\ncontrol\n4\n10.5\n\n\nS5\ndrug_A\n1\n15.3\n\n\nS6\ndrug_A\n2\n14.8\n\n\nS7\ndrug_A\n3\n16.2\n\n\nS8\ndrug_A\n4\n15.0\n\n\nS9\ndrug_B\n1\n18.1\n\n\nS10\ndrug_B\n2\n19.2\n\n\nS11\ndrug_B\n3\n17.5\n\n\nS12\ndrug_B\n4\n18.8\n\n\n\n\n\n\n# Select using helper functions\nexperiments |&gt;\n  select(starts_with(\"exp\"))\n\n\n\n\nexpression\n\n\n\n10.2\n\n\n11.1\n\n\n9.8\n\n\n10.5\n\n\n15.3\n\n\n14.8\n\n\n16.2\n\n\n15.0\n\n\n18.1\n\n\n19.2\n\n\n17.5\n\n\n18.8\n\n\n\n\n\n\nexperiments |&gt;\n  select(where(is.numeric))\n\n\n\n\nreplicate\nexpression\ncell_count\n\n\n\n1\n10.2\n1000\n\n\n2\n11.1\n1050\n\n\n3\n9.8\n980\n\n\n4\n10.5\n1020\n\n\n1\n15.3\n1100\n\n\n2\n14.8\n1080\n\n\n3\n16.2\n1150\n\n\n4\n15.0\n1090\n\n\n1\n18.1\n1200\n\n\n2\n19.2\n1250\n\n\n3\n17.5\n1180\n\n\n4\n18.8\n1220\n\n\n\n\n\n\nArranging Rows with arrange()\n\nSort data by one or more columns:\n\n# Ascending order (default)\nexperiments |&gt;\n  arrange(expression)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS3\ncontrol\n3\n9.8\n980\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS4\ncontrol\n4\n10.5\n1020\n\n\nS2\ncontrol\n2\n11.1\n1050\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\n\n\n\n\n# Descending order\nexperiments |&gt;\n  arrange(desc(expression))\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS2\ncontrol\n2\n11.1\n1050\n\n\nS4\ncontrol\n4\n10.5\n1020\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS3\ncontrol\n3\n9.8\n980\n\n\n\n\n\n\n# Multiple columns\nexperiments |&gt;\n  arrange(treatment, desc(expression))\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS2\ncontrol\n2\n11.1\n1050\n\n\nS4\ncontrol\n4\n10.5\n1020\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS3\ncontrol\n3\n9.8\n980\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\n\n\n\n\nCreating Columns with mutate()\n\nAdd new columns or modify existing ones:\n\nexperiments |&gt;\n  mutate(\n    # Create new columns\n    log_expression = log2(expression),\n    expression_per_cell = expression / cell_count * 1000,\n    # Modify existing\n    treatment = toupper(treatment)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\nlog_expression\nexpression_per_cell\n\n\n\nS1\nCONTROL\n1\n10.2\n1000\n3.350497\n10.20000\n\n\nS2\nCONTROL\n2\n11.1\n1050\n3.472488\n10.57143\n\n\nS3\nCONTROL\n3\n9.8\n980\n3.292782\n10.00000\n\n\nS4\nCONTROL\n4\n10.5\n1020\n3.392317\n10.29412\n\n\nS5\nDRUG_A\n1\n15.3\n1100\n3.935460\n13.90909\n\n\nS6\nDRUG_A\n2\n14.8\n1080\n3.887525\n13.70370\n\n\nS7\nDRUG_A\n3\n16.2\n1150\n4.017922\n14.08696\n\n\nS8\nDRUG_A\n4\n15.0\n1090\n3.906891\n13.76147\n\n\nS9\nDRUG_B\n1\n18.1\n1200\n4.177918\n15.08333\n\n\nS10\nDRUG_B\n2\n19.2\n1250\n4.263034\n15.36000\n\n\nS11\nDRUG_B\n3\n17.5\n1180\n4.129283\n14.83051\n\n\nS12\nDRUG_B\n4\n18.8\n1220\n4.232661\n15.40984\n\n\n\n\n\n\nReplacing Columns with transmute()\n\nIf you only want to keep the new columns (discarding all others), use transmute():\n\nexperiments |&gt;\n  transmute(\n    sample_id,  # Keep this column\n    log_expression = log2(expression),\n    normalized = expression / cell_count * 1000\n  )\n\n\n\n\nsample_id\nlog_expression\nnormalized\n\n\n\nS1\n3.350497\n10.20000\n\n\nS2\n3.472488\n10.57143\n\n\nS3\n3.292782\n10.00000\n\n\nS4\n3.392317\n10.29412\n\n\nS5\n3.935460\n13.90909\n\n\nS6\n3.887525\n13.70370\n\n\nS7\n4.017922\n14.08696\n\n\nS8\n3.906891\n13.76147\n\n\nS9\n4.177918\n15.08333\n\n\nS10\n4.263034\n15.36000\n\n\nS11\n4.129283\n14.83051\n\n\nS12\n4.232661\n15.40984\n\n\n\n\n\n\nThis is useful when you’re creating a derived dataset and don’t need the original columns.\nSummarizing Data with summarize()\n\nCollapse multiple rows into summary statistics:\n\nexperiments |&gt;\n  summarize(\n    mean_expression = mean(expression),\n    sd_expression = sd(expression),\n    n_samples = n()\n  )\n\n\n\n\nmean_expression\nsd_expression\nn_samples\n\n\n14.70833\n3.490626\n12\n\n\n\n\n\nGrouping with group_by()\n\nThe real power of summarize() comes with group_by():\n\n# Summary statistics by treatment\nexperiments |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    mean_expression = mean(expression),\n    sd_expression = sd(expression),\n    n = n()\n  )\n\n\n\n\ntreatment\nmean_expression\nsd_expression\nn\n\n\n\ncontrol\n10.400\n0.5477226\n4\n\n\ndrug_A\n15.325\n0.6184658\n4\n\n\ndrug_B\n18.400\n0.7527727\n4\n\n\n\n\n\n\n# Multiple grouping variables\nexperiments |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    mean_expr = mean(expression),\n    max_expr = max(expression),\n    min_expr = min(expression)\n  ) |&gt;\n  arrange(desc(mean_expr))\n\n\n\n\ntreatment\nmean_expr\nmax_expr\nmin_expr\n\n\n\ndrug_B\n18.400\n19.2\n17.5\n\n\ndrug_A\n15.325\n16.2\n14.8\n\n\ncontrol\n10.400\n11.1\n9.8\n\n\n\n\n\n\nChaining Operations Together\nThe power of dplyr shines when chaining multiple operations:\n\nexperiments |&gt;\n  filter(treatment != \"control\") |&gt;           # Remove controls\n  mutate(log_expr = log2(expression)) |&gt;      # Log transform\n  group_by(treatment) |&gt;                       # Group by treatment\n  summarize(\n    mean_log_expr = mean(log_expr),\n    se = sd(log_expr) / sqrt(n())\n  ) |&gt;\n  arrange(desc(mean_log_expr))                 # Sort by mean\n\n\n\n\ntreatment\nmean_log_expr\nse\n\n\n\ndrug_B\n4.200724\n0.0296177\n\n\ndrug_A\n3.936949\n0.0287301",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tibbles",
    "href": "chapters/08-tidy-data.html#sec-tibbles",
    "title": "8  Tidy Data Principles",
    "section": "\n8.11 Tibbles: Modern Data Frames",
    "text": "8.11 Tibbles: Modern Data Frames\nTibbles are the tidyverse’s enhanced version of data frames. They have several advantages:\nCreating Tibbles\n\n# Create a tibble\nmy_tibble &lt;- tibble(\n  x = 1:5,\n  y = x^2,  # Can reference earlier columns\n  z = c(\"a\", \"b\", \"c\", \"d\", \"e\")\n)\n\nmy_tibble\n\n\n\n\nx\ny\nz\n\n\n\n1\n1\na\n\n\n2\n4\nb\n\n\n3\n9\nc\n\n\n4\n16\nd\n\n\n5\n25\ne\n\n\n\n\n\n\n# Convert data frame to tibble\nas_tibble(mtcars) |&gt; head()\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\nCreating Small Tibbles with tribble()\n\nFor small datasets (especially examples or test data), tribble() provides a row-by-row format that’s easier to read:\n\n# tribble uses column headers with ~ and row-by-row values\nsample_data &lt;- tribble(\n  ~sample_id, ~treatment, ~expression,\n  \"S001\",     \"control\",  10.2,\n  \"S002\",     \"drug_A\",   15.3,\n  \"S003\",     \"drug_B\",   18.1\n)\n\nsample_data\n\n\n\n\nsample_id\ntreatment\nexpression\n\n\n\nS001\ncontrol\n10.2\n\n\nS002\ndrug_A\n15.3\n\n\nS003\ndrug_B\n18.1\n\n\n\n\n\n\nThis format is particularly useful for:\n\nCreating examples in documentation\nQuick test datasets\nSmall lookup tables\nTibble Advantages\n\n\nBetter printing: Shows first 10 rows and fits columns to screen\n\nNo partial matching: df$col won’t match df$column\n\n\nNo string-to-factor conversion: Characters stay as characters\n\nPreserves column types: Subsetting always returns a tibble\n\n\n# Data frame subsetting can surprise you\ndf &lt;- data.frame(x = 1:3, y = 4:6)\nclass(df[, 1])  # Returns vector!\n#&gt; [1] \"integer\"\n\n# Tibble subsetting is consistent\ntb &lt;- tibble(x = 1:3, y = 4:6)\nclass(tb[, 1])  # Returns tibble\n#&gt; [1] \"tbl_df\"     \"tbl\"        \"data.frame\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-dplyr-additional",
    "href": "chapters/08-tidy-data.html#sec-dplyr-additional",
    "title": "8  Tidy Data Principles",
    "section": "\n8.12 Additional dplyr Functions",
    "text": "8.12 Additional dplyr Functions\nConditional Logic with case_when()\n\n\nexperiments |&gt;\n  mutate(\n    expression_level = case_when(\n      expression &lt; 12 ~ \"low\",\n      expression &lt; 17 ~ \"medium\",\n      TRUE ~ \"high\"\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\nexpression_level\n\n\n\nS1\ncontrol\n1\n10.2\n1000\nlow\n\n\nS2\ncontrol\n2\n11.1\n1050\nlow\n\n\nS3\ncontrol\n3\n9.8\n980\nlow\n\n\nS4\ncontrol\n4\n10.5\n1020\nlow\n\n\nS5\ndrug_A\n1\n15.3\n1100\nmedium\n\n\nS6\ndrug_A\n2\n14.8\n1080\nmedium\n\n\nS7\ndrug_A\n3\n16.2\n1150\nmedium\n\n\nS8\ndrug_A\n4\n15.0\n1090\nmedium\n\n\nS9\ndrug_B\n1\n18.1\n1200\nhigh\n\n\nS10\ndrug_B\n2\n19.2\n1250\nhigh\n\n\nS11\ndrug_B\n3\n17.5\n1180\nhigh\n\n\nS12\ndrug_B\n4\n18.8\n1220\nhigh\n\n\n\n\n\n\nCounting with count()\n\n\nexperiments |&gt;\n  count(treatment)\n\n\n\n\ntreatment\nn\n\n\n\ncontrol\n4\n\n\ndrug_A\n4\n\n\ndrug_B\n4\n\n\n\n\n\n\nexperiments |&gt;\n  count(treatment, sort = TRUE)\n\n\n\n\ntreatment\nn\n\n\n\ncontrol\n4\n\n\ndrug_A\n4\n\n\ndrug_B\n4\n\n\n\n\n\n\nYou can also use n_distinct() within summarize() to count unique values:\n\n# Count unique treatments per experiment condition\nexperiments |&gt;\n  summarize(\n    n_samples = n(),\n    n_treatments = n_distinct(treatment),\n    n_replicates = n_distinct(replicate)\n  )\n\n\n\n\nn_samples\nn_treatments\nn_replicates\n\n\n12\n3\n4\n\n\n\n\n\n# n_distinct() is especially useful with group_by()\nexperiments |&gt;\n  group_by(treatment) |&gt;\n  summarize(\n    samples = n(),\n    unique_replicates = n_distinct(replicate)\n  )\n\n\n\n\ntreatment\nsamples\nunique_replicates\n\n\n\ncontrol\n4\n4\n\n\ndrug_A\n4\n4\n\n\ndrug_B\n4\n4\n\n\n\n\n\n\nDistinct Values with distinct()\n\n\nexperiments |&gt;\n  distinct(treatment)\n\n\n\n\ntreatment\n\n\n\ncontrol\n\n\ndrug_A\n\n\ndrug_B\n\n\n\n\n\n\nRenaming with rename()\n\n\nexperiments |&gt;\n  rename(expr = expression, cells = cell_count)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpr\ncells\n\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS2\ncontrol\n2\n11.1\n1050\n\n\nS3\ncontrol\n3\n9.8\n980\n\n\nS4\ncontrol\n4\n10.5\n1020\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS6\ndrug_A\n2\n14.8\n1080\n\n\nS7\ndrug_A\n3\n16.2\n1150\n\n\nS8\ndrug_A\n4\n15.0\n1090\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\n\n\n\n\nSubsetting Rows with slice()\n\nWhile filter() selects rows based on conditions, slice() selects rows by position:\n\n# First 3 rows\nexperiments |&gt;\n  slice(1:3)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS2\ncontrol\n2\n11.1\n1050\n\n\nS3\ncontrol\n3\n9.8\n980\n\n\n\n\n\n\n# Last 2 rows\nexperiments |&gt;\n  slice_tail(n = 2)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS11\ndrug_B\n3\n17.5\n1180\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\n\n\n\n\n# Random sample\nset.seed(42)\nexperiments |&gt;\n  slice_sample(n = 3)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS1\ncontrol\n1\n10.2\n1000\n\n\nS5\ndrug_A\n1\n15.3\n1100\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\n\n\n\n\n# Top 3 by expression\nexperiments |&gt;\n  slice_max(expression, n = 3)\n\n\n\n\nsample_id\ntreatment\nreplicate\nexpression\ncell_count\n\n\n\nS10\ndrug_B\n2\n19.2\n1250\n\n\nS12\ndrug_B\n4\n18.8\n1220\n\n\nS9\ndrug_B\n1\n18.1\n1200\n\n\n\n\n\n\nExtracting Columns with pull()\n\nTo extract a single column as a vector (rather than a data frame), use pull():\n\n# Returns a vector, not a data frame\nexperiments |&gt;\n  filter(treatment == \"drug_A\") |&gt;\n  pull(expression)\n#&gt; [1] 15.3 14.8 16.2 15.0\n\n# Useful for passing to functions that expect vectors\nexperiments |&gt;\n  pull(expression) |&gt;\n  mean()\n#&gt; [1] 14.70833",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-factors",
    "href": "chapters/08-tidy-data.html#sec-factors",
    "title": "8  Tidy Data Principles",
    "section": "\n8.13 Working with Factors",
    "text": "8.13 Working with Factors\nFactors are R’s way of representing categorical data with a fixed set of possible values (called levels). The forcats package (part of the tidyverse) provides tools for working with factors.\nCreating and Inspecting Factors\n\n# Create a factor with explicit levels\ntreatment &lt;- factor(\n  c(\"drug\", \"control\", \"drug\", \"placebo\"),\n  levels = c(\"control\", \"placebo\", \"drug\")  # Explicit order\n)\n\ntreatment\n#&gt; [1] drug    control drug    placebo\n#&gt; Levels: control placebo drug\nlevels(treatment)\n#&gt; [1] \"control\" \"placebo\" \"drug\"\n\nReordering Factor Levels\nFor visualization, you often want to order factor levels meaningfully:\n\nlibrary(forcats)\n\n# Sample data\ngene_data &lt;- tibble(\n  gene = c(\"BRCA1\", \"TP53\", \"EGFR\", \"KRAS\", \"MYC\"),\n  expression = c(5.2, 8.1, 3.4, 6.7, 9.2)\n)\n\n# Reorder gene by expression for better plotting\ngene_data |&gt;\n  mutate(gene = fct_reorder(gene, expression)) |&gt;\n  ggplot(aes(x = expression, y = gene)) +\n  geom_point(size = 3) +\n  labs(title = \"Gene Expression (ordered)\")\n\n\n\n\n\n\n\nOther useful reordering functions:\n\n\nfct_infreq(): Order by frequency (most common first)\n\nfct_rev(): Reverse the order\n\nfct_relevel(): Manually move specific levels to the front\n\n\n# Order by frequency\nfct_infreq(treatment)\n\n# Move \"control\" to be first\nfct_relevel(treatment, \"control\")\n\nRecoding Factor Levels\nUse fct_recode() to change level names:\n\n# Original factor\nstatus &lt;- factor(c(\"WT\", \"WT\", \"KO\", \"HET\", \"KO\"))\n\n# Recode to more descriptive names\nfct_recode(status,\n  \"Wild Type\" = \"WT\",\n  \"Knockout\" = \"KO\",\n  \"Heterozygous\" = \"HET\"\n)\n#&gt; [1] Wild Type    Wild Type    Knockout     Heterozygous Knockout    \n#&gt; Levels: Heterozygous Knockout Wild Type\n\nCollapsing Factor Levels\nUse fct_collapse() to combine multiple levels into one:\n\n# Combine related categories\ntissue &lt;- factor(c(\"brain\", \"liver\", \"heart\", \"kidney\", \"lung\", \"spleen\"))\n\nfct_collapse(tissue,\n  neural = \"brain\",\n  digestive = \"liver\",\n  circulatory = c(\"heart\", \"lung\"),\n  other = c(\"kidney\", \"spleen\")\n)\n#&gt; [1] neural      digestive   circulatory other       circulatory other      \n#&gt; Levels: neural circulatory other digestive\n\nLumping Rare Levels\nUse fct_lump_*() functions to combine rare categories into “Other”:\n\n# Sample with many categories\nsamples &lt;- factor(rep(c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"), c(50, 30, 10, 5, 3, 2)))\n\n# Keep only the top 3 most common\nfct_lump_n(samples, n = 3)\n#&gt;   [1] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [13] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [25] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [37] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [49] A     A     B     B     B     B     B     B     B     B     B     B    \n#&gt;  [61] B     B     B     B     B     B     B     B     B     B     B     B    \n#&gt;  [73] B     B     B     B     B     B     B     B     C     C     C     C    \n#&gt;  [85] C     C     C     C     C     C     Other Other Other Other Other Other\n#&gt;  [97] Other Other Other Other\n#&gt; Levels: A B C Other\n\n# Keep levels that appear at least 10 times\nfct_lump_min(samples, min = 10)\n#&gt;   [1] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [13] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [25] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [37] A     A     A     A     A     A     A     A     A     A     A     A    \n#&gt;  [49] A     A     B     B     B     B     B     B     B     B     B     B    \n#&gt;  [61] B     B     B     B     B     B     B     B     B     B     B     B    \n#&gt;  [73] B     B     B     B     B     B     B     B     C     C     C     C    \n#&gt;  [85] C     C     C     C     C     C     Other Other Other Other Other Other\n#&gt;  [97] Other Other Other Other\n#&gt; Levels: A B C Other\n\n\n\n\n\n\n\nTip\n\n\n\nWhen plotting categorical data, proper factor ordering makes visualizations much easier to read. Always consider whether alphabetical order (the default) or a data-driven order (by value, frequency) is more meaningful.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-joins",
    "href": "chapters/08-tidy-data.html#sec-joins",
    "title": "8  Tidy Data Principles",
    "section": "\n8.14 Joining Data Frames",
    "text": "8.14 Joining Data Frames\nIn real-world analysis, data often lives in multiple tables. Joins combine tables based on matching values in key columns.\nTypes of Joins\n\n\nTable 8.7: Types of joins in dplyr\n\n\n\n\n\n\n\nJoin Type\nResult\n\n\n\ninner_join()\nKeep only rows with matches in both tables\n\n\nleft_join()\nKeep all rows from left table, add matching data from right\n\n\nright_join()\nKeep all rows from right table, add matching data from left\n\n\nfull_join()\nKeep all rows from both tables\n\n\nsemi_join()\nKeep rows from left table that have matches in right\n\n\nanti_join()\nKeep rows from left table that have NO matches in right\n\n\n\n\n\n\nJoin Example\nLet’s create two related tables:\n\n# Sample information\nsamples &lt;- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S3\", \"S4\"),\n  patient = c(\"Patient_A\", \"Patient_B\", \"Patient_A\", \"Patient_C\"),\n  timepoint = c(\"baseline\", \"baseline\", \"week4\", \"baseline\")\n)\n\n# Expression measurements (note: S3 is missing, S5 is extra)\nexpression_data &lt;- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S4\", \"S5\"),\n  gene_A = c(5.2, 3.1, 4.8, 6.0),\n  gene_B = c(2.1, 1.8, 2.5, 3.2)\n)\n\nsamples\n\n\n\n\nsample_id\npatient\ntimepoint\n\n\n\nS1\nPatient_A\nbaseline\n\n\nS2\nPatient_B\nbaseline\n\n\nS3\nPatient_A\nweek4\n\n\nS4\nPatient_C\nbaseline\n\n\n\n\n\nexpression_data\n\n\n\n\nsample_id\ngene_A\ngene_B\n\n\n\nS1\n5.2\n2.1\n\n\nS2\n3.1\n1.8\n\n\nS4\n4.8\n2.5\n\n\nS5\n6.0\n3.2\n\n\n\n\n\n\nLeft Join (Most Common)\nKeep all rows from the left table, add matching data from right:\n\nleft_join(samples, expression_data, by = \"sample_id\")\n\n\n\n\nsample_id\npatient\ntimepoint\ngene_A\ngene_B\n\n\n\nS1\nPatient_A\nbaseline\n5.2\n2.1\n\n\nS2\nPatient_B\nbaseline\n3.1\n1.8\n\n\nS3\nPatient_A\nweek4\nNA\nNA\n\n\nS4\nPatient_C\nbaseline\n4.8\n2.5\n\n\n\n\n\n\nNote that S3 has NA values for the expression columns (no match in right table), and S5 is not included (not in left table).\nInner Join\nKeep only rows with matches in both tables:\n\ninner_join(samples, expression_data, by = \"sample_id\")\n\n\n\n\nsample_id\npatient\ntimepoint\ngene_A\ngene_B\n\n\n\nS1\nPatient_A\nbaseline\n5.2\n2.1\n\n\nS2\nPatient_B\nbaseline\n3.1\n1.8\n\n\nS4\nPatient_C\nbaseline\n4.8\n2.5\n\n\n\n\n\n\nAnti Join\nFind rows in the left table with no match in the right:\n\n# Which samples are missing expression data?\nanti_join(samples, expression_data, by = \"sample_id\")\n\n\n\n\nsample_id\npatient\ntimepoint\n\n\nS3\nPatient_A\nweek4\n\n\n\n\n\nJoining on Multiple Columns\nWhen tables share multiple key columns, specify them all:\n\n# Join on multiple columns\nleft_join(df1, df2, by = c(\"patient_id\", \"visit_date\"))\n\n# When column names differ between tables\nleft_join(df1, df2, by = c(\"id\" = \"patient_id\"))\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen joining, watch for columns with the same name but different meanings (e.g., “year” in both tables meaning different things). dplyr will automatically suffix them with .x and .y, but it’s better to rename ambiguous columns before joining.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tidyr",
    "href": "chapters/08-tidy-data.html#sec-tidyr",
    "title": "8  Tidy Data Principles",
    "section": "\n8.15 Reshaping Data with tidyr",
    "text": "8.15 Reshaping Data with tidyr\nWe’ve already seen pivot_longer() and separate(). Let’s complete the picture with their counterparts.\nPivot Wider\nConvert long data back to wide format with pivot_wider():\n\n# Long format data\ngene_expression_long &lt;- tibble(\n  sample = c(\"S1\", \"S1\", \"S2\", \"S2\", \"S3\", \"S3\"),\n  gene = c(\"BRCA1\", \"TP53\", \"BRCA1\", \"TP53\", \"BRCA1\", \"TP53\"),\n  expression = c(5.2, 3.1, 4.8, 2.9, 6.1, 3.5)\n)\n\ngene_expression_long\n\n\n\n\nsample\ngene\nexpression\n\n\n\nS1\nBRCA1\n5.2\n\n\nS1\nTP53\n3.1\n\n\nS2\nBRCA1\n4.8\n\n\nS2\nTP53\n2.9\n\n\nS3\nBRCA1\n6.1\n\n\nS3\nTP53\n3.5\n\n\n\n\n\n\n\n# Spread genes into columns\ngene_expression_long |&gt;\n  pivot_wider(names_from = gene, values_from = expression)\n\n\n\n\nsample\nBRCA1\nTP53\n\n\n\nS1\n5.2\n3.1\n\n\nS2\n4.8\n2.9\n\n\nS3\n6.1\n3.5\n\n\n\n\n\n\nWhen to Use Wide vs. Long\n\n\nLong format: Better for analysis and plotting with ggplot2\n\nWide format: Better for certain statistical tests, viewing many variables at once, or sharing with collaborators expecting a spreadsheet layout\n\nYou can easily convert between formats as needed:\n\n# Round trip: long -&gt; wide -&gt; long\nstocks &lt;- tibble(\n  date = as.Date('2024-01-01') + 0:2,\n  AAPL = c(185, 187, 184),\n  GOOGL = c(140, 142, 141),\n  MSFT = c(375, 378, 376)\n)\n\nstocks |&gt;\n  pivot_longer(-date, names_to = \"ticker\", values_to = \"price\") |&gt;\n  pivot_wider(names_from = ticker, values_from = price)\n\n\n\n\ndate\nAAPL\nGOOGL\nMSFT\n\n\n\n2024-01-01\n185\n140\n375\n\n\n2024-01-02\n187\n142\n378\n\n\n2024-01-03\n184\n141\n376\n\n\n\n\n\n\nUnite: Combining Columns\nThe unite() function is the opposite of separate():\n\n# Data with separate date components\ndate_parts &lt;- tibble(\n  year = c(2024, 2024, 2024),\n  month = c(1, 2, 3),\n  day = c(15, 20, 25),\n  value = c(100, 150, 200)\n)\n\ndate_parts\n\n\n\n\nyear\nmonth\nday\nvalue\n\n\n\n2024\n1\n15\n100\n\n\n2024\n2\n20\n150\n\n\n2024\n3\n25\n200\n\n\n\n\n\n\n# Combine into a single date column\ndate_parts |&gt;\n  unite(date, year, month, day, sep = \"-\")\n\n\n\n\ndate\nvalue\n\n\n\n2024-1-15\n100\n\n\n2024-2-20\n150\n\n\n2024-3-25\n200\n\n\n\n\n\n\nYou can convert the united column to a proper date type:\n\nlibrary(lubridate)\n\ndate_parts |&gt;\n  unite(date, year, month, day, sep = \"-\") |&gt;\n  mutate(date = ymd(date))\n\n\n\n\ndate\nvalue\n\n\n\n2024-01-15\n100\n\n\n2024-02-20\n150\n\n\n2024-03-25\n200\n\n\n\n\n\n\nSeparate Rows\nSometimes cells contain multiple values. Use separate_rows() to split them into separate rows:\n\n# Common in survey data: multiple selections in one cell\nsurvey &lt;- tibble(\n  respondent = c(\"Alice\", \"Bob\"),\n  favorite_foods = c(\"pizza, tacos, sushi\", \"burgers, fries\")\n)\n\nsurvey\n\n\n\n\nrespondent\nfavorite_foods\n\n\n\nAlice\npizza, tacos, sushi\n\n\nBob\nburgers, fries\n\n\n\n\n\n\n# Split into separate rows\nsurvey |&gt;\n  separate_rows(favorite_foods, sep = \", \")\n\n\n\n\nrespondent\nfavorite_foods\n\n\n\nAlice\npizza\n\n\nAlice\ntacos\n\n\nAlice\nsushi\n\n\nBob\nburgers\n\n\nBob\nfries\n\n\n\n\n\n\nComplete and Expand\nWhen data has implicit missing values, use complete() to make them explicit:\n\n# Data with missing combinations\nobservations &lt;- tibble(\n  site = c(\"A\", \"A\", \"B\"),\n  year = c(2020, 2021, 2020),\n  count = c(10, 15, 8)\n)\n\nobservations\n\n\n\n\nsite\nyear\ncount\n\n\n\nA\n2020\n10\n\n\nA\n2021\n15\n\n\nB\n2020\n8\n\n\n\n\n\n\n# Make all combinations explicit\nobservations |&gt;\n  complete(site, year, fill = list(count = 0))\n\n\n\n\nsite\nyear\ncount\n\n\n\nA\n2020\n10\n\n\nA\n2021\n15\n\n\nB\n2020\n8\n\n\nB\n2021\n0\n\n\n\n\n\n\nUse crossing() to generate all combinations of values:\n\ncrossing(\n  treatment = c(\"control\", \"drug_A\", \"drug_B\"),\n  timepoint = c(0, 24, 48),\n  replicate = 1:3\n)\n\n\n\n\ntreatment\ntimepoint\nreplicate\n\n\n\ncontrol\n0\n1\n\n\ncontrol\n0\n2\n\n\ncontrol\n0\n3\n\n\ncontrol\n24\n1\n\n\ncontrol\n24\n2\n\n\ncontrol\n24\n3\n\n\ncontrol\n48\n1\n\n\ncontrol\n48\n2\n\n\ncontrol\n48\n3\n\n\ndrug_A\n0\n1\n\n\ndrug_A\n0\n2\n\n\ndrug_A\n0\n3\n\n\ndrug_A\n24\n1\n\n\ndrug_A\n24\n2\n\n\ndrug_A\n24\n3\n\n\ndrug_A\n48\n1\n\n\ndrug_A\n48\n2\n\n\ndrug_A\n48\n3\n\n\ndrug_B\n0\n1\n\n\ndrug_B\n0\n2\n\n\ndrug_B\n0\n3\n\n\ndrug_B\n24\n1\n\n\ndrug_B\n24\n2\n\n\ndrug_B\n24\n3\n\n\ndrug_B\n48\n1\n\n\ndrug_B\n48\n2\n\n\ndrug_B\n48\n3",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-data-table",
    "href": "chapters/08-tidy-data.html#sec-data-table",
    "title": "8  Tidy Data Principles",
    "section": "\n8.16 High-Performance Data Wrangling with data.table",
    "text": "8.16 High-Performance Data Wrangling with data.table\nWhile the tidyverse is excellent for learning and most data analysis tasks, the data.table package offers substantial performance advantages for large datasets. It’s worth knowing about, especially if you work with big data.\nWhy data.table?\ndata.table excels in several areas:\n\n\nSpeed: Often 10-100x faster than dplyr for large datasets\n\nMemory efficiency: Modifies data in place rather than making copies\n\nConcise syntax: Complex operations in a single expression\n\nNo dependencies: Minimal external package requirements\nBasic data.table Syntax\ndata.table uses a compact DT[i, j, by] syntax:\n\n\ni: Which rows? (like filter)\n\nj: What to do? (like select, mutate, summarize)\n\nby: Grouped by what? (like group_by)\n\n\nlibrary(data.table)\n\n# Convert to data.table\ndt &lt;- as.data.table(experiments)\n\n# Filter and summarize in one expression\ndt[treatment == \"drug_A\",\n   .(mean_expr = mean(expression)),\n   by = treatment]\n\n# Equivalent dplyr:\n# experiments |&gt;\n#   filter(treatment == \"drug_A\") |&gt;\n#   group_by(treatment) |&gt;\n#   summarize(mean_expr = mean(expression))\n\nWhen to Use data.table\nConsider data.table when:\n\nYour dataset has millions of rows\nYou’re doing repetitive operations on large data\nMemory is constrained\nSpeed is critical (e.g., real-time analysis)\n\nFor learning and most everyday work, stick with dplyr—it’s more readable and has excellent documentation.\ndtplyr: Best of Both Worlds\nThe dtplyr package lets you write dplyr code that runs on data.table:\n\nlibrary(dtplyr)\n\n# Create a lazy data.table\nlazy_dt &lt;- lazy_dt(experiments)\n\n# Use dplyr syntax, get data.table speed\nlazy_dt |&gt;\n  filter(treatment != \"control\") |&gt;\n  group_by(treatment) |&gt;\n  summarize(mean_expr = mean(expression)) |&gt;\n  as_tibble()  # Convert back to tibble\n\nThis gives you dplyr’s readable syntax with much of data.table’s performance.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#sec-tidy-summary",
    "href": "chapters/08-tidy-data.html#sec-tidy-summary",
    "title": "8  Tidy Data Principles",
    "section": "\n8.17 Summary",
    "text": "8.17 Summary\nTidy data provides a consistent structure that simplifies analysis:\n\nEach variable is a column\nEach observation is a row\nEach value occupies one cell\n\nKey principles for data organization:\n\nUse descriptive, consistent naming conventions\nDocument your data with a data dictionary\nPreserve raw data separately from processed data\nChoose appropriate file formats for your needs\nUnderstand the difference between data types\n\nThe tidyverse provides powerful tools for working with tidy data:\n\nThe pipe operator (|&gt; or %&gt;%) chains operations for readable code\n\ndplyr verbs provide intuitive data manipulation:\n\n\nfilter() selects rows by condition\n\nselect() picks columns by name\n\nmutate() creates or modifies columns\n\narrange() sorts rows\n\nsummarize() computes summary statistics\n\ngroup_by() enables grouped operations\n\nslice() and pull() provide positional subsetting and column extraction\n\n\n\nJoins (left_join(), inner_join(), anti_join(), etc.) combine data from multiple tables\n\nTibbles are enhanced data frames with consistent behavior\nFunctions like case_when(), count(), and distinct() handle common tasks\n\nThe tidyr package provides tools for reshaping data:\n\n\npivot_longer() converts wide data to long format\n\npivot_wider() converts long data to wide format\n\nseparate() splits one column into multiple columns\n\nunite() combines multiple columns into one\n\nseparate_rows() splits cells with multiple values into rows\n\ncomplete() and crossing() handle missing combinations\n\nFor high-performance needs, data.table offers:\n\nConcise DT[i, j, by] syntax\n10-100x speed improvements on large datasets\nMemory efficiency through modification by reference\n\ndtplyr as a bridge for those who prefer dplyr syntax\n\nThese practices and tools apply across data science workflows. Well-organized data combined with tidyverse tools makes analysis efficient, reproducible, and shareable.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#additional-reading",
    "href": "chapters/08-tidy-data.html#additional-reading",
    "title": "8  Tidy Data Principles",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo deepen your understanding of tidy data and the tidyverse:\n\n\nR for Data Science by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund — The definitive guide to the tidyverse\n\nTidy Data (Original Paper) by Hadley Wickham — The foundational paper on tidy data principles\n\ndplyr Documentation — Official documentation with examples and vignettes\n\ntidyr Documentation — Guide to reshaping data\n\nforcats Documentation — Working with categorical data\n\njanitor Package — Tools for cleaning messy data\n\nFor high-performance data manipulation:\n\n\ndata.table Documentation — The authoritative guide to data.table\n\ndtplyr Package — Using dplyr syntax with data.table speed",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#exercises",
    "href": "chapters/08-tidy-data.html#exercises",
    "title": "8  Tidy Data Principles",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Identify Tidy Data\nFor each dataset description, determine if it’s tidy. If not, explain what violates tidy principles:\n\nPatient data with columns: patient_id, age, blood_pressure_systolic, blood_pressure_diastolic\nGene expression with columns: gene_name, sample_1, sample_2, sample_3\nSurvey data with columns: respondent_id, question, response\nWeather data with columns: city, 2023_temp, 2024_temp, 2023_precip, 2024_precip\n\nExercise 2: Reshape Data\nGiven this messy dataset:\nCountry   1990   2000   2010\nUSA       250    280    310  \nCanada    27     31     35\nMexico    86     100    120\nWrite R code using pivot_longer() to convert it to tidy format.\nExercise 3: Data Dictionary\nCreate a data dictionary for an experiment tracking: - Gene expression levels measured at 0, 12, and 24 hours - Three treatment groups (control, low dose, high dose) - Twenty biological replicates per group - Quality control pass/fail flags\nExercise 4: Spot the Problems\nReview a dataset you’ve worked with (or your lab’s data). Identify any violations of tidy data principles and propose solutions.\nExercise 5: dplyr Practice\nUsing the built-in mtcars dataset: 1. Filter to only cars with 6 or 8 cylinders 2. Select only the columns mpg, cyl, hp, and wt 3. Create a new column hp_per_cyl (horsepower per cylinder) 4. Arrange by mpg in descending order\nExercise 6: Grouped Summaries\nUsing the iris dataset: 1. Calculate the mean and standard deviation of Sepal.Length for each Species 2. Find the maximum Petal.Width for each species 3. Count the number of observations per species 4. Chain all these operations using the pipe operator\nExercise 7: Data Transformation Pipeline\nCreate a complete analysis pipeline that: 1. Starts with the mtcars dataset 2. Filters to automatic transmission cars (am == 0) 3. Groups by number of cylinders 4. Calculates mean mpg, mean horsepower, and count per group 5. Creates a new column classifying fuel efficiency as “good” (mpg &gt;= 20) or “poor” 6. Arranges by mean mpg descending\nExercise 8: Tibble Exploration\n\nConvert mtcars to a tibble\nCompare how the data frame and tibble display when printed\nTest subsetting behavior: what class does mtcars[, 1] return vs as_tibble(mtcars)[, 1]?\n\nExercise 9: Joining Tables\nCreate two tibbles:\npatients &lt;- tibble(\n  patient_id = c(\"P001\", \"P002\", \"P003\", \"P004\"),\n  name = c(\"Alice\", \"Bob\", \"Carol\", \"David\"),\n  age = c(45, 52, 38, 61)\n)\n\nlab_results &lt;- tibble(\n  patient_id = c(\"P001\", \"P002\", \"P002\", \"P005\"),\n  test = c(\"glucose\", \"glucose\", \"cholesterol\", \"glucose\"),\n  result = c(95, 110, 185, 88)\n)\n\nPerform a left join to add lab results to patient information\nUse an anti-join to find patients with no lab results\nUse an inner join to keep only patients who have lab results\nExplain why P005 appears or doesn’t appear in each result\n\nExercise 10: Reshaping Practice\nGiven this long-format data:\nmeasurements &lt;- tibble(\n  sample = rep(c(\"A\", \"B\", \"C\"), each = 2),\n  metric = rep(c(\"weight\", \"height\"), 3),\n  value = c(70, 175, 65, 168, 80, 182)\n)\n\nUse pivot_wider() to create columns for weight and height\nCalculate BMI (weight / (height/100)^2) on the wide data\nConvert back to long format with pivot_longer()\n\n\nExercise 11: Separate and Unite\n\nGiven data with a combined column \"2024-01-15\", use separate() to split it into year, month, and day columns\nUse unite() to combine them back, but with slashes as separators (\"2024/01/15\")\nWhen might you prefer separate_rows() over separate()?\n\n\n\n\n\n\n\nWickham, H. (2014). Tidy data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html",
    "href": "chapters/10-git-github.html",
    "title": "9  Version Control with Git and GitHub",
    "section": "",
    "text": "9.1 Why Version Control?\nHave you ever had files named like this?\nThis ad-hoc versioning is error-prone, confusing, and doesn’t scale. Version control solves this problem by systematically tracking changes to files over time.\nVersion control systems provide:\nComplete History. Every change is recorded. You can see what changed, when, and why.\nSafe Experimentation. Try new approaches without fear. You can always return to a working version.\nCollaboration. Multiple people can work on the same project without overwriting each other’s work.\nBackup. Your code exists in multiple places, protecting against data loss.\nReproducibility. Tag specific versions for publications. Anyone can access exactly what you used (Blischak et al., 2016).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-why-version-control",
    "href": "chapters/10-git-github.html#sec-why-version-control",
    "title": "9  Version Control with Git and GitHub",
    "section": "",
    "text": "analysis_final.R\nanalysis_final_v2.R\nanalysis_final_v2_REALLY_FINAL.R\nanalysis_final_v2_REALLY_FINAL_fixed.R",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-distributed",
    "href": "chapters/10-git-github.html#sec-git-distributed",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.2 Git: Distributed Version Control",
    "text": "9.2 Git: Distributed Version Control\nGit is the most widely used version control system. Created by Linus Torvalds (who also created Linux), Git is:\n\n\nDistributed: Every copy is a complete repository with full history\n\nFast: Operations happen locally without network delays\n\nFlexible: Supports many workflows from solo to large team projects\n\nKey Concepts\nUnderstanding Git’s mental model helps everything else make sense:\n\n\n\n\n\nflowchart LR\n    A[Working Directory] --&gt; GA([git add])\n    GA --&gt; B[Staging Area]\n    B --&gt; GC([git commit])\n    GC --&gt; C[Local Repository]\n    C --&gt; GP([git push])\n    GP --&gt; D[Remote Repository]\n    D --&gt; GPL([git pull])\n    GPL --&gt; A\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 9.1: The Git workflow: files move from working directory to staging area to repository\n\n\n\n\nWorking Directory. The files you see and edit on your computer.\nStaging Area (Index). A preparation area where you compose your next commit.\nLocal Repository. The .git directory containing all version history.\nRemote Repository. A copy hosted elsewhere (like GitHub) for backup and collaboration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-setup",
    "href": "chapters/10-git-github.html#sec-git-setup",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.3 Setting Up Git",
    "text": "9.3 Setting Up Git\nInstallation\nCheck if Git is already installed:\n$ git --version\ngit version 2.43.0\nIf not installed:\n# macOS (with Homebrew)\n$ brew install git\n\n# Ubuntu/Debian Linux\n$ sudo apt-get install git\n\n# Windows: Download from git-scm.com\nConfiguration\nConfigure your identity (this appears in your commit history):\n# Set your name\n$ git config --global user.name \"Your Name\"\n\n# Set your email (use the same email as your GitHub account)\n$ git config --global user.email \"your.email@example.com\"\n\n# Set default branch name to 'main'\n$ git config --global init.defaultBranch main\n\n# Verify settings\n$ git config --list\n\n\n\n\n\n\nTip\n\n\n\nThe --global flag sets these options for all repositories on your computer. You can override them per-repository by omitting --global while inside a specific repo.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-creating-repo",
    "href": "chapters/10-git-github.html#sec-creating-repo",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.4 Creating a Repository",
    "text": "9.4 Creating a Repository\nMethod 1: Initialize Locally\nCreate a new repository from an existing directory:\n# Navigate to your project directory\n$ cd my_project\n\n# Initialize Git repository\n$ git init\n\n# You'll see a message about creating .git directory\nInitialized empty Git repository in /home/user/my_project/.git/\nMethod 2: Clone from GitHub\nCopy an existing repository:\n# Clone a repository\n$ git clone https://github.com/username/repository.git\n\n# Clone into a specific directory\n$ git clone https://github.com/username/repository.git my_local_name",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-workflow",
    "href": "chapters/10-git-github.html#sec-git-workflow",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.5 Basic Git Workflow",
    "text": "9.5 Basic Git Workflow\nThe daily rhythm of Git involves four main commands: status, add, commit, and push.\nChecking Status\nAlways start by checking your repository’s status:\n$ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n        modified:   analysis.R\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        new_data.csv\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nStaging Changes\nAdd files to the staging area:\n# Add specific file\n$ git add analysis.R\n\n# Add all changes in current directory\n$ git add .\n\n# Add all changes everywhere\n$ git add -A\n\n# Add only certain file types\n$ git add *.R\nCommitting Changes\nRecord staged changes with a descriptive message:\n$ git commit -m \"Add initial data analysis script\"\n[main 7d4e8c2] Add initial data analysis script\n 1 file changed, 45 insertions(+)\n create mode 100644 analysis.R\n\n\n\n\n\n\nWrite Good Commit Messages\n\n\n\nA good commit message:\n\nUses present tense: “Add feature” not “Added feature”\nIs concise but descriptive\nExplains what and why, not how\n\nReferences issues if applicable: “Fix alignment bug (#42)”\n\nBad: “updates”Good: “Add GC content calculation to genome analysis”\n\n\nViewing History\nSee your commit history:\n# Full history\n$ git log\n\n# Condensed one-line format\n$ git log --oneline\n\n# With graph showing branches\n$ git log --oneline --graph --all\n\n# Last 5 commits\n$ git log -5\nExample output:\n$ git log --oneline\n7d4e8c2 (HEAD -&gt; main) Add initial data analysis script\n3a1b2c3 Update README with project description\n9f8e7d6 Initial commit",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-github",
    "href": "chapters/10-git-github.html#sec-github",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.6 Working with GitHub",
    "text": "9.6 Working with GitHub\nGitHub is a web-based platform for hosting Git repositories. It adds:\n\nCloud backup for your code\nCollaboration features (issues, pull requests)\nProject management tools\nFree website hosting (GitHub Pages)\nIntegration with other services\n\nCreating a GitHub Account\n\nGo to github.com\n\nClick “Sign up”\nChoose a professional username\nUse your academic email for the Student Developer Pack\n\n\n\n\n\n\n\nTip\n\n\n\nApply for the GitHub Student Developer Pack for free access to premium features and developer tools.\n\n\nConnecting Local to Remote\nAfter creating a repository on GitHub:\n# Add remote repository (origin is conventional name)\n$ git remote add origin https://github.com/username/repository.git\n\n# Verify the remote\n$ git remote -v\norigin  https://github.com/username/repository.git (fetch)\norigin  https://github.com/username/repository.git (push)\n\n# Push your commits to GitHub\n$ git push -u origin main\nThe -u flag sets up tracking, so future pushes only need git push.\nPush and Pull\nPushing sends your local commits to GitHub:\n$ git push origin main\n# or simply, if tracking is set up:\n$ git push\nPulling retrieves commits from GitHub:\n$ git pull origin main\n# or simply:\n$ git pull\n\n\n\n\n\n\nWarning\n\n\n\nAlways pull before starting work to get any changes from collaborators. Push frequently to back up your work.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-branching",
    "href": "chapters/10-git-github.html#sec-branching",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.7 Branching",
    "text": "9.7 Branching\nBranches let you develop features in isolation without affecting the main codebase.\n\n\n\n\n\ngitGraph\n    commit id: \"Initial\"\n    commit id: \"Add README\"\n    branch feature\n    checkout feature\n    commit id: \"New feature\"\n    commit id: \"Fix bug\"\n    checkout main\n    commit id: \"Update docs\"\n    merge feature\n    commit id: \"Continue\"\n\n\n\n\nFigure 9.2: Branching allows parallel development that can later be merged\n\n\n\n\nCreating and Switching Branches\n# Create a new branch\n$ git branch feature-analysis\n\n# Switch to the branch\n$ git checkout feature-analysis\n\n# Or create and switch in one command\n$ git checkout -b feature-analysis\n\n# List all branches\n$ git branch\n* feature-analysis\n  main\nMerging Branches\nWhen your feature is complete, merge it back to main:\n# Switch to main\n$ git checkout main\n\n# Merge the feature branch\n$ git merge feature-analysis\n\n# Delete the branch after merging (optional)\n$ git branch -d feature-analysis\nResolving Conflicts\nIf Git can’t automatically merge changes, you’ll get a conflict:\n$ git merge feature-branch\nAuto-merging analysis.R\nCONFLICT (content): Merge conflict in analysis.R\nAutomatic merge failed; fix conflicts and then commit the result.\nThe file will contain conflict markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Main branch version\nresult &lt;- mean(data)\n=======\n# Feature branch version\nresult &lt;- median(data)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\nTo resolve:\n\nEdit the file to keep the code you want\nRemove the conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;)\nStage and commit the resolved file",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-pull-requests",
    "href": "chapters/10-git-github.html#sec-pull-requests",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.8 Pull Requests",
    "text": "9.8 Pull Requests\nPull requests (PRs) are GitHub’s mechanism for proposing changes and requesting code review. They’re central to collaborative workflows and open-source contributions.\nCreating a Pull Request\nAfter pushing a branch to GitHub:\n\nNavigate to your repository on GitHub\nYou’ll see a prompt: “Compare & pull request” — click it\nAdd a descriptive title and summary of your changes\nAssign reviewers if working with collaborators\nClick “Create pull request”\n\nReviewers can then:\n\nComment on specific lines of code\nRequest changes\nApprove the PR\n\nOnce approved, click “Merge pull request” to integrate the changes into the main branch.\n\n\n\n\n\n\nTip\n\n\n\nPull requests aren’t just for teams! Use them on solo projects to:\n\nDocument significant changes with a summary\nReview your own code before merging\nKeep a clean history of feature additions\n\n\n\nPushing Branches to GitHub\nTo share a branch with collaborators or create a PR:\n# Push a new branch to GitHub\n$ git push origin feature-branch\n\n# Delete a remote branch (note the colon)\n$ git push origin :old-branch",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-forking",
    "href": "chapters/10-git-github.html#sec-forking",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.9 Forking",
    "text": "9.9 Forking\nForking creates your own copy of someone else’s repository under your GitHub account. It’s different from cloning:\n\n\nClone: Direct copy linked to the original repo\n\nFork: Independent copy on your GitHub account (which you then clone)\n\nWhen to Fork\n\nContributing to open-source projects\nUsing someone’s code as a starting point for your own project\nExperimenting with changes you don’t have permission to push\nFork Workflow\n\nClick “Fork” on the GitHub repository page\nClone your fork to your local machine\nMake changes and push to your fork\nCreate a pull request to the original repository\n\nThis is how most open-source software is developed—outside contributors fork, improve, and submit pull requests back to the main project.\n\n\n\n\n\n\nNote\n\n\n\nKeeping a fork synchronized with the original (“upstream”) repository requires additional steps. See GitHub’s guide on Syncing a fork.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-gitignore",
    "href": "chapters/10-git-github.html#sec-gitignore",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.10 The .gitignore File",
    "text": "9.10 The .gitignore File\nNot everything should be tracked. The .gitignore file specifies intentionally untracked files:\n# Create .gitignore\n$ touch .gitignore\nCommon patterns for scientific projects:\n# Data files (too large or sensitive)\n*.fastq\n*.fastq.gz\n*.bam\n*.vcf\ndata/raw/*\n\n# R artifacts\n.Rhistory\n.RData\n.Rproj.user/\n\n# Python artifacts\n__pycache__/\n*.pyc\n.ipynb_checkpoints/\n\n# Output files\nresults/temp/*\n*.log\n\n# System files\n.DS_Store\nThumbs.db\n\n# Credentials (NEVER commit these!)\n*.pem\n.env\nsecrets.txt\n\n\n\n\n\n\nWarning\n\n\n\nAdd .gitignore early! Once files are tracked, adding them to .gitignore won’t untrack them.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-github-pages",
    "href": "chapters/10-git-github.html#sec-github-pages",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.11 GitHub Pages: Free Website Hosting",
    "text": "9.11 GitHub Pages: Free Website Hosting\nGitHub Pages turns your repository into a website—perfect for project documentation, lab websites, or personal portfolios.\n\n\n\n\n\nflowchart LR\n    A[Quarto Documents] --&gt; R([Render])\n    R --&gt; B[HTML Files]\n    B --&gt; GP([git push])\n    GP --&gt; C[GitHub Repository]\n    C --&gt; AU([Automatic])\n    AU --&gt; D[GitHub Pages Website]\n    D --&gt; E[yourname.github.io/repo]\n\n\n\n\nFigure 9.3: GitHub Pages workflow: from Quarto documents to live website\n\n\n\n\nSetting Up GitHub Pages\n\nGo to your repository on GitHub\nClick Settings → Pages\n\nUnder “Source”, select Deploy from a branch\n\nSelect branch: main and folder: /docs (or root)\nClick Save\n\n\nYour site will be live at: https://username.github.io/repository/\nFor Quarto Projects\nIn your _quarto.yml:\nproject:\n  type: website\n  output-dir: docs\nThen render and push:\n# Render your site\n$ quarto render\n\n# Commit the docs folder\n$ git add docs/\n$ git commit -m \"Update website\"\n\n# Push to GitHub\n$ git push",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-best-practices",
    "href": "chapters/10-git-github.html#sec-git-best-practices",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.12 Best Practices",
    "text": "9.12 Best Practices\nRepository Organization\nproject/\n├── README.md          # Project description (always include!)\n├── LICENSE            # How others can use your code\n├── .gitignore         # Files to ignore\n├── data/              # Data files (consider .gitignore for large files)\n│   ├── raw/           # Original, immutable data\n│   └── processed/     # Cleaned data\n├── scripts/           # Analysis scripts\n│   ├── 01_clean.R\n│   └── 02_analyze.R\n├── results/           # Output files\n└── docs/              # Documentation or website\nCommit Frequency\n\nCommit early, commit often\nEach commit should be a logical unit of change\nDon’t commit half-finished work to main\n\nUse branches for experimental work\nCollaboration Workflow\nFor team projects:\n\n\nPull before starting work\nCreate a branch for your changes\nMake commits with clear messages\n\nPush your branch to GitHub\nCreate a Pull Request for review\nAfter approval, merge to main",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-auth",
    "href": "chapters/10-git-github.html#sec-git-auth",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.13 Authentication: HTTPS vs SSH",
    "text": "9.13 Authentication: HTTPS vs SSH\nGit can authenticate with GitHub using either HTTPS or SSH:\nHTTPS (easier to start). Uses your GitHub username and a personal access token. Credentials can be cached so you don’t enter them repeatedly.\nSSH (recommended for regular use). Uses cryptographic keys stored on your computer. More secure and convenient once set up.\nTo switch to SSH, see GitHub’s guide on Connecting with SSH.\nCredential Caching (HTTPS)\nAvoid entering credentials repeatedly:\n# Cache credentials for 1 hour (3600 seconds)\n$ git config --global credential.helper 'cache --timeout=3600'\n\n# On macOS, use the keychain\n$ git config --global credential.helper osxkeychain\n\n# On Windows, use the credential manager\n$ git config --global credential.helper manager",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-line-endings",
    "href": "chapters/10-git-github.html#sec-line-endings",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.14 Line Endings Across Operating Systems",
    "text": "9.14 Line Endings Across Operating Systems\nDifferent operating systems use different characters to mark line endings:\n\n\nLinux/macOS: LF (Line Feed)\n\nWindows: CRLF (Carriage Return + Line Feed)\n\nThis can cause Git to show changes on lines you didn’t edit when collaborating across operating systems.\nSolution\nConfigure Git to handle line endings automatically:\n# On macOS/Linux\n$ git config --global core.autocrlf input\n\n# On Windows\n$ git config --global core.autocrlf true",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-stash",
    "href": "chapters/10-git-github.html#sec-git-stash",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.15 Temporarily Saving Changes: Git Stash",
    "text": "9.15 Temporarily Saving Changes: Git Stash\nSometimes you need to switch branches but aren’t ready to commit your current changes. git stash temporarily saves your work:\n# Stash current changes\n$ git stash\n\n# List stashed changes\n$ git stash list\n\n# Restore most recent stash\n$ git stash pop\n\n# Restore a specific stash\n$ git stash apply stash@{2}\n\n# Delete a stash\n$ git stash drop stash@{0}\nThis is useful when you need to quickly switch context—perhaps to fix an urgent bug on another branch—without committing incomplete work.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-troubleshooting",
    "href": "chapters/10-git-github.html#sec-git-troubleshooting",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.16 Troubleshooting",
    "text": "9.16 Troubleshooting\nCommon Issues and Solutions\nQ: I made changes to the wrong branch!\n# Stash your changes\n$ git stash\n\n# Switch to correct branch\n$ git checkout correct-branch\n\n# Apply the stashed changes\n$ git stash pop\nQ: I want to undo my last commit (but keep the changes)\n$ git reset --soft HEAD~1\nQ: I committed something I shouldn’t have\n# Remove file from Git but keep it locally\n$ git rm --cached sensitive_file.txt\n$ git commit -m \"Remove sensitive file from tracking\"\nQ: Everything is broken!\nWhen all else fails, you can always clone a fresh copy:\n# Save your current work somewhere safe first!\n$ cp -r my_project my_project_backup\n\n# Clone fresh from GitHub\n$ git clone https://github.com/username/repository.git my_project_fresh\n\n\n\n\n\n\nTip\n\n\n\nFor more Git emergencies, visit ohshitgit.com — a humorous but genuinely helpful resource for common Git mistakes.\n\n\nWhen to Commit?\nCommit early and often. A good rule of thumb:\n\nCommit when you’ve completed a logical unit of work\nCommit before trying something experimental\nCommit at the end of each work session\nIf you’re asking “should I commit?” — the answer is usually yes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-commands",
    "href": "chapters/10-git-github.html#sec-git-commands",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.17 Useful Commands Reference",
    "text": "9.17 Useful Commands Reference\n\n\nTable 9.1: Essential Git commands\n\n\n\nCommand\nDescription\n\n\n\ngit init\nInitialize new repository\n\n\ngit clone &lt;url&gt;\nCopy remote repository\n\n\ngit status\nShow current state\n\n\ngit add &lt;file&gt;\nStage changes\n\n\ngit commit -m \"msg\"\nRecord staged changes\n\n\ngit push\nUpload to remote\n\n\ngit pull\nDownload from remote\n\n\ngit branch\nList branches\n\n\ngit checkout -b &lt;name&gt;\nCreate and switch branch\n\n\ngit merge &lt;branch&gt;\nMerge branch\n\n\ngit log --oneline\nView commit history\n\n\ngit diff\nShow unstaged changes\n\n\ngit stash\nTemporarily store changes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#sec-git-summary",
    "href": "chapters/10-git-github.html#sec-git-summary",
    "title": "9  Version Control with Git and GitHub",
    "section": "\n9.18 Summary",
    "text": "9.18 Summary\nVersion control with Git and GitHub is essential for modern research:\n\nGit tracks all changes to your files with complete history\nThe workflow is: edit → stage → commit → push\nBranches enable parallel development and experimentation\nGitHub provides cloud hosting, collaboration, and free websites\nGood practices include meaningful commits, .gitignore, and regular pushing\n\nLearning Git has a steep initial curve, but the investment pays dividends throughout your career. Start simple, use it consistently, and gradually adopt more advanced features.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#additional-reading",
    "href": "chapters/10-git-github.html#additional-reading",
    "title": "9  Version Control with Git and GitHub",
    "section": "Additional Reading",
    "text": "Additional Reading\n\n\nHappy Git and GitHub for the useR — Excellent guide for R users by Jenny Bryan\n\nPro Git Book — Comprehensive free reference (Chacon & Straub, 2014)\n\n\nGitHub Documentation — Official guides\n\nSoftware Carpentry Git Lesson — Beginner-friendly tutorial\n\nOh Shit, Git!?! — Solutions for common Git mistakes\n\nQuarto + GitHub Pages — Publishing documentation",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-git-github.html#exercises",
    "href": "chapters/10-git-github.html#exercises",
    "title": "9  Version Control with Git and GitHub",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: First Repository\n\nCreate a new directory for a test project\nInitialize it as a Git repository\nCreate a README.md file with a project description\nStage and commit the file\nView your commit history\n\nExercise 2: GitHub Integration\n\nCreate a new repository on GitHub (don’t initialize with README)\nConnect your local repository to GitHub\nPush your commits\nVerify your files appear on GitHub\n\nExercise 3: Branching Practice\n\nCreate a branch called add-analysis\n\nAdd a new script file on that branch\nCommit your changes\nSwitch back to main\nMerge the branch\nDelete the branch\n\nExercise 4: Collaboration Simulation\n\nClone a repository (your own or a public one)\nMake changes on GitHub directly (edit a file in the browser)\nPull those changes to your local repository\nMake local changes and push them back\n\n\n\n\n\n\n\nBlischak, J. D., Davenport, E. R., & Wilson, G. (2016). A quick introduction to version control with git and GitHub.\n\n\nChacon, S., & Straub, B. (2014). Pro git.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html",
    "href": "chapters/09-data-visualization.html",
    "title": "10  Data Visualization with ggplot2",
    "section": "",
    "text": "10.1 The Grammar of Graphics\nggplot2 is R’s most popular visualization package, built on the “Grammar of Graphics” framework. Just as grammar provides rules for constructing sentences, the grammar of graphics provides rules for constructing statistical graphics.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-grammar-graphics",
    "href": "chapters/09-data-visualization.html#sec-grammar-graphics",
    "title": "10  Data Visualization with ggplot2",
    "section": "",
    "text": "Seven Components of a Graphic\nEvery ggplot2 visualization is built from these components:\n\n\nTable 10.1: Components of the Grammar of Graphics\n\n\n\n\n\n\n\n\nComponent\nDescription\nExample\n\n\n\nData\nThe dataset being visualized\n\nmtcars, iris\n\n\n\nAesthetics\nMappings from data to visual properties\nx, y, color, size, shape\n\n\nGeometries\nVisual elements representing data\npoints, lines, bars\n\n\nFacets\nSubplots for data subsets\npanels by category\n\n\nStatistics\nTransformations of data\nbinning, smoothing\n\n\nCoordinates\nThe coordinate system\nCartesian, polar\n\n\nThemes\nNon-data visual elements\nfonts, backgrounds\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.1: The Grammar of Graphics framework",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-plot-layers",
    "href": "chapters/09-data-visualization.html#sec-plot-layers",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.2 Building a Plot Layer by Layer",
    "text": "10.2 Building a Plot Layer by Layer\nggplot2 builds plots incrementally using the + operator:\n\n# Start with data and aesthetics\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  # Add a geometry layer\n  geom_point()\n\n\n\n\n\n\n\nLet’s break this down:\n\n\nggplot() initializes the plot with data\n\naes() maps variables to visual properties (aesthetics)\n\ngeom_point() adds the point geometry\n\nThe Template\nMost ggplot2 code follows this template:\nggplot(data = &lt;DATA&gt;, aes(&lt;MAPPINGS&gt;)) +\n  &lt;GEOM_FUNCTION&gt;() +\n  &lt;OPTIONAL_LAYERS&gt;",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-geoms",
    "href": "chapters/09-data-visualization.html#sec-geoms",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.3 Essential Geoms",
    "text": "10.3 Essential Geoms\nScatter Plots with geom_point()\n\nScatter plots show relationships between two continuous variables:\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Fuel Efficiency vs. Weight\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\nFigure 10.2: Relationship between car weight and fuel efficiency\n\n\n\n\nAdding Color and Shape\nMap additional variables to aesthetics:\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Fuel Efficiency by Weight and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n\nFigure 10.3: Scatter plot with color representing cylinders\n\n\n\n\nHistograms with geom_histogram()\n\nHistograms show the distribution of a single continuous variable:\n\nggplot(diamonds, aes(x = price)) +\n  geom_histogram(binwidth = 500, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Diamond Prices\",\n    x = \"Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\nFigure 10.4: Distribution of diamond prices\n\n\n\n\nFrequency Polygons with geom_freqpoly()\n\nFrequency polygons are an alternative to histograms, showing counts as lines instead of bars. They’re especially useful for comparing multiple distributions:\n\nggplot(diamonds, aes(x = price, color = cut)) +\n  geom_freqpoly(binwidth = 1000, linewidth = 1) +\n  labs(\n    title = \"Price Distribution by Cut Quality\",\n    x = \"Price (USD)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\nFigure 10.5: Comparing distributions with frequency polygons\n\n\n\n\nDensity Plots with geom_density()\n\nSmoothed version of histograms:\n\nggplot(diamonds, aes(x = price, fill = cut)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Price Distribution by Cut Quality\",\n    x = \"Price (USD)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\nFigure 10.6: Density plot comparing distributions by cut quality\n\n\n\n\nBox Plots with geom_boxplot()\n\nBox plots show distribution summaries and are excellent for comparisons:\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\n  geom_boxplot(show.legend = FALSE) +\n  labs(\n    title = \"Fuel Efficiency by Cylinder Count\",\n    x = \"Number of Cylinders\",\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\nFigure 10.7: Fuel efficiency by number of cylinders\n\n\n\n\n\n\n\n\n\nFigure 10.8: Anatomy of a box plot\n\n\nBar Charts with geom_bar() and geom_col()\n\nUse geom_bar() for counts and geom_col() for values:\n\nggplot(diamonds, aes(x = cut, fill = cut)) +\n  geom_bar(show.legend = FALSE) +\n  labs(\n    title = \"Diamond Count by Cut Quality\",\n    x = \"Cut\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\nFigure 10.9: Count of diamonds by cut quality\n\n\n\n\nGrouped Bar Charts with position = \"dodge\"\n\nWhen you have two categorical variables, use position = \"dodge\" to place bars side-by-side:\n\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Diamond Count by Cut and Clarity\",\n    x = \"Cut\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\nFigure 10.10: Grouped bar chart showing cut quality by clarity\n\n\n\n\nHorizontal Bar Charts with coord_flip()\n\nFor long category names or many categories, flip the coordinates to make labels readable:\n\nggplot(mpg, aes(x = class, fill = class)) +\n  geom_bar(show.legend = FALSE) +\n  coord_flip() +\n  labs(\n    title = \"Vehicle Count by Class\",\n    x = NULL,\n    y = \"Count\"\n  )\n\n\n\n\n\n\nFigure 10.11: Horizontal bar chart for better label readability\n\n\n\n\nLine Plots with geom_line()\n\nLine plots are ideal for time series or connected observations:\n\n# Create sample time series data\nset.seed(42)\ntime_data &lt;- tibble(\n  day = 1:30,\n  value = cumsum(rnorm(30))\n)\n\nggplot(time_data, aes(x = day, y = value)) +\n  geom_line(color = \"darkblue\", linewidth = 1) +\n  geom_point(color = \"darkblue\") +\n  labs(\n    title = \"Simulated Time Series\",\n    x = \"Day\",\n    y = \"Value\"\n  )\n\n\n\n\n\n\nFigure 10.12: Example line plot",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-combining-geoms",
    "href": "chapters/09-data-visualization.html#sec-combining-geoms",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.4 Combining Geoms",
    "text": "10.4 Combining Geoms\nLayer multiple geoms to create richer visualizations:\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(aes(color = factor(cyl)), size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"black\") +\n  labs(\n    title = \"Weight vs. MPG with Linear Trend\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    color = \"Cylinders\"\n  )\n\n\n\n\n\n\nFigure 10.13: Scatter plot with trend line",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-faceting",
    "href": "chapters/09-data-visualization.html#sec-faceting",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.5 Faceting: Small Multiples",
    "text": "10.5 Faceting: Small Multiples\nFaceting creates separate panels for subsets of data.\n\nfacet_wrap() for One Variable\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~cyl) +\n  labs(\n    title = \"Fuel Efficiency by Weight (Panels by Cylinders)\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\nFigure 10.14: MPG vs weight, faceted by cylinder count\n\n\n\n\n\nfacet_grid() for Two Variables\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_grid(am ~ cyl, labeller = labeller(\n    am = c(\"0\" = \"Automatic\", \"1\" = \"Manual\"),\n    cyl = c(\"4\" = \"4 cyl\", \"6\" = \"6 cyl\", \"8\" = \"8 cyl\")\n  )) +\n  labs(\n    title = \"MPG by Weight, Transmission, and Cylinders\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\nFigure 10.15: Two-dimensional faceting",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-plot-customization",
    "href": "chapters/09-data-visualization.html#sec-plot-customization",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.6 Customizing Your Plots",
    "text": "10.6 Customizing Your Plots\nLabels and Titles\n\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(\n    title = \"Car Fuel Efficiency\",\n    subtitle = \"Data from the 1974 Motor Trend magazine\",\n    x = \"Weight (1000 lbs)\",\n    y = \"Miles per Gallon\",\n    caption = \"Source: mtcars dataset\"\n  )\n\n\n\n\n\n\nFigure 10.16: Plot with comprehensive labels\n\n\n\n\nThemes\nggplot2 includes several built-in themes:\n\nlibrary(patchwork)\n\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"\")\n\np1 &lt;- p + theme_gray() + labs(title = \"theme_gray (default)\")\np2 &lt;- p + theme_minimal() + labs(title = \"theme_minimal\")\np3 &lt;- p + theme_classic() + labs(title = \"theme_classic\")\np4 &lt;- p + theme_bw() + labs(title = \"theme_bw\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\nFigure 10.17: Comparison of ggplot2 themes\n\n\n\n\nColor Scales\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Using Color Brewer Palette\",\n    color = \"Cylinders\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 10.18: Custom color palette",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-viz-best-practices",
    "href": "chapters/09-data-visualization.html#sec-viz-best-practices",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.7 Data Visualization Best Practices",
    "text": "10.7 Data Visualization Best Practices\nGood data visualization communicates clearly and honestly. Follow these principles:\nShow the Data\nDon’t hide your data behind summaries when individual points are informative:\n\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot(outlier.shape = NA, fill = \"lightblue\") +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Show the Data, Not Just the Summary\",\n    x = \"Cylinders\",\n    y = \"MPG\"\n  )\n\n\n\n\n\n\nFigure 10.19: Showing raw data with summary statistics\n\n\n\n\nUse Position and Length for Precision\nHumans are best at comparing positions and lengths. Angles (pie charts) and areas are harder to judge:\n\n# Create summary data\ncut_summary &lt;- diamonds |&gt;\n  count(cut) |&gt;\n  mutate(pct = n / sum(n) * 100)\n\nggplot(cut_summary, aes(x = reorder(cut, pct), y = pct, fill = cut)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(\n    title = \"Diamond Distribution by Cut\",\n    x = \"Cut Quality\",\n    y = \"Percentage\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 10.20: Bar chart provides clearer comparison than a pie chart would\n\n\n\n\n\n\n\n\n\n\nAvoid Pie Charts\n\n\n\nPie charts make comparisons difficult because humans are poor at judging angles. Use bar charts instead for categorical comparisons.\n\n\nOrder by Meaningful Values\nBy default, R orders categorical variables alphabetically, which is rarely meaningful. Use reorder() to order by a numeric value instead:\n\n# Calculate mean mpg by manufacturer\nmtcars_summary &lt;- mtcars |&gt;\n  mutate(car = rownames(mtcars)) |&gt;\n  slice_head(n = 10) |&gt;\n  mutate(car = reorder(car, mpg))  # Order car names by mpg values\n\nggplot(mtcars_summary, aes(x = car, y = mpg)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Fuel Efficiency by Car (Ordered)\",\n    x = NULL,\n    y = \"Miles per Gallon\"\n  )\n\n\n\n\n\n\nFigure 10.21: Ordering categories by value aids comparison\n\n\n\n\nThe reorder() function takes a categorical variable and a numeric variable, then reorders the categories by the numeric values. You can also specify a summary function when there are multiple values per category:\n\n# Order vehicle class by median highway mpg\nggplot(mpg, aes(x = reorder(class, hwy, FUN = median), y = hwy)) +\n  geom_boxplot(fill = \"lightblue\") +\n  coord_flip() +\n  labs(\n    title = \"Highway MPG by Vehicle Class\",\n    subtitle = \"Ordered by median MPG\",\n    x = NULL,\n    y = \"Highway MPG\"\n  )\n\n\n\n\n\n\n\nInclude Zero for Bar Charts\nWhen using bars, start the y-axis at zero. Otherwise, you distort relative magnitudes:\n\n# Example data\nsales &lt;- tibble(\n  quarter = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"),\n  revenue = c(100, 105, 110, 108)\n)\n\nggplot(sales, aes(x = quarter, y = revenue)) +\n  geom_col(fill = \"coral\") +\n  labs(\n    title = \"Quarterly Revenue (Starting at Zero)\",\n    x = \"Quarter\",\n    y = \"Revenue\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 10.22: Bar charts should include zero to show true proportions\n\n\n\n\nConsider Color Blindness\nAbout 8% of men and 0.5% of women have color vision deficiency. Use colorblind-friendly palettes:\n\n# Colorblind-friendly colors\ncb_palette &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\n                \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point(size = 3) +\n  scale_color_manual(values = cb_palette) +\n  labs(\n    title = \"Using a Colorblind-Friendly Palette\",\n    color = \"Cylinders\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 10.23: Colorblind-friendly palette\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe viridis color scales (scale_color_viridis_d(), scale_fill_viridis_c()) are designed to be colorblind-friendly and perceptually uniform.\n\n\nAvoid Chart Junk\nRemove unnecessary visual elements that don’t convey information:\n\nAvoid 3D effects\nRemove unnecessary gridlines\nDon’t use gradient fills for categorical data\nSkip decorative elements",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-common-viz-tasks",
    "href": "chapters/09-data-visualization.html#sec-common-viz-tasks",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.8 Common Visualization Tasks",
    "text": "10.8 Common Visualization Tasks\nComparing Distributions\n\nggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.6) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Sepal Length Distribution by Species\",\n    x = \"Sepal Length (cm)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 10.24: Comparing distributions with density plots\n\n\n\n\nBefore-After Comparisons\nSlope charts effectively show changes between two time points:\n\n# Sample before-after data\nchange_data &lt;- tibble(\n  subject = rep(LETTERS[1:5], 2),\n  time = rep(c(\"Before\", \"After\"), each = 5),\n  value = c(10, 12, 8, 15, 11, 14, 15, 11, 18, 13)\n)\n\nggplot(change_data, aes(x = time, y = value, group = subject)) +\n  geom_line(aes(color = subject), linewidth = 1) +\n  geom_point(aes(color = subject), size = 3) +\n  labs(\n    title = \"Before-After Comparison\",\n    x = NULL,\n    y = \"Value\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 10.25: Slope chart showing change over time",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-saving-plots",
    "href": "chapters/09-data-visualization.html#sec-saving-plots",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.9 Saving Plots",
    "text": "10.9 Saving Plots\nUse ggsave() to export plots:\n\n# Create a plot\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  theme_minimal()\n\n# Save as PNG (good for web)\nggsave(\"my_plot.png\", plot = p, width = 8, height = 6, dpi = 300)\n\n# Save as PDF (good for publications)\nggsave(\"my_plot.pdf\", plot = p, width = 8, height = 6)\n\n# Save as SVG (good for editing)\nggsave(\"my_plot.svg\", plot = p, width = 8, height = 6)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#sec-viz-summary",
    "href": "chapters/09-data-visualization.html#sec-viz-summary",
    "title": "10  Data Visualization with ggplot2",
    "section": "\n10.10 Summary",
    "text": "10.10 Summary\nggplot2 provides a powerful and consistent framework for data visualization:\n\nBuild plots layer by layer using the Grammar of Graphics\nMap data to aesthetics (x, y, color, size, shape)\nAdd geometries to represent data visually\nUse faceting to create multi-panel figures\nCustomize with themes, labels, and color scales\n\nBest practices for effective visualization:\n\nShow the data when possible\nUse position and length over angles and area\nOrder categories meaningfully\nInclude zero for bar charts\nUse colorblind-friendly palettes\nAvoid chart junk and 3D effects",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#additional-reading",
    "href": "chapters/09-data-visualization.html#additional-reading",
    "title": "10  Data Visualization with ggplot2",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo master data visualization in R:\n\n\nggplot2: Elegant Graphics for Data Analysis by Hadley Wickham — The comprehensive guide to ggplot2\n\nR Graphics Cookbook by Winston Chang — Practical recipes for common visualization tasks\n\nFundamentals of Data Visualization by Claus Wilke — Excellent guide to visualization principles\n\nData Visualization: A Practical Introduction by Kieran Healy — Thoughtful introduction with R examples\n\nColorBrewer — Tool for choosing effective color palettes\n\nFor publication-quality figures:\n\n\ncowplot Package — Combining and arranging ggplot figures\n\npatchwork Package — Easy multi-panel figure composition\n\nggpubr Package — Publication-ready plots with statistical annotations",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/09-data-visualization.html#exercises",
    "href": "chapters/09-data-visualization.html#exercises",
    "title": "10  Data Visualization with ggplot2",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Scatter Plot\nUsing the iris dataset: 1. Create a scatter plot of Sepal.Length vs Sepal.Width 2. Color points by Species 3. Add appropriate labels and a title\nExercise 2: Distribution Visualization\nUsing the diamonds dataset: 1. Create a histogram of carat 2. Experiment with different bin widths 3. Add a density curve overlay\nExercise 3: Grouped Comparisons\nUsing mtcars: 1. Create box plots of mpg grouped by cyl 2. Add jittered points to show individual observations 3. Use a colorblind-friendly palette\nExercise 4: Faceted Plot\nUsing the diamonds dataset: 1. Create a scatter plot of carat vs price 2. Facet by cut 3. Add a smoothed trend line to each panel\nExercise 5: Publication-Ready Figure\nCreate a polished figure using any dataset that includes: 1. Meaningful title and axis labels 2. A clean theme 3. Appropriate colors 4. No chart junk 5. Save it as a high-resolution PNG",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html",
    "href": "chapters/13-writing-functions.html",
    "title": "11  Writing Functions in R",
    "section": "",
    "text": "11.1 Why Write Your Own Functions?\nThroughout this book, we’ve used many functions: some from base R (like mean() and sum()), others from packages (like dplyr::filter() and ggplot2::ggplot()). But the real power of programming comes from writing your own functions.\nFunctions allow you to:\nIn biosciences, you’ll often need custom functions for tasks like:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-why-functions",
    "href": "chapters/13-writing-functions.html#sec-why-functions",
    "title": "11  Writing Functions in R",
    "section": "",
    "text": "Avoid repetition: Write code once, use it many times\n\nReduce errors: Fix bugs in one place rather than many\n\nImprove readability: Give meaningful names to complex operations\n\nShare code: Package your solutions for others to use\n\n\n\nCalculating enzyme kinetics parameters\nNormalizing gene expression data\nComputing population genetics statistics\nProcessing experimental replicates",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-function-syntax",
    "href": "chapters/13-writing-functions.html#sec-function-syntax",
    "title": "11  Writing Functions in R",
    "section": "\n11.2 Basic Function Syntax",
    "text": "11.2 Basic Function Syntax\nAll R functions follow the same basic syntax:\nfunction_name = function(ARGUMENTS) {\n  OPERATIONS\n  return(VALUE)\n}\nLet’s break this down:\n\n\nfunction_name: A descriptive name for your function\n\nARGUMENTS: Input values the function accepts\n\nOPERATIONS: What the function does\n\nreturn(VALUE): What the function gives back\n\nFor short functions, you can write everything on one line:\nmy_short_func = function(x) x^2\n\n\n\n\n\n\nTip\n\n\n\nGive your functions short, descriptive names that indicate what they do. calculate_gc_content is better than func1 or cgc.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-gc-example",
    "href": "chapters/13-writing-functions.html#sec-gc-example",
    "title": "11  Writing Functions in R",
    "section": "\n11.3 A Simple Example: GC Content Calculator",
    "text": "11.3 A Simple Example: GC Content Calculator\nLet’s write a function that calculates the GC content of a DNA sequence — the proportion of guanine (G) and cytosine (C) bases.\n\ngc_content = function(sequence) {\n  # Convert to uppercase for consistency\n  seq_upper = toupper(sequence)\n\n  # Count G and C bases\n  gc_count = str_count(seq_upper, \"[GC]\")\n\n  # Calculate total length (excluding any non-ATGC characters)\n  total_bases = str_count(seq_upper, \"[ATGC]\")\n\n  # Return GC proportion\n  gc_count / total_bases\n}\n\nTest it:\n\ngc_content(\"ATGCGATCGATCG\")\n#&gt; [1] 0.5384615\ngc_content(\"atgcGATCgatcg\")  # Works with mixed case\n#&gt; [1] 0.5384615\ngc_content(\"AAAAAATTTTTT\")   # Low GC content\n#&gt; [1] 0\ngc_content(\"GGGGGGCCCCCC\")   # High GC content\n#&gt; [1] 1\n\nSpecifying Return Values\nIn the example above, R automatically returned the last evaluated expression. However, it’s good practice to use explicit return() statements, especially for complex functions:\n\ngc_content = function(sequence) {\n  seq_upper = toupper(sequence)\n  gc_count = str_count(seq_upper, \"[GC]\")\n  total_bases = str_count(seq_upper, \"[ATGC]\")\n  gc_proportion = gc_count / total_bases\n\n  return(gc_proportion)  # Explicit return\n}\n\nReturning Multiple Values\nWhat if we want to return more than one piece of information? Use a list or data frame:\n\ngc_content_detailed = function(sequence) {\n  seq_upper = toupper(sequence)\n\n  # Count each base\n  a_count = str_count(seq_upper, \"A\")\n  t_count = str_count(seq_upper, \"T\")\n  g_count = str_count(seq_upper, \"G\")\n  c_count = str_count(seq_upper, \"C\")\n\n  total = a_count + t_count + g_count + c_count\n  gc_prop = (g_count + c_count) / total\n\n  # Return as a data frame\n  result = tibble(\n    sequence = sequence,\n    length = total,\n    A = a_count,\n    T = t_count,\n    G = g_count,\n    C = c_count,\n    gc_content = round(gc_prop, 4)\n  )\n\n  return(result)\n}\n\ngc_content_detailed(\"ATGCGATCGATCGATCG\")\n\n\n\n\nsequence\nlength\nA\nT\nG\nC\ngc_content\n\n\nATGCGATCGATCGATCG\n17\n4\n4\n5\n4\n0.5294\n\n\n\n\n\nDefault Argument Values\nYou can specify default values for arguments. This is useful when most calls will use the same value:\n\ngc_content = function(sequence, digits = 4) {\n  seq_upper = toupper(sequence)\n  gc_count = str_count(seq_upper, \"[GC]\")\n  total_bases = str_count(seq_upper, \"[ATGC]\")\n  gc_proportion = gc_count / total_bases\n\n  return(round(gc_proportion, digits))\n}\n\ngc_content(\"ATGCGATCGATCG\")       # Uses default 4 digits\n#&gt; [1] 0.5385\ngc_content(\"ATGCGATCGATCG\", 2)    # Override to 2 digits\n#&gt; [1] 0.54",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-lexical-scoping",
    "href": "chapters/13-writing-functions.html#sec-lexical-scoping",
    "title": "11  Writing Functions in R",
    "section": "\n11.4 Environments and Lexical Scoping",
    "text": "11.4 Environments and Lexical Scoping\nAn important concept to understand is that functions operate in their own environment. Objects created inside a function don’t appear in your global environment:\n\ntest_function = function(x) {\n  internal_var = x * 2  # This stays inside the function\n  return(internal_var)\n}\n\nresult = test_function(5)\nresult\n#&gt; [1] 10\n\n# Try to access internal_var - it doesn't exist in the global environment\n# internal_var  # Would produce an error\n\nThis is called lexical scoping. R looks for objects in this order:\n\nInside the function\nIn the environment where the function was defined\nIn parent environments, up to the global environment\n\n\n\n\n\n\n\nImportant\n\n\n\nFunctions are “sandboxed” — they don’t pollute your global environment with intermediate variables. This makes code cleaner and reduces the risk of naming conflicts.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-control-flow",
    "href": "chapters/13-writing-functions.html#sec-control-flow",
    "title": "11  Writing Functions in R",
    "section": "\n11.5 Control Flow in Functions",
    "text": "11.5 Control Flow in Functions\nUsing if/else\nControl flow allows your functions to behave differently based on conditions:\n\n# Calculate enzyme velocity using Michaelis-Menten kinetics\nenzyme_velocity = function(substrate, vmax, km, inhibitor = NULL, ki = NULL) {\n\n  if (is.null(inhibitor)) {\n    # No inhibitor: standard Michaelis-Menten\n    velocity = (vmax * substrate) / (km + substrate)\n  } else {\n    # Competitive inhibition\n    if (is.null(ki)) {\n      stop(\"If inhibitor concentration is provided, ki must also be specified\")\n    }\n    apparent_km = km * (1 + inhibitor / ki)\n    velocity = (vmax * substrate) / (apparent_km + substrate)\n  }\n\n  return(velocity)\n}\n\n# Test without inhibitor\nenzyme_velocity(substrate = 10, vmax = 100, km = 5)\n#&gt; [1] 66.66667\n\n# Test with inhibitor\nenzyme_velocity(substrate = 10, vmax = 100, km = 5, inhibitor = 2, ki = 3)\n#&gt; [1] 54.54545\n\nUsing case_when for Multiple Conditions\nWhen you have many conditions, nested if/else statements become hard to read. Use dplyr::case_when() instead:\n\n# Classify gene expression fold changes\nclassify_expression = function(fold_change) {\n  category = case_when(\n    fold_change &gt;= 2 ~ \"strongly_upregulated\",\n    fold_change &gt;= 1.5 ~ \"upregulated\",\n    fold_change &lt;= 0.5 ~ \"strongly_downregulated\",\n    fold_change &lt;= 0.67 ~ \"downregulated\",\n    TRUE ~ \"unchanged\"\n  )\n  return(category)\n}\n\n# Test with a vector of fold changes\nfold_changes = c(0.3, 0.6, 1.0, 1.6, 2.5)\nclassify_expression(fold_changes)\n#&gt; [1] \"strongly_downregulated\" \"downregulated\"          \"unchanged\"             \n#&gt; [4] \"upregulated\"            \"strongly_upregulated\"\n\nThe equivalent data.table::fcase() function works similarly:\n\nclassify_expression_dt = function(fold_change) {\n  fcase(\n    fold_change &gt;= 2, \"strongly_upregulated\",\n    fold_change &gt;= 1.5, \"upregulated\",\n    fold_change &lt;= 0.5, \"strongly_downregulated\",\n    fold_change &lt;= 0.67, \"downregulated\",\n    default = \"unchanged\"\n  )\n}\n\nclassify_expression_dt(fold_changes)\n#&gt; [1] \"strongly_downregulated\" \"downregulated\"          \"unchanged\"             \n#&gt; [4] \"upregulated\"            \"strongly_upregulated\"",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-iteration",
    "href": "chapters/13-writing-functions.html#sec-iteration",
    "title": "11  Writing Functions in R",
    "section": "\n11.6 Iteration and Functional Programming",
    "text": "11.6 Iteration and Functional Programming\nThe Problem with for Loops\nYou’ll often need to apply the same operation to many inputs. The traditional approach uses for loops:\n\n# Calculate GC content for multiple sequences\nsequences = c(\"ATGCGATCG\", \"GGGGCCCC\", \"AAAATTTT\", \"ATATATAT\")\n\n# Using a for loop\ngc_results = NULL\nfor (i in 1:length(sequences)) {\n  gc_results[i] = gc_content(sequences[i])\n}\ngc_results\n#&gt; [1] 0.5556 1.0000 0.0000 0.0000\n\nThis works, but has several problems:\n\n\nVerbose: Lots of boilerplate code\n\nError-prone: Easy to make off-by-one errors\n\nPollutes environment: The loop variable i persists after the loop\n\nNot R-like: R is designed for vectorized operations\nFunctional Programming with lapply\nThe functional programming approach treats iteration as applying a function across elements:\n\n# Much cleaner!\ngc_results = lapply(sequences, gc_content)\ngc_results\n#&gt; [[1]]\n#&gt; [1] 0.5556\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 0\n\nThe result is a list. To get a vector instead:\n\ngc_results = sapply(sequences, gc_content)\ngc_results\n#&gt; ATGCGATCG  GGGGCCCC  AAAATTTT  ATATATAT \n#&gt;    0.5556    1.0000    0.0000    0.0000\n\nTo combine results into a data frame, use our detailed function with bind_rows():\n\nlapply(sequences, gc_content_detailed) %&gt;%\n  bind_rows()\n\n\n\n\nsequence\nlength\nA\nT\nG\nC\ngc_content\n\n\n\nATGCGATCG\n9\n2\n2\n3\n2\n0.5556\n\n\nGGGGCCCC\n8\n0\n0\n4\n4\n1.0000\n\n\nAAAATTTT\n8\n4\n4\n0\n0\n0.0000\n\n\nATATATAT\n8\n4\n4\n0\n0\n0.0000\n\n\n\n\n\n\nUsing purrr for Iteration\nThe purrr package provides a consistent and powerful set of mapping functions:\n\n# map() returns a list (like lapply)\nmap(sequences, gc_content)\n#&gt; [[1]]\n#&gt; [1] 0.5556\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 0\n\n# map_dbl() returns a numeric vector\nmap_dbl(sequences, gc_content)\n#&gt; [1] 0.5556 1.0000 0.0000 0.0000\n\n# map_df() returns a data frame\nmap_df(sequences, gc_content_detailed)\n\n\n\n\nsequence\nlength\nA\nT\nG\nC\ngc_content\n\n\n\nATGCGATCG\n9\n2\n2\n3\n2\n0.5556\n\n\nGGGGCCCC\n8\n0\n0\n4\n4\n1.0000\n\n\nAAAATTTT\n8\n4\n4\n0\n0\n0.0000\n\n\nATATATAT\n8\n4\n4\n0\n0\n0.0000\n\n\n\n\n\n\nCreating and Reusing Named Functions\nFor complex analyses, separate the function definition from iteration:\n\n# Define a function for processing protein sequences\nanalyze_protein = function(sequence) {\n  seq_upper = toupper(sequence)\n  length = nchar(seq_upper)\n\n  # Count hydrophobic residues (simplified)\n  hydrophobic = str_count(seq_upper, \"[AILMFVPW]\")\n\n  # Count charged residues\n  charged = str_count(seq_upper, \"[DEKRH]\")\n\n  tibble(\n    sequence = sequence,\n    length = length,\n    hydrophobic_pct = round(hydrophobic / length * 100, 1),\n    charged_pct = round(charged / length * 100, 1)\n  )\n}\n\n# Some sample protein sequences\nproteins = c(\n  \"MKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAKWKRQQIAAALEHHHHHH\",\n  \"MVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSH\",\n  \"GLSDGEWQQVLNVWGKVEADIAGHGQEVLIRLFTGHPETLEKFDKFKHLKTEAEMKASEDLKKHGTVVLTALGGILKKKGHHEAELKPLAQSHATKHKIPIKYLEFISDAIIHVLHSKHPGDFGADAQGAMTKALELFRNDIAAKYKELGFQG\"\n)\n\n# Apply to all proteins\nmap_df(proteins, analyze_protein)\n\n\n\n\n\n\n\n\n\n\nsequence\nlength\nhydrophobic_pct\ncharged_pct\n\n\n\nMKTAYIAKQRQISFVKSHFSRQLEERLGLIEVQAPILSRVGDGTQDNLSGAEKAVQVKVKALPDAQFEVVHSLAKWKRQQIAAALEHHHHHH\n92\n42.4\n32.6\n\n\nMVLSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTYFPHFDLSH\n51\n47.1\n25.5\n\n\nGLSDGEWQQVLNVWGKVEADIAGHGQEVLIRLFTGHPETLEKFDKFKHLKTEAEMKASEDLKKHGTVVLTALGGILKKKGHHEAELKPLAQSHATKHKIPIKYLEFISDAIIHVLHSKHPGDFGADAQGAMTKALELFRNDIAAKYKELGFQG\n153\n41.2\n34.6",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-multiple-inputs",
    "href": "chapters/13-writing-functions.html#sec-multiple-inputs",
    "title": "11  Writing Functions in R",
    "section": "\n11.7 Iterating Over Multiple Inputs",
    "text": "11.7 Iterating Over Multiple Inputs\nSometimes you need to iterate over multiple inputs simultaneously.\nUsing pmap for Multiple Arguments\n\n# Simulate growth curves with different parameters\nsimulate_growth = function(time, k, n0, carrying_capacity) {\n  # Logistic growth model\n  n = carrying_capacity / (1 + ((carrying_capacity - n0) / n0) * exp(-k * time))\n\n  tibble(\n    time = time,\n    k = k,\n    n0 = n0,\n    K = carrying_capacity,\n    population = round(n, 0)\n  )\n}\n\n# Parameters for different bacterial strains\nparams = tibble(\n  k = c(0.5, 0.8, 0.3),              # Growth rates\n  n0 = c(100, 50, 200),              # Initial populations\n  carrying_capacity = c(10000, 15000, 8000)  # Carrying capacities\n)\n\n# Run simulations for each strain at time = 10\npmap_df(params, ~simulate_growth(time = 10, k = ..1, n0 = ..2, carrying_capacity = ..3))\n\n\n\n\ntime\nk\nn0\nK\npopulation\n\n\n\n10\n0.5\n100\n10000\n5999\n\n\n10\n0.8\n50\n15000\n13633\n\n\n10\n0.3\n200\n8000\n2720\n\n\n\n\n\n\nUsing a Data Frame of Inputs\nA cleaner approach for complex cases is to pass a data frame of input combinations:\n\n# Create all parameter combinations\nsimulation_params = expand.grid(\n  time = seq(0, 20, by = 2),\n  strain = c(\"wild_type\", \"mutant_A\", \"mutant_B\")\n) %&gt;%\n  as_tibble() %&gt;%\n  mutate(\n    k = case_when(\n      strain == \"wild_type\" ~ 0.5,\n      strain == \"mutant_A\" ~ 0.8,\n      strain == \"mutant_B\" ~ 0.3\n    ),\n    n0 = 100,\n    carrying_capacity = 10000\n  )\n\n# Function that takes a row of parameters\nrun_simulation = function(row_data) {\n  simulate_growth(\n    time = row_data$time,\n    k = row_data$k,\n    n0 = row_data$n0,\n    carrying_capacity = row_data$carrying_capacity\n  ) %&gt;%\n    mutate(strain = row_data$strain)\n}\n\n# Run all simulations\nresults = map_df(1:nrow(simulation_params), function(i) {\n  run_simulation(simulation_params[i, ])\n})\n\n# Plot the results\nggplot(results, aes(x = time, y = population, color = strain)) +\n  geom_line(linewidth = 1) +\n  geom_point() +\n  labs(\n    title = \"Bacterial Growth Curves\",\n    x = \"Time (hours)\",\n    y = \"Population Size\",\n    color = \"Strain\"\n  ) +\n  scale_y_continuous(labels = scales::comma)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-function-best-practices",
    "href": "chapters/13-writing-functions.html#sec-function-best-practices",
    "title": "11  Writing Functions in R",
    "section": "\n11.8 Best Practices for Writing Functions",
    "text": "11.8 Best Practices for Writing Functions\n1. Use Descriptive Names\n\n# Good names\ncalculate_gc_content()\nnormalize_expression()\nfit_growth_curve()\n\n# Bad names\nfunc1()\ndo_stuff()\nf()\n\n2. Document Your Functions\nAdd comments explaining what the function does, its arguments, and return value:\n\n# Calculate the melting temperature of a DNA primer\n#\n# Uses the Wallace rule for short oligonucleotides (&lt;14 bp)\n# or the nearest-neighbor method for longer sequences\n#\n# Arguments:\n#   sequence: DNA sequence (string of A, T, G, C)\n#   method: \"wallace\" or \"nearest_neighbor\"\n#\n# Returns:\n#   Melting temperature in degrees Celsius\n\ncalculate_tm = function(sequence, method = \"wallace\") {\n  seq_upper = toupper(sequence)\n  length = nchar(seq_upper)\n\n  a_count = str_count(seq_upper, \"A\")\n  t_count = str_count(seq_upper, \"T\")\n  g_count = str_count(seq_upper, \"G\")\n  c_count = str_count(seq_upper, \"C\")\n\n  if (method == \"wallace\" || length &lt; 14) {\n    # Wallace rule: Tm = 2(A+T) + 4(G+C)\n    tm = 2 * (a_count + t_count) + 4 * (g_count + c_count)\n  } else {\n    # Simplified nearest-neighbor approximation\n    tm = 64.9 + 41 * (g_count + c_count - 16.4) / length\n  }\n\n  return(tm)\n}\n\ncalculate_tm(\"ATGCGATCGATCG\")\n#&gt; [1] 40\n\n3. Validate Inputs\nCheck that inputs are valid before processing:\n\ngc_content_safe = function(sequence) {\n  # Check input type\n  if (!is.character(sequence)) {\n    stop(\"Sequence must be a character string\")\n  }\n\n  # Check for valid DNA characters\n  seq_upper = toupper(sequence)\n  invalid_chars = str_remove_all(seq_upper, \"[ATGCN]\")\n  if (nchar(invalid_chars) &gt; 0) {\n    warning(paste(\"Invalid characters removed:\", invalid_chars))\n  }\n\n  gc_count = str_count(seq_upper, \"[GC]\")\n  total_bases = str_count(seq_upper, \"[ATGC]\")\n\n  if (total_bases == 0) {\n    stop(\"No valid DNA bases found in sequence\")\n  }\n\n  return(gc_count / total_bases)\n}\n\ngc_content_safe(\"ATGCGATCG\")     # Works fine\n#&gt; [1] 0.5555556\ngc_content_safe(\"ATGCXYZGATCG\")  # Warning about invalid characters\n#&gt; [1] 0.5555556\n\n4. Keep Functions Focused\nEach function should do one thing well. Break complex operations into smaller functions:\n\n# Instead of one giant function, break it up:\n\n# Step 1: Clean the sequence\nclean_sequence = function(seq) {\n  seq %&gt;%\n    toupper() %&gt;%\n    str_remove_all(\"[^ATGC]\")\n}\n\n# Step 2: Calculate base composition\nbase_composition = function(seq) {\n  tibble(\n    A = str_count(seq, \"A\"),\n    T = str_count(seq, \"T\"),\n    G = str_count(seq, \"G\"),\n    C = str_count(seq, \"C\")\n  )\n}\n\n# Step 3: Derive statistics\nsequence_stats = function(composition) {\n  total = sum(composition)\n  composition %&gt;%\n    mutate(\n      total = total,\n      gc_content = (G + C) / total,\n      at_content = (A + T) / total\n    )\n}\n\n# Combine them in a pipeline\n\"atgcGATCgatcgNNNN\" %&gt;%\n  clean_sequence() %&gt;%\n  base_composition() %&gt;%\n  sequence_stats()\n\n\n\n\nA\nT\nG\nC\ntotal\ngc_content\nat_content\n\n\n3\n3\n4\n3\n13\n0.5384615\n0.4615385",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#sec-functions-summary",
    "href": "chapters/13-writing-functions.html#sec-functions-summary",
    "title": "11  Writing Functions in R",
    "section": "\n11.9 Summary",
    "text": "11.9 Summary\nWriting functions is a fundamental programming skill that will dramatically improve your data analysis workflows:\n\n\nBasic syntax: function_name = function(args) { body; return(value) }\n\n\nReturn values: Use explicit return() statements; return lists or data frames for multiple values\n\nDefault arguments: Provide sensible defaults for commonly-used parameter values\n\nLexical scoping: Functions operate in their own environment, keeping your global environment clean\n\nControl flow: Use if/else and case_when() to handle different conditions\n\nFunctional programming: Prefer lapply()/map() over for loops for iteration\n\nMultiple inputs: Use pmap() or data frame inputs for complex parameter combinations\n\nThe next chapter will show you how to debug functions when things go wrong, catch user errors gracefully, and cache results for expensive computations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#additional-reading",
    "href": "chapters/13-writing-functions.html#additional-reading",
    "title": "11  Writing Functions in R",
    "section": "Additional Reading",
    "text": "Additional Reading\nTo deepen your understanding of functions and functional programming in R:\n\n\nR for Data Science: Functions — Excellent introduction to writing functions\n\nAdvanced R: Functions by Hadley Wickham — Deep dive into function mechanics\n\npurrr Documentation — Complete guide to functional programming with purrr\n\nR for Data Science: Iteration — More on map functions and iteration\n\nFor advanced function development:\n\n\nAdvanced R: Functional Programming — Closures, function factories, and more\n\nR Packages by Wickham and Bryan — When you’re ready to package your functions",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/13-writing-functions.html#exercises",
    "href": "chapters/13-writing-functions.html#exercises",
    "title": "11  Writing Functions in R",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Function\nWrite a function called count_codons() that takes a DNA sequence and returns the number of complete codons (groups of 3 nucleotides). For example, “ATGCGA” has 2 complete codons.\nExercise 2: Function with Multiple Returns\nWrite a function analyze_sequence() that takes a DNA sequence and returns a data frame with: - The sequence length - GC content - Number of each base (A, T, G, C) - Whether it starts with a start codon (ATG) - Whether it ends with a stop codon (TAA, TAG, or TGA)\nExercise 3: Control Flow\nWrite a function translate_codon() that takes a three-letter codon and returns the corresponding amino acid (use a simplified table with just 5-10 codons). Include error handling for invalid inputs.\nExercise 4: Iteration\nYou have a vector of 10 DNA sequences. Use map_df() to apply your analyze_sequence() function from Exercise 2 to all of them and combine the results into a single data frame.\nExercise 5: Multiple Inputs\nWrite a function dilution_series() that takes an initial concentration and dilution factor, then returns a data frame showing the concentration at each step for a specified number of dilutions. Use pmap() to apply this to multiple starting conditions.\nExercise 6: Refactoring\nTake the following for loop and rewrite it using functional programming:\nresults = c()\nfor (i in 1:length(sequences)) {\n  gc = gc_content(sequences[i])\n  if (gc &gt; 0.5) {\n    results = c(results, \"high\")\n  } else {\n    results = c(results, \"low\")\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Writing Functions in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html",
    "href": "chapters/14-parallel-computing.html",
    "title": "12  Parallel Computing in R",
    "section": "",
    "text": "12.1 Why Parallel Computing?\nModern computers have multiple processor cores, but by default R only uses one. This means that when you run a computationally intensive analysis — like bootstrapping, permutation tests, or processing thousands of samples — most of your computer’s power sits idle.\nParallel computing lets you use all available cores simultaneously, potentially reducing computation time by a factor equal to your core count. A task that takes 8 hours on one core might take just 1 hour across 8 cores.\nIn biosciences, parallel computing is especially valuable for:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-why-parallel",
    "href": "chapters/14-parallel-computing.html#sec-why-parallel",
    "title": "12  Parallel Computing in R",
    "section": "",
    "text": "Bootstrapping for confidence intervals\n\nPermutation tests for significance testing\n\nSimulation studies (e.g., population genetics)\n\nProcessing many samples (e.g., analyzing hundreds of RNA-seq files)\n\nCross-validation in machine learning\n\nParameter sweeps for computational models",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-cores",
    "href": "chapters/14-parallel-computing.html#sec-cores",
    "title": "12  Parallel Computing in R",
    "section": "\n12.2 How Many Cores Do You Have?",
    "text": "12.2 How Many Cores Do You Have?\nBefore parallelizing, find out how many cores are available:\n\nparallel::detectCores()\n#&gt; [1] 14\n\nThis number includes both physical cores and “logical” cores (from technologies like Intel’s hyperthreading). For CPU-intensive tasks, your effective speedup is typically limited by the number of physical cores.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-parallel-example",
    "href": "chapters/14-parallel-computing.html#sec-parallel-example",
    "title": "12  Parallel Computing in R",
    "section": "\n12.3 A Motivating Example",
    "text": "12.3 A Motivating Example\nLet’s start with a simple example to demonstrate the power of parallelization. We’ll create a deliberately slow function that simulates a computationally expensive operation:\n\n# Simulate analyzing a biological sample (takes 2 seconds each)\nanalyze_sample = function(sample_id) {\n  # Simulate processing time\n  Sys.sleep(2)\n\n  # Return some fake results\n  tibble(\n    sample_id = sample_id,\n    gene_count = sample(5000:20000, 1),\n    quality_score = runif(1, 0.8, 1.0)\n  )\n}\n\nSerial Execution\nFirst, let’s process 8 samples the traditional way:\n\nlibrary(tictoc)\n\ntic()\nserial_results = lapply(1:8, analyze_sample) %&gt;% bind_rows()\ntoc()\n#&gt; 16.049 sec elapsed\n\nAs expected, processing 8 samples at 2 seconds each takes about 16 seconds.\nParallel Execution\nNow let’s do the same thing in parallel using the future.apply package:\n\nlibrary(future.apply)\nplan(multisession)  # Enable parallel processing\n\ntic()\nparallel_results = future_lapply(1:8, analyze_sample) %&gt;% bind_rows()\ntoc()\n#&gt; 2.296 sec elapsed\n\nThe parallel version is dramatically faster! With 8 cores, we can process all 8 samples simultaneously, reducing the time from ~16 seconds to ~2 seconds.\nLet’s verify the results are identical:\n\n# Results have same structure (values differ due to random generation)\nall.equal(names(serial_results), names(parallel_results))\n#&gt; [1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-future",
    "href": "chapters/14-parallel-computing.html#sec-future",
    "title": "12  Parallel Computing in R",
    "section": "\n12.4 The Future Ecosystem",
    "text": "12.4 The Future Ecosystem\nThe examples above use the future package ecosystem, which provides a unified, elegant approach to parallel programming in R. The key packages are:\n\n\nTable 12.1: The future ecosystem\n\n\n\nPackage\nDescription\n\n\n\nfuture\nCore package defining how code is evaluated\n\n\nfuture.apply\nParallel versions of apply functions\n\n\nfurrr\nParallel versions of purrr functions\n\n\n\n\n\n\nSetting Up Parallel Execution\nBefore running parallel code, you must set a “plan” that determines how futures are resolved:\n\nlibrary(future)\n\n# Sequential (no parallelization - useful for debugging)\nplan(sequential)\n\n# Parallel using multiple R sessions (works on all systems)\nplan(multisession)\n\n# Parallel using forked processes (faster, but Unix/Mac only)\nplan(multicore)\n\nFor most users, plan(multisession) is the safest choice as it works on all operating systems.\nfuture_lapply: Parallel Apply\nThe future_lapply() function is a drop-in replacement for lapply():\n\n# Serial version\n# lapply(1:4, function(x) x^2)\n\n# Parallel version - just add \"future_\"\nfuture_lapply(1:4, function(x) x^2)\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 4\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 9\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 16\n\nfurrr: Parallel purrr\nIf you prefer purrr’s syntax, furrr provides parallel versions of all map functions:\n\nlibrary(furrr)\n\n# Serial: map_dbl(1:4, function(x) x^2)\n# Parallel:\nfuture_map_dbl(1:4, function(x) x^2)\n#&gt; [1]  1  4  9 16\n\n# Returns a data frame directly\nfuture_map_dfr(1:4, function(x) {\n  tibble(value = x, squared = x^2)\n})\n\n\n\n\nvalue\nsquared\n\n\n\n1\n1\n\n\n2\n4\n\n\n3\n9\n\n\n4\n16",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-bootstrap-example",
    "href": "chapters/14-parallel-computing.html#sec-bootstrap-example",
    "title": "12  Parallel Computing in R",
    "section": "\n12.5 A Real Example: Bootstrapping Gene Expression",
    "text": "12.5 A Real Example: Bootstrapping Gene Expression\nLet’s apply parallel computing to a realistic bioinformatics task: bootstrapping to estimate confidence intervals for differential expression.\nSetup\n\nset.seed(1234)\n\n# Simulate gene expression data for 1000 genes, 50 samples\nn_genes = 1000\nn_samples = 50\n\nexpression_data = tibble(\n  gene_id = rep(paste0(\"gene_\", 1:n_genes), each = n_samples),\n  sample_id = rep(paste0(\"sample_\", 1:n_samples), n_genes),\n  group = rep(rep(c(\"control\", \"treatment\"), each = n_samples/2), n_genes),\n  expression = rnorm(n_genes * n_samples, mean = 10, sd = 2)\n) %&gt;%\n  # Add a treatment effect to some genes\n  mutate(\n    expression = if_else(\n      gene_id %in% paste0(\"gene_\", 1:100) & group == \"treatment\",\n      expression + rnorm(n(), 2, 0.5),  # Upregulated genes\n      expression\n    )\n  )\n\nhead(expression_data)\n\n\n\n\ngene_id\nsample_id\ngroup\nexpression\n\n\n\ngene_1\nsample_1\ncontrol\n7.585869\n\n\ngene_1\nsample_2\ncontrol\n10.554858\n\n\ngene_1\nsample_3\ncontrol\n12.168882\n\n\ngene_1\nsample_4\ncontrol\n5.308605\n\n\ngene_1\nsample_5\ncontrol\n10.858249\n\n\ngene_1\nsample_6\ncontrol\n11.012112\n\n\n\n\n\n\nBootstrap Function\n\n# Function to bootstrap fold change for one gene\nbootstrap_fold_change = function(gene_data) {\n  # Resample within each group\n  boot_sample = gene_data %&gt;%\n    group_by(group) %&gt;%\n    slice_sample(prop = 1, replace = TRUE) %&gt;%\n    ungroup()\n\n  # Calculate mean expression per group\n  means = boot_sample %&gt;%\n    group_by(group) %&gt;%\n    summarise(mean_expr = mean(expression), .groups = \"drop\")\n\n  # Calculate fold change (treatment / control)\n  fc = means$mean_expr[means$group == \"treatment\"] /\n       means$mean_expr[means$group == \"control\"]\n\n  return(fc)\n}\n\n# Function to run many bootstrap iterations for one gene\nrun_bootstrap = function(gene_id, data, n_boot = 1000) {\n  gene_data = data %&gt;% filter(gene_id == !!gene_id)\n\n  # Run bootstrap iterations\n  boot_fcs = replicate(n_boot, bootstrap_fold_change(gene_data))\n\n  # Return summary statistics\n  tibble(\n    gene_id = gene_id,\n    mean_fc = mean(boot_fcs),\n    lower_ci = quantile(boot_fcs, 0.025),\n    upper_ci = quantile(boot_fcs, 0.975)\n  )\n}\n\nCompare Serial vs Parallel\nLet’s bootstrap confidence intervals for 20 genes:\n\ngenes_to_test = paste0(\"gene_\", 1:20)\n\n# Serial version\ntic()\nserial_boot = map_dfr(genes_to_test, run_bootstrap,\n                      data = expression_data, n_boot = 500)\ntoc()\n#&gt; 16.204 sec elapsed\n\n# Parallel version\ntic()\nparallel_boot = future_map_dfr(genes_to_test, run_bootstrap,\n                                data = expression_data, n_boot = 500,\n                                .options = furrr_options(seed = TRUE))\ntoc()\n#&gt; 2.95 sec elapsed\n\nThe parallel version should be substantially faster, with speedup proportional to your core count.\nView Results\n\nparallel_boot %&gt;%\n  mutate(\n    significant = lower_ci &gt; 1 | upper_ci &lt; 1,\n    direction = case_when(\n      lower_ci &gt; 1 ~ \"upregulated\",\n      upper_ci &lt; 1 ~ \"downregulated\",\n      TRUE ~ \"not significant\"\n    )\n  ) %&gt;%\n  head(10)\n\n\n\n\ngene_id\nmean_fc\nlower_ci\nupper_ci\nsignificant\ndirection\n\n\n\ngene_1\n1.131773\n1.024564\n1.238272\nTRUE\nupregulated\n\n\ngene_2\n1.172248\n1.048010\n1.295856\nTRUE\nupregulated\n\n\ngene_3\n1.186548\n1.088120\n1.289473\nTRUE\nupregulated\n\n\ngene_4\n1.183704\n1.054100\n1.320405\nTRUE\nupregulated\n\n\ngene_5\n1.127931\n1.012363\n1.237380\nTRUE\nupregulated\n\n\ngene_6\n1.131323\n1.031547\n1.240307\nTRUE\nupregulated\n\n\ngene_7\n1.176178\n1.069463\n1.302288\nTRUE\nupregulated\n\n\ngene_8\n1.178690\n1.031968\n1.313715\nTRUE\nupregulated\n\n\ngene_9\n1.273898\n1.166305\n1.398772\nTRUE\nupregulated\n\n\ngene_10\n1.265714\n1.126358\n1.420870\nTRUE\nupregulated",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-fork-socket",
    "href": "chapters/14-parallel-computing.html#sec-fork-socket",
    "title": "12  Parallel Computing in R",
    "section": "\n12.6 Forking vs Sockets",
    "text": "12.6 Forking vs Sockets\nThere are two main approaches to parallelization:\nSocket-based (multisession)\n\nCreates new R sessions for each worker\nWorks on all operating systems including Windows\nRequires copying data to each worker (slower startup)\nSelected with plan(multisession)\n\nFork-based (multicore)\n\nClones the current R session\nWorks on Unix/Mac only (not Windows)\nShares memory with parent process (faster)\nSelected with plan(multicore)\n\nMay cause issues in RStudio; run from terminal for best results\n\n\n# Safe choice for all systems\nplan(multisession)\n\n# Faster on Unix/Mac (run scripts from terminal)\nplan(multicore)\n\n# Specify number of workers\nplan(multisession, workers = 4)\n\n\n\n\n\n\n\nWarning\n\n\n\nOn Windows, plan(multicore) silently falls back to plan(sequential). Always use plan(multisession) for cross-platform code.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-parallel-errors",
    "href": "chapters/14-parallel-computing.html#sec-parallel-errors",
    "title": "12  Parallel Computing in R",
    "section": "\n12.7 Handling Errors in Parallel Code",
    "text": "12.7 Handling Errors in Parallel Code\nErrors in parallel code can be tricky to debug. The purrr package provides safely() and possibly() to handle errors gracefully:\n\n# A function that sometimes fails\nrisky_analysis = function(x) {\n  if (x == 5) stop(\"Error processing sample 5!\")\n  tibble(sample = x, result = x^2)\n}\n\n# Wrap in safely() to catch errors\nsafe_analysis = safely(risky_analysis, otherwise = NULL)\n\n# Run in parallel - won't crash if one fails\nresults = future_map(1:10, safe_analysis)\n\n# Extract successful results\nsuccessful = map(results, \"result\") %&gt;%\n  compact() %&gt;%  # Remove NULLs\n\n  bind_rows()\n\n# See which failed\nerrors = map(results, \"error\") %&gt;%\n  compact()\n\ncat(\"Successful:\", nrow(successful), \"\\n\")\n#&gt; Successful: 9\ncat(\"Failed:\", length(errors), \"\\n\")\n#&gt; Failed: 1\n\nFor simpler error handling, use possibly():\n\n# Returns NA for failed cases\nrobust_analysis = possibly(risky_analysis, otherwise = tibble(sample = NA, result = NA))\n\nfuture_map_dfr(1:10, robust_analysis)\n\n\n\n\nsample\nresult\n\n\n\n1\n1\n\n\n2\n4\n\n\n3\n9\n\n\n4\n16\n\n\nNA\nNA\n\n\n6\n36\n\n\n7\n49\n\n\n8\n64\n\n\n9\n81\n\n\n10\n100",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-caching",
    "href": "chapters/14-parallel-computing.html#sec-caching",
    "title": "12  Parallel Computing in R",
    "section": "\n12.8 Caching Results",
    "text": "12.8 Caching Results\nFor expensive computations, cache intermediate results so you don’t have to recompute them:\n\nlibrary(memoise)\n\n# Create a slow function\nslow_gc_analysis = function(sequence) {\n  Sys.sleep(1)  # Simulate slow computation\n  tibble(\n    sequence = sequence,\n    gc_content = str_count(toupper(sequence), \"[GC]\") / nchar(sequence)\n  )\n}\n\n# Create memoised version\nmemo_gc_analysis = memoise(slow_gc_analysis)\n\n# First call - slow\ntic()\nresult1 = memo_gc_analysis(\"ATGCGATCGATCG\")\ntoc()\n#&gt; 1.014 sec elapsed\n\n# Second call with same input - instant (cached)\ntic()\nresult2 = memo_gc_analysis(\"ATGCGATCGATCG\")\ntoc()\n#&gt; 0.019 sec elapsed\n\nPersistent Caching\nTo cache results across R sessions (useful for long analyses):\n\n# Create a cache directory\ncache_dir = \"analysis_cache\"\nif (!dir.exists(cache_dir)) dir.create(cache_dir)\n\n# Create memoised function with persistent cache\nmemo_gc_persistent = memoise(\n  slow_gc_analysis,\n  cache = cache_filesystem(cache_dir)\n)\n\n# Results will be saved to disk and available in future sessions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-when-parallel",
    "href": "chapters/14-parallel-computing.html#sec-when-parallel",
    "title": "12  Parallel Computing in R",
    "section": "\n12.9 When to Use Parallel Computing",
    "text": "12.9 When to Use Parallel Computing\nParallel computing isn’t always faster. Consider:\nGood Candidates for Parallelization\n\n\nIndependent iterations: Each iteration doesn’t depend on others\n\nCPU-intensive tasks: Complex calculations, simulations\n\nMany iterations: Overhead is amortized across many runs\n\nLong-running operations: Minutes or hours of total computation\nPoor Candidates\n\n\nFast operations: Overhead exceeds computation time\n\nFew iterations: Not enough work to distribute\n\nMemory-intensive: Each worker needs its own copy of data\n\nI/O bound: Waiting for disk/network, not CPU\nAmdahl’s Law\nThe maximum speedup from parallelization is limited by the sequential portion of your code. If 50% of your code must run sequentially, you can never achieve more than 2× speedup, regardless of core count.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-parallel-best-practices",
    "href": "chapters/14-parallel-computing.html#sec-parallel-best-practices",
    "title": "12  Parallel Computing in R",
    "section": "\n12.10 Best Practices",
    "text": "12.10 Best Practices\n1. Start Sequential, Then Parallelize\nAlways develop and debug your code in sequential mode first:\n\n# Debug with sequential execution\nplan(sequential)\n\n# Test with a small subset\ntest_results = map_dfr(1:3, my_analysis_function)\n\n# Once working, switch to parallel\nplan(multisession)\nfull_results = future_map_dfr(1:1000, my_analysis_function)\n\n2. Set Random Seeds\nFor reproducible parallel simulations:\n\n# future.apply\nfuture_lapply(1:10, my_function, future.seed = 123)\n\n# furrr\nfuture_map(1:10, my_function, .options = furrr_options(seed = 123))\n\n3. Monitor Progress\nUse progress bars to track long-running parallel jobs:\n\nlibrary(progressr)\n\nhandlers(\"txtprogressbar\")\n\nwith_progress({\n  p = progressor(steps = 100)\n\n  results = future_map(1:100, function(x) {\n    p()  # Signal progress\n    slow_function(x)\n  })\n})\n\n4. Manage Memory\nEach parallel worker needs memory for: - A copy of R - Any data passed to the function - Results being computed\nIf you run out of memory, reduce the number of workers:\n\n# Use fewer workers for memory-intensive tasks\nplan(multisession, workers = 2)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#sec-parallel-summary",
    "href": "chapters/14-parallel-computing.html#sec-parallel-summary",
    "title": "12  Parallel Computing in R",
    "section": "\n12.11 Summary",
    "text": "12.11 Summary\nParallel computing can dramatically speed up computationally intensive analyses:\n\nThe future ecosystem provides a simple, unified approach to parallelization\nUse plan(multisession) for cross-platform compatibility\nReplace lapply() with future_lapply() or map() with future_map()\n\nAlways develop in sequential mode first, then parallelize\nUse safely() or possibly() to handle errors gracefully\nCache expensive results with memoise for persistent storage\nConsider Amdahl’s law: not everything benefits from parallelization\n\nParallel programming in R has never been easier. With just a few lines of code, you can harness all your computer’s cores for faster, more efficient analyses.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#exercises",
    "href": "chapters/14-parallel-computing.html#exercises",
    "title": "12  Parallel Computing in R",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Parallelization\nWrite a function that simulates processing a biological sample (include a Sys.sleep(1) to simulate processing time). Use future_lapply() to process 10 samples in parallel and compare the time to serial execution.\nExercise 2: Bootstrap Analysis\nUsing the gene expression simulation from this chapter: 1. Write a function to bootstrap the mean expression of a single gene 2. Apply it in parallel to 50 genes 3. Calculate 95% confidence intervals for each gene\nExercise 3: Error Handling\nCreate a function that analyzes DNA sequences but fails if the sequence contains invalid characters. Use safely() to process a list of sequences where some are invalid, and report which ones failed.\nExercise 4: Parameter Sweep\nWrite a function that simulates bacterial growth with parameters for growth rate (k), carrying capacity (K), and initial population (n0). Use parallel computing to run simulations across a grid of parameter combinations.\nExercise 5: Caching\nCreate a memoised version of a “slow” function that calculates protein properties. Demonstrate that repeated calls with the same input are instant.\nExercise 6: Benchmarking\nFor a computationally intensive task of your choice: 1. Time the serial version 2. Time the parallel version with 2, 4, and 8 workers 3. Calculate the speedup factor for each 4. Does the speedup match what you’d expect from Amdahl’s law?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/14-parallel-computing.html#additional-reading",
    "href": "chapters/14-parallel-computing.html#additional-reading",
    "title": "12  Parallel Computing in R",
    "section": "Additional Reading",
    "text": "Additional Reading\n\nfuture package documentation\nfurrr package documentation\n\nParallel Computing with R: A Brief Review by Dirk Eddelbuettel\n\nR for Data Science: Iteration chapter on functional programming",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Parallel Computing in R</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html",
    "href": "chapters/15-databases.html",
    "title": "13  Working with Databases",
    "section": "",
    "text": "13.1 Why Databases?\nA huge amount of scientific data lives in databases. While you can sometimes ask a collaborator to export a snapshot to a CSV file, this approach becomes painful quickly: every time you need updated data or a different subset, you must coordinate with another person. Being able to access databases directly gives you the data you need, when you need it.\nDatabases offer several advantages over flat files:\nIn biosciences, you’ll encounter databases for:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-why-databases",
    "href": "chapters/15-databases.html#sec-why-databases",
    "title": "13  Working with Databases",
    "section": "",
    "text": "Scale: Databases can handle datasets too large to fit in memory\n\nSpeed: Indexes enable fast queries on specific subsets\n\nConcurrent access: Multiple users can safely read and write simultaneously\n\nData integrity: Constraints prevent invalid data from being entered\n\nSecurity: Fine-grained access controls protect sensitive information\n\n\n\nGenomic sequence repositories\nClinical trial data\nLaboratory information management systems (LIMS)\nInstitutional data warehouses\nPublic resources like NCBI, Ensembl, and UniProt",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-db-basics",
    "href": "chapters/15-databases.html#sec-db-basics",
    "title": "13  Working with Databases",
    "section": "\n13.2 Database Basics",
    "text": "13.2 Database Basics\nAt the simplest level, you can think of a database as a collection of data frames, called tables in database terminology. Like a data frame, a database table is a collection of named columns where every value in a column is the same type.\nThere are three key differences between data frames and database tables:\n\nStorage: Database tables are stored on disk and can be arbitrarily large. Data frames are stored in memory and limited by available RAM.\nIndexes: Database tables almost always have indexes that enable fast lookups without scanning every row—like the index of a book.\nOrientation: Traditional databases are row-oriented (optimized for adding records), while analytical databases are increasingly column-oriented (optimized for computing statistics).\n\nTypes of Database Systems\nDatabase management systems (DBMS) come in three main forms:\n\n\nTable 13.1: Types of database management systems\n\n\n\n\n\n\n\n\nType\nDescription\nExamples\n\n\n\nClient-server\nCentral server, multiple client connections\nPostgreSQL, MySQL, SQL Server\n\n\nCloud\nManaged cloud services, auto-scaling\nAmazon Redshift, Google BigQuery, Snowflake\n\n\nIn-process\nRuns entirely within your application\nSQLite, DuckDB\n\n\n\n\n\n\nFor learning and local analysis, in-process databases like duckdb are ideal—no server setup required, and they’re designed for analytical workloads.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-db-connecting",
    "href": "chapters/15-databases.html#sec-db-connecting",
    "title": "13  Working with Databases",
    "section": "\n13.3 Connecting to Databases",
    "text": "13.3 Connecting to Databases\nTo connect to a database from R, you need two packages:\n\n\nDBI (DataBase Interface): Provides generic functions for connecting, querying, and managing databases\n\nA database driver: A package specific to your DBMS (e.g., RPostgres, RMariaDB, duckdb)\n\nUsing duckdb for Learning\nduckdb is a high-performance analytical database that’s perfect for learning. It runs entirely within R and requires no external setup:\n\n# Create an in-memory database (temporary, deleted when R closes)\ncon &lt;- DBI::dbConnect(duckdb::duckdb())\n\nFor persistent storage in a real project, specify a file path:\n\n# Persistent database stored in a file\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), dbdir = \"my_database.duckdb\")\n\nConnecting to Other Databases\nDifferent databases require different connection parameters:\n\n# PostgreSQL\ncon &lt;- DBI::dbConnect(\n  RPostgres::Postgres(),\n  host = \"database.example.com\",\n  port = 5432,\n  dbname = \"mydb\",\n  user = \"username\",\n  password = \"password\"\n)\n\n# MySQL/MariaDB\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  host = \"localhost\",\n  dbname = \"mydb\",\n  username = \"user\"\n)\n\n\n\n\n\n\n\nTip\n\n\n\nNever put passwords directly in your code! Use environment variables, the keyring package, or configuration files that aren’t committed to version control.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-db-loading",
    "href": "chapters/15-databases.html#sec-db-loading",
    "title": "13  Working with Databases",
    "section": "\n13.4 Loading Data into a Database",
    "text": "13.4 Loading Data into a Database\nLet’s create some example tables to work with. We’ll use familiar datasets:\n\n# Write data frames to database tables\ndbWriteTable(con, \"mtcars\", mtcars)\ndbWriteTable(con, \"iris\", iris)\n\n# Verify the tables exist\ndbListTables(con)\n#&gt; [1] \"iris\"   \"mtcars\"\n\nReading Tables\nYou can read an entire table into R:\n\n# Read a table into R as a data frame\ncon |&gt;\n  dbReadTable(\"mtcars\") |&gt;\n  as_tibble()\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n\n\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n\n\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n\n\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\nHowever, for large tables this defeats the purpose of using a database. Instead, you’ll want to query just the data you need.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-sql",
    "href": "chapters/15-databases.html#sec-sql",
    "title": "13  Working with Databases",
    "section": "\n13.5 SQL: The Language of Databases",
    "text": "13.5 SQL: The Language of Databases\nSQL (Structured Query Language) is the standard language for working with databases. While we’ll primarily use dbplyr to write dplyr code that gets translated to SQL, understanding SQL basics is valuable.\nBasic SQL Queries\nThe most common SQL operation is SELECT, which retrieves data:\n\n# Run a SQL query directly\nsql &lt;- \"SELECT mpg, cyl, hp FROM mtcars WHERE mpg &gt; 25\"\nresult &lt;- dbGetQuery(con, sql)\nas_tibble(result)\n\n\n\n\nmpg\ncyl\nhp\n\n\n\n32.4\n4\n66\n\n\n30.4\n4\n52\n\n\n33.9\n4\n65\n\n\n27.3\n4\n66\n\n\n26.0\n4\n91\n\n\n30.4\n4\n113\n\n\n\n\n\n\nSQL Query Structure\nA SQL query follows this general pattern:\nSELECT columns\nFROM table\nWHERE conditions\nGROUP BY columns\nORDER BY columns\nThe clauses must appear in this order, though not all are required.\n\n\nTable 13.2: SQL clauses and their dplyr equivalents\n\n\n\nClause\nPurpose\ndplyr equivalent\n\n\n\nSELECT\nChoose columns, compute values\n\nselect(), mutate()\n\n\n\nFROM\nSpecify the table\nThe data frame name\n\n\nWHERE\nFilter rows\nfilter()\n\n\nGROUP BY\nDefine groups for aggregation\ngroup_by()\n\n\nORDER BY\nSort results\narrange()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-dbplyr",
    "href": "chapters/15-databases.html#sec-dbplyr",
    "title": "13  Working with Databases",
    "section": "\n13.6 Using dbplyr: dplyr for Databases",
    "text": "13.6 Using dbplyr: dplyr for Databases\ndbplyr is a dplyr backend that translates your dplyr code to SQL and runs it on the database. This means you can use familiar tidyverse syntax without learning SQL.\nCreating a Database Reference\nUse tbl() to create a reference to a database table:\n\nmtcars_db &lt;- tbl(con, \"mtcars\")\nmtcars_db\n\n\nTable 13.3\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n14.3\n8\n360.0\n245\n3.21\n3.570\n15.84\n0\n0\n3\n4\n\n\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n16.4\n8\n275.8\n180\n3.07\n4.070\n17.40\n0\n0\n3\n3\n\n\n17.3\n8\n275.8\n180\n3.07\n3.730\n17.60\n0\n0\n3\n3\n\n\n15.2\n8\n275.8\n180\n3.07\n3.780\n18.00\n0\n0\n3\n3\n\n\n10.4\n8\n472.0\n205\n2.93\n5.250\n17.98\n0\n0\n3\n4\n\n\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n14.7\n8\n440.0\n230\n3.23\n5.345\n17.42\n0\n0\n3\n4\n\n\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\n15.5\n8\n318.0\n150\n2.76\n3.520\n16.87\n0\n0\n3\n2\n\n\n15.2\n8\n304.0\n150\n3.15\n3.435\n17.30\n0\n0\n3\n2\n\n\n13.3\n8\n350.0\n245\n3.73\n3.840\n15.41\n0\n0\n3\n4\n\n\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\nNotice that this doesn’t load the data—it just creates a reference. The data stays in the database.\nLazy Evaluation\ndbplyr operations are lazy: they build up a query without executing it until you explicitly request the data:\n\n# Build a query (doesn't execute yet)\nefficient_cars &lt;- mtcars_db |&gt;\n  filter(mpg &gt; 25) |&gt;\n  select(mpg, cyl, hp, wt) |&gt;\n  arrange(desc(mpg))\n\n# See the generated SQL\nefficient_cars |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT mpg, cyl, hp, wt\n#&gt; FROM mtcars\n#&gt; WHERE (mpg &gt; 25.0)\n#&gt; ORDER BY mpg DESC\n\nCollecting Results\nUse collect() to execute the query and bring results into R:\n\n# Execute query and return results to R\nefficient_cars |&gt; collect()\n\n\n\n\nmpg\ncyl\nhp\nwt\n\n\n\n33.9\n4\n65\n1.835\n\n\n32.4\n4\n66\n2.200\n\n\n30.4\n4\n52\n1.615\n\n\n30.4\n4\n113\n1.513\n\n\n27.3\n4\n66\n1.935\n\n\n26.0\n4\n91\n2.140\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnly collect() what you need! Fetching an entire large table defeats the purpose of using a database. Filter and aggregate in the database first, then collect the summarized results.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-dplyr-sql",
    "href": "chapters/15-databases.html#sec-dplyr-sql",
    "title": "13  Working with Databases",
    "section": "\n13.7 Translating dplyr to SQL",
    "text": "13.7 Translating dplyr to SQL\ndbplyr translates dplyr verbs to SQL clauses. Let’s see how common operations map:\nFiltering and Selecting\n\nmtcars_db |&gt;\n  filter(cyl == 6, mpg &gt; 18) |&gt;\n  select(mpg, cyl, hp, wt) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT mpg, cyl, hp, wt\n#&gt; FROM mtcars\n#&gt; WHERE (cyl = 6.0) AND (mpg &gt; 18.0)\n\nMutating (Creating New Columns)\n\nmtcars_db |&gt;\n  mutate(\n    efficiency = mpg / wt,\n    hp_per_cyl = hp / cyl\n  ) |&gt;\n  select(mpg, wt, efficiency, hp_per_cyl) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT mpg, wt, mpg / wt AS efficiency, hp / cyl AS hp_per_cyl\n#&gt; FROM mtcars\n\nGrouping and Summarizing\n\nmtcars_db |&gt;\n  group_by(cyl) |&gt;\n  summarize(\n    n = n(),\n    avg_mpg = mean(mpg, na.rm = TRUE),\n    max_hp = max(hp, na.rm = TRUE)\n  ) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT cyl, COUNT(*) AS n, AVG(mpg) AS avg_mpg, MAX(hp) AS max_hp\n#&gt; FROM mtcars\n#&gt; GROUP BY cyl\n\nArranging (Sorting)\n\nmtcars_db |&gt;\n  arrange(cyl, desc(mpg)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT mtcars.*\n#&gt; FROM mtcars\n#&gt; ORDER BY cyl, mpg DESC",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-sql-syntax",
    "href": "chapters/15-databases.html#sec-sql-syntax",
    "title": "13  Working with Databases",
    "section": "\n13.8 SQL Syntax Details",
    "text": "13.8 SQL Syntax Details\nUnderstanding SQL helps you write better dbplyr code and debug issues.\nKey Differences from R\n\n\nTable 13.4: Syntax differences between R and SQL\n\n\n\nAspect\nR/dplyr\nSQL\n\n\n\nEquality\n==\n=\n\n\nLogical AND\n&\nAND\n\n\nLogical OR\n|\nOR\n\n\nMissing values\nNA\nNULL\n\n\nStrings\n\n\"text\" or 'text'\n\n\n'text' only\n\n\nCase sensitivity\nCase sensitive\nKeywords case insensitive\n\n\n\n\n\n\nNULL Handling\nSQL’s NULL behaves like R’s NA—it’s “contagious” in calculations:\n\nmtcars_db |&gt;\n  filter(!is.na(mpg)) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT mtcars.*\n#&gt; FROM mtcars\n#&gt; WHERE (NOT((mpg IS NULL)))\n\nSubqueries\nSometimes dbplyr generates subqueries—queries nested inside other queries:\n\nmtcars_db |&gt;\n  mutate(mpg_group = if_else(mpg &gt; 20, \"high\", \"low\")) |&gt;\n  filter(mpg_group == \"high\") |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT q01.*\n#&gt; FROM (\n#&gt;   SELECT\n#&gt;     mtcars.*,\n#&gt;     CASE WHEN (mpg &gt; 20.0) THEN 'high' WHEN NOT (mpg &gt; 20.0) THEN 'low' END AS mpg_group\n#&gt;   FROM mtcars\n#&gt; ) q01\n#&gt; WHERE (mpg_group = 'high')\n\nThe subquery is needed because SQL evaluates WHERE before SELECT, so you can’t filter on a column you’re creating in the same query.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-db-multiple-tables",
    "href": "chapters/15-databases.html#sec-db-multiple-tables",
    "title": "13  Working with Databases",
    "section": "\n13.9 Working with Multiple Tables",
    "text": "13.9 Working with Multiple Tables\nDatabases often store data across multiple related tables (normalized data). You can use joins in dbplyr just like with regular data frames:\n\n# Join tables in the database\norders_db |&gt;\n  left_join(customers_db, by = \"customer_id\") |&gt;\n  show_query()\n\nThe join is executed in the database, which is much more efficient than loading both tables into R and joining there.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-window-functions",
    "href": "chapters/15-databases.html#sec-window-functions",
    "title": "13  Working with Databases",
    "section": "\n13.10 Window Functions",
    "text": "13.10 Window Functions\nSQL window functions perform calculations across sets of rows related to the current row. dbplyr translates functions like lag(), lead(), and rank() to their SQL equivalents:\n\nmtcars_db |&gt;\n  group_by(cyl) |&gt;\n  mutate(\n    rank_mpg = min_rank(desc(mpg)),\n    mpg_vs_avg = mpg - mean(mpg, na.rm = TRUE)\n  ) |&gt;\n  select(cyl, mpg, rank_mpg, mpg_vs_avg) |&gt;\n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   cyl,\n#&gt;   mpg,\n#&gt;   CASE\n#&gt; WHEN (NOT((mpg IS NULL))) THEN RANK() OVER (PARTITION BY cyl, (CASE WHEN ((mpg IS NULL)) THEN 1 ELSE 0 END) ORDER BY mpg DESC)\n#&gt; END AS rank_mpg,\n#&gt;   mpg - AVG(mpg) OVER (PARTITION BY cyl) AS mpg_vs_avg\n#&gt; FROM mtcars",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-db-best-practices",
    "href": "chapters/15-databases.html#sec-db-best-practices",
    "title": "13  Working with Databases",
    "section": "\n13.11 Best Practices",
    "text": "13.11 Best Practices\n1. Filter Early\nPush filtering to the database rather than collecting all data and filtering in R:\n\n# Good: Filter in database\nmtcars_db |&gt;\n  filter(mpg &gt; 25) |&gt;\n  collect()\n\n# Bad: Collect everything then filter\nmtcars_db |&gt;\n  collect() |&gt;\n  filter(mpg &gt; 25)\n\n2. Select Only Needed Columns\n\n# Good: Select specific columns\nmtcars_db |&gt;\n  select(mpg, cyl, hp) |&gt;\n  collect()\n\n# Bad: Collect all columns\nmtcars_db |&gt;\n  collect() |&gt;\n  select(mpg, cyl, hp)\n\n3. Aggregate in the Database\n\n# Good: Summarize in database\nmtcars_db |&gt;\n  group_by(cyl) |&gt;\n  summarize(avg_mpg = mean(mpg)) |&gt;\n  collect()\n\n# Bad: Collect then summarize\nmtcars_db |&gt;\n  collect() |&gt;\n  group_by(cyl) |&gt;\n  summarize(avg_mpg = mean(mpg))\n\n4. Close Connections\nAlways close your database connection when you’re done:\n\ndbDisconnect(con)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-when-databases",
    "href": "chapters/15-databases.html#sec-when-databases",
    "title": "13  Working with Databases",
    "section": "\n13.12 When to Use Databases",
    "text": "13.12 When to Use Databases\nDatabases are valuable when:\n\n\nData is too large for memory: Even modest databases handle billions of rows\n\nMultiple users need access: Databases manage concurrent access safely\n\nData changes frequently: Updates are immediate, no file copying needed\n\nYou need specific subsets: Queries return just what you need\n\nData integrity matters: Constraints prevent invalid data\n\nFor smaller datasets that fit comfortably in memory, regular data frames are simpler and faster for exploratory analysis.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#sec-db-summary",
    "href": "chapters/15-databases.html#sec-db-summary",
    "title": "13  Working with Databases",
    "section": "\n13.13 Summary",
    "text": "13.13 Summary\nIn this chapter, you learned how to:\n\nConnect to databases from R using DBI\nUse duckdb as a lightweight analytical database\nWrite basic SQL queries with SELECT, WHERE, and GROUP BY\nUse dbplyr to work with databases using familiar dplyr syntax\nTranslate dplyr operations to SQL and understand the generated queries\nApply best practices for efficient database access\n\nThe combination of dbplyr and DBI lets you leverage your existing tidyverse skills while accessing data at any scale. When you need to work with data too large for memory or collaborate with others through a shared database, these tools let you stay productive without learning an entirely new programming paradigm.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#exercises",
    "href": "chapters/15-databases.html#exercises",
    "title": "13  Working with Databases",
    "section": "Exercises",
    "text": "Exercises\n\nBasic Connection: Connect to a duckdb database and load the nycflights13::flights data into a table called “flights”.\n\nSQL Practice: Write SQL queries to:\n\nFind all flights from JFK airport\nCalculate the average departure delay by carrier\nFind the 10 destinations with the most flights\n\n\ndbplyr Translation: Use dbplyr to write the equivalent dplyr code for each SQL query above. Compare the generated SQL with your hand-written queries.\n\nEfficient Querying: Given a database with a large “sales” table:\n\nWhat’s wrong with: tbl(con, \"sales\") |&gt; collect() |&gt; filter(year == 2023)?\nHow would you rewrite this query efficiently?\n\n\n\nJoins in Databases: If you have two tables, “experiments” and “samples”, linked by sample_id:\n\nWrite dbplyr code to join them\nUse show_query() to see the generated SQL\nExplain when the join happens (in R or in the database)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/15-databases.html#additional-resources",
    "href": "chapters/15-databases.html#additional-resources",
    "title": "13  Working with Databases",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDBI package documentation\ndbplyr package documentation\nduckdb for R\n\nSQL for Data Scientists by Renée M. P. Teate\n\nPractical SQL by Anthony DeBarros",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with Databases</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html",
    "href": "chapters/12-quarto-documents.html",
    "title": "14  Reproducible Documents with Quarto",
    "section": "",
    "text": "14.1 What is Quarto?\nQuarto is an open-source scientific publishing system that enables you to create dynamic documents combining narrative text, code, and results. It represents the next generation of literate programming tools, building on the legacy of R Markdown while extending support to multiple programming languages.\nQuarto documents are plain text files with the .qmd extension that can be rendered to:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-what-is-quarto",
    "href": "chapters/12-quarto-documents.html#sec-what-is-quarto",
    "title": "14  Reproducible Documents with Quarto",
    "section": "",
    "text": "HTML documents and websites\nPDF documents\nMicrosoft Word documents\nPresentations (PowerPoint, reveal.js)\nBooks (like this one!)\nInteractive dashboards\n\nWhy Use Quarto?\nQuarto solves fundamental problems in scientific communication:\n\nReproducibility\n\nYour analysis and its documentation live together. Anyone can re-run your code and verify your results.\n\nEfficiency\n\nNo more copying and pasting results between applications. Update your data, re-render, and all figures and statistics update automatically.\n\nTransparency\n\nReaders can see exactly how results were generated. The code is the methods section.\n\nCollaboration\n\nPlain text files work seamlessly with version control systems like Git.\n\nFlexibility\n\nWrite once, render to multiple formats. The same document can produce a lab report, a presentation, and a web page.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis entire textbook was written in Quarto! You can see the source files in the repository.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-markdown",
    "href": "chapters/12-quarto-documents.html#sec-markdown",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.2 Markdown: The Foundation",
    "text": "14.2 Markdown: The Foundation\nMarkdown is a lightweight markup language that lets you add formatting to plain text. Created by John Gruber in 2004, it’s designed to be readable in its raw form while rendering to beautifully formatted documents.\nWhy Markdown?\n\n\nSimple: No complex menus or formatting palettes\n\nPortable: Plain text files work everywhere\n\nFuture-proof: Text files will always be readable\n\nVersion-control friendly: Git can track every change\nBasic Text Formatting\n*italic text* or _italic text_\n\n**bold text** or __bold text__\n\n***bold and italic*** or ___bold and italic___\n\n~~strikethrough text~~\n\n`inline code`\nRenders as:\nitalic text or italic text\nbold text or bold text\nbold and italic or bold and italic\nstrikethrough text\ninline code\nHeaders\nCreate hierarchical structure with headers:\n# Header 1 (Chapter title)\n## Header 2 (Section)\n### Header 3 (Subsection)\n#### Header 4 (Sub-subsection)\n\n\n\n\n\n\nTip\n\n\n\nIn Quarto books, each chapter typically starts with a single # header, with subsequent sections using ## and ###.\n\n\nLists\nUnordered lists use asterisks, hyphens, or plus signs:\n- Item one\n- Item two\n  - Nested item (indent with 2 spaces)\n  - Another nested item\n- Item three\n\nItem one\nItem two\n\nNested item (indent with 2 spaces)\nAnother nested item\n\n\nItem three\n\nOrdered lists use numbers:\n1. First step\n2. Second step\n3. Third step\n   1. Sub-step\n   2. Another sub-step\n\nFirst step\nSecond step\nThird step\n\nSub-step\nAnother sub-step\n\n\nLinks and Images\n[Link text](https://example.com)\n\n[Link with title](https://example.com \"Hover text\")\n\n![Image caption](images/figure.png)\n\n![Caption with size](images/figure.png){width=\"50%\"}\nBlockquotes\n&gt; This is a blockquote. Use it for quotations\n&gt; or to highlight important text.\n&gt;\n&gt; Multiple paragraphs work too.\n\nThis is a blockquote. Use it for quotations or to highlight important text.\nMultiple paragraphs work too.\n\nCode Blocks\nFor displaying code without execution, use fenced code blocks with language specification:\n```python\ndef hello():\n    print(\"Hello, World!\")\n```\ndef hello():\n    print(\"Hello, World!\")\nTables\nCreate tables with pipes and hyphens:\n| Column 1 | Column 2 | Column 3 |\n|:---------|:--------:|---------:|\n| Left     | Center   | Right    |\n| aligned  | aligned  | aligned  |\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\nLeft\nCenter\nRight\n\n\naligned\naligned\naligned\n\n\n\nThe colons indicate alignment: :--- left, :---: center, ---: right.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-quarto-structure",
    "href": "chapters/12-quarto-documents.html#sec-quarto-structure",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.3 Quarto Document Structure",
    "text": "14.3 Quarto Document Structure\nA Quarto document has three main parts:\n\n\nYAML Header: Document metadata and settings\n\nMarkdown Content: Text and formatting\n\nCode Chunks: Executable code blocks\n\nYAML Header\nThe YAML header appears at the top of the document, enclosed by triple dashes:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nYAML stands for “YAML Ain’t Markup Language” (a recursive acronym). It uses key-value pairs with consistent indentation.\nCommon YAML Options\n---\ntitle: \"Genomic Analysis Report\"\nauthor: \"Jane Scientist\"\ndate: \"2025-01-15\"\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    toc-depth: 3\n    code-fold: true\nexecute:\n  echo: true\n  warning: false\n---\n\n\nTable 14.1: Common YAML header options\n\n\n\nOption\nDescription\n\n\n\ntitle\nDocument title\n\n\nauthor\nAuthor name(s)\n\n\ndate\nPublication date (today for current date)\n\n\nformat\nOutput format(s)\n\n\ntoc\nInclude table of contents\n\n\ntoc-depth\nHow many header levels in TOC\n\n\ncode-fold\nCollapse code by default\n\n\nexecute\nGlobal code execution options",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-code-chunks",
    "href": "chapters/12-quarto-documents.html#sec-code-chunks",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.4 Code Chunks",
    "text": "14.4 Code Chunks\nCode chunks are the heart of Quarto’s power—they let you embed executable code directly in your document.\nBasic Syntax\nIn Quarto, code chunks use this syntax:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Your R code here\nmean(c(1, 2, 3, 4, 5))\n#&gt; [1] 3\n```\n:::\n\n# Your R code here\nmean(c(1, 2, 3, 4, 5))\n#&gt; [1] 3\n\nChunk Options\nControl chunk behavior with special comments starting with #|:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata &lt;- c(1, 2, 3, 4, 5)\nmean(data)\n#&gt; [1] 3\n```\n:::\nEssential Chunk Options\n\n\nTable 14.2: Code chunk options\n\n\n\nOption\nDescription\nDefault\n\n\n\necho\nShow code in output\ntrue\n\n\neval\nExecute the code\ntrue\n\n\ninclude\nInclude chunk in output\ntrue\n\n\noutput\nShow results\ntrue\n\n\nwarning\nShow warnings\ntrue\n\n\nmessage\nShow messages\ntrue\n\n\nerror\nContinue on error\nfalse\n\n\ncache\nCache results\nfalse\n\n\n\n\n\n\nControlling Output\n\n```{r}\n#| label: output-examples\n\n# echo: true shows this code\nx &lt;- rnorm(100)\nsummary(x)\n```\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; -2.14426 -0.67007  0.13944  0.03042  0.54618  2.79007\n\nHide code, show results (great for final reports):\n\n\n\n\n\n\n\n\nShow code, hide results (for teaching):\n\n# This code is displayed but not executed\ninstall.packages(\"new_package\")  # Don't actually run this\n\nFigure Options\nControl figure appearance:\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point(color = \"steelblue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"coral\") +\n  labs(x = \"Weight (1000 lbs)\", y = \"Miles per Gallon\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 14.1: A demonstration plot with custom dimensions\n\n\n\n\n\n\nTable 14.3: Figure chunk options\n\n\n\nOption\nDescription\n\n\n\nfig-cap\nFigure caption\n\n\nfig-width\nWidth in inches\n\n\nfig-height\nHeight in inches\n\n\nfig-align\nAlignment: left, center, right\n\n\nout-width\nOutput width (e.g., “80%”)\n\n\nfig-format\nFormat: png, svg, pdf",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-math-equations",
    "href": "chapters/12-quarto-documents.html#sec-math-equations",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.5 Mathematical Equations",
    "text": "14.5 Mathematical Equations\nQuarto supports LaTeX math notation for equations.\nInline Math\nUse single dollar signs for inline equations:\nThe mean is calculated as $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$.\nThe mean is calculated as \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\).\nDisplay Math\nUse double dollar signs for centered equations:\n$$\n\\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n$$\n\\[\n\\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\]\nCommon Math Notation\n\n\nTable 14.4: Common LaTeX math notation\n\n\n\nSymbol\nCode\nResult\n\n\n\nGreek letters\n$\\alpha, \\beta, \\gamma$\n\\(\\alpha, \\beta, \\gamma\\)\n\n\nSubscript\n$x_1, x_{12}$\n\\(x_1, x_{12}\\)\n\n\nSuperscript\n$x^2, x^{10}$\n\\(x^2, x^{10}\\)\n\n\nFraction\n$\\frac{a}{b}$\n\\(\\frac{a}{b}\\)\n\n\nSquare root\n$\\sqrt{x}$\n\\(\\sqrt{x}\\)\n\n\nSum\n$\\sum_{i=1}^{n}$\n\\(\\sum_{i=1}^{n}\\)\n\n\nProduct\n$\\prod_{i=1}^{n}$\n\\(\\prod_{i=1}^{n}\\)\n\n\nIntegral\n$\\int_0^1 f(x)dx$\n\\(\\int_0^1 f(x)dx\\)\n\n\n\n\n\n\nExample: Statistical Formulas\nThe probability density function of a normal distribution:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\]\nA regression equation:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-callout-blocks",
    "href": "chapters/12-quarto-documents.html#sec-callout-blocks",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.6 Callout Blocks",
    "text": "14.6 Callout Blocks\nQuarto provides special callout blocks for highlighting information:\n::: {.callout-note}\nThis is a note callout for additional information.\n:::\n\n::: {.callout-warning}\nThis warns readers about potential issues.\n:::\n\n::: {.callout-important}\nThis highlights critical information.\n:::\n\n::: {.callout-tip}\nThis provides helpful tips.\n:::\n\n::: {.callout-caution}\nThis urges caution.\n:::\n\n\n\n\n\n\nNote\n\n\n\nThis is a note callout for additional information.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis warns readers about potential issues.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis provides helpful tips.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-cross-references",
    "href": "chapters/12-quarto-documents.html#sec-cross-references",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.7 Cross-References",
    "text": "14.7 Cross-References\nReference figures, tables, and sections:\nFigures and Tables\nSee @fig-example for the scatter plot.\nThe options are listed in @tbl-chunk-options.\nSee Figure 14.1 for the scatter plot. The options are listed in Table 14.2.\nSections\nAs discussed in @sec-quarto-documents, Quarto enables...\nAs discussed in Chapter 14, Quarto enables reproducible documents.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-rendering",
    "href": "chapters/12-quarto-documents.html#sec-rendering",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.8 Rendering Documents",
    "text": "14.8 Rendering Documents\nFrom RStudio\n\nClick the Render button, or\nUse the keyboard shortcut: Ctrl+Shift+K (Windows/Linux) or Cmd+Shift+K (Mac)\nFrom Command Line\n# Render to default format\n$ quarto render document.qmd\n\n# Render to specific format\n$ quarto render document.qmd --to pdf\n\n# Render all documents in a project\n$ quarto render\nLive Preview\nFor interactive editing:\n$ quarto preview document.qmd\nThis opens a browser window that automatically updates as you edit.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-output-formats",
    "href": "chapters/12-quarto-documents.html#sec-output-formats",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.9 Multiple Output Formats",
    "text": "14.9 Multiple Output Formats\nRender to multiple formats from one source:\n---\ntitle: \"My Report\"\nformat:\n  html:\n    theme: cosmo\n  pdf:\n    documentclass: article\n  docx: default\n---\nThen render all formats:\n$ quarto render document.qmd",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-quarto-projects",
    "href": "chapters/12-quarto-documents.html#sec-quarto-projects",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.10 Project Organization",
    "text": "14.10 Project Organization\nFor multi-document projects (like this book), use a _quarto.yml file:\nproject:\n  type: book\n  output-dir: docs\n\nbook:\n  title: \"My Analysis Book\"\n  author: \"Your Name\"\n  chapters:\n    - index.qmd\n    - intro.qmd\n    - methods.qmd\n    - results.qmd\n    - references.qmd\n\nformat:\n  html:\n    theme: cosmo",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-quarto-best-practices",
    "href": "chapters/12-quarto-documents.html#sec-quarto-best-practices",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.11 Best Practices",
    "text": "14.11 Best Practices\nDocument Structure\n\n\nStart with clear objectives: What should readers learn?\n\nOrganize hierarchically: Use headers to create logical sections\n\nBalance code and narrative: Explain the “why” not just the “what”\n\nUse meaningful chunk labels: #| label: fig-gene-expression not #| label: chunk1\n\nCode Quality\n\n\nShow relevant code: Hide boilerplate, show analysis\n\nComment your code: Explain complex operations\n\nUse consistent style: Follow a style guide (tidyverse, etc.)\n\nSet global options: Configure defaults in YAML, override as needed\nReproducibility\n\n\nDocument your environment: Include package versions\n\nUse relative paths: Avoid /Users/yourname/...\n\n\nSet a random seed: For reproducible random operations\n\nVersion control: Track your .qmd files with Git\n\n\n# Document your R environment\nsessionInfo()\n#&gt; R version 4.4.2 (2024-10-31)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#&gt; \n#&gt; locale:\n#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#&gt; \n#&gt; time zone: America/Los_Angeles\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] gt_1.2.0        lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0  \n#&gt;  [5] dplyr_1.1.4     purrr_1.2.0     readr_2.1.6     tidyr_1.3.2    \n#&gt;  [9] tibble_3.3.0    ggplot2_4.0.1   tidyverse_2.0.0\n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;  [1] Matrix_1.7-4       gtable_0.3.6       jsonlite_2.0.0     compiler_4.4.2    \n#&gt;  [5] tidyselect_1.2.1   xml2_1.5.1         splines_4.4.2      scales_1.4.0      \n#&gt;  [9] yaml_2.3.12        fastmap_1.2.0      lattice_0.22-7     R6_2.6.1          \n#&gt; [13] labeling_0.4.3     generics_0.1.4     knitr_1.51         htmlwidgets_1.6.4 \n#&gt; [17] pillar_1.11.1      RColorBrewer_1.1-3 tzdb_0.5.0         rlang_1.1.6       \n#&gt; [21] stringi_1.8.7      xfun_0.55          fs_1.6.6           S7_0.2.1          \n#&gt; [25] otel_0.2.0         timechange_0.3.0   cli_3.6.5          mgcv_1.9-4        \n#&gt; [29] withr_3.0.2        magrittr_2.0.4     digest_0.6.39      grid_4.4.2        \n#&gt; [33] rstudioapi_0.17.1  hms_1.1.4          nlme_3.1-168       lifecycle_1.0.4   \n#&gt; [37] vctrs_0.6.5        evaluate_1.0.5     glue_1.8.0         farver_2.1.2      \n#&gt; [41] rmarkdown_2.30     tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.9",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#sec-quarto-summary",
    "href": "chapters/12-quarto-documents.html#sec-quarto-summary",
    "title": "14  Reproducible Documents with Quarto",
    "section": "\n14.12 Summary",
    "text": "14.12 Summary\nQuarto transforms how we communicate scientific work:\n\n\nMarkdown provides simple, readable formatting syntax\n\nYAML headers configure document settings\n\nCode chunks embed executable, reproducible analysis\n\nMultiple formats serve different audiences from one source\n\nCross-references create professional, navigable documents\n\nThe investment in learning Quarto pays dividends every time you write a report, prepare a presentation, or publish research. Your future self (and your collaborators) will thank you for the reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#additional-reading",
    "href": "chapters/12-quarto-documents.html#additional-reading",
    "title": "14  Reproducible Documents with Quarto",
    "section": "Additional Reading",
    "text": "Additional Reading\n\n\nQuarto Documentation — Official comprehensive guide\n\nQuarto Gallery — Examples and templates\n\nR Markdown Cookbook — Many techniques apply to Quarto\n\nMarkdown Guide — Comprehensive Markdown reference",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/12-quarto-documents.html#exercises",
    "href": "chapters/12-quarto-documents.html#exercises",
    "title": "14  Reproducible Documents with Quarto",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: First Quarto Document\n\nCreate a new file called first_report.qmd\n\nAdd a YAML header with title, author, and date\nWrite a brief introduction using headers, bold, and italic text\nInclude a bulleted list of your analysis goals\nRender to HTML\n\nExercise 2: Code Integration\n\nCreate a document that loads a dataset (use mtcars or similar)\nAdd a code chunk that calculates summary statistics\nAdd a code chunk that creates a figure with a caption\nUse chunk options to hide code but show results for one chunk\nUse chunk options to show code but hide results for another\n\nExercise 3: Mathematical Notation\n\nWrite a document explaining a statistical concept\nInclude at least one inline equation\nInclude at least one display equation\nUse appropriate callout blocks for tips and warnings\n\nExercise 4: Multi-Format Output\n\nConfigure your document to render to both HTML and PDF\nRender both versions\nNote any differences in how content appears\nAdjust settings to optimize for each format",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Reproducible Documents with Quarto</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html",
    "href": "chapters/11-hpc-talapas.html",
    "title": "15  High-Performance Computing with Talapas",
    "section": "",
    "text": "15.1 What is High-Performance Computing?\nWhen your laptop isn’t enough—when analyses run for days, require hundreds of gigabytes of memory, or need specialized hardware like GPUs—you need high-performance computing (HPC).\nAn HPC cluster is a collection of interconnected computers (called nodes) that work together to solve computational problems. Instead of upgrading your personal computer indefinitely, you submit jobs to a shared resource with far more capability.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-what-is-hpc",
    "href": "chapters/11-hpc-talapas.html#sec-what-is-hpc",
    "title": "15  High-Performance Computing with Talapas",
    "section": "",
    "text": "When to Use HPC\nConsider HPC when:\n\nAnalyses take hours or days on your laptop\nData is too large to fit in your computer’s memory\nYou need GPUs for machine learning\nYou want to run many similar jobs in parallel\nYour analysis would monopolize your personal computer\nTalapas: UO’s Computing Cluster\nTalapas is the University of Oregon’s HPC cluster, named from a Chinook word for coyote (the educator and keeper of knowledge). It’s managed by Research Advanced Computing Services (RACS).\nKey resources:\n\nOver 14,520 CPU cores\n129 TB of total memory\n89 GPUs (including NVIDIA A100 and H100)\n3+ petabytes of storage",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-talapas-architecture",
    "href": "chapters/11-hpc-talapas.html#sec-talapas-architecture",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.2 Talapas Architecture",
    "text": "15.2 Talapas Architecture\nUnderstanding the cluster architecture helps you use it effectively.\n\n\n\n\n\nflowchart TB\n    U[Your Computer] --&gt; SSH([SSH])\n    SSH --&gt; L1[Login Node 1]\n    SSH --&gt; L2[Login Node 2]\n    SSH --&gt; L3[Login Node 3]\n    SSH --&gt; L4[Login Node 4]\n    L1 --&gt; S[SLURM Scheduler]\n    L2 --&gt; S\n    L3 --&gt; S\n    L4 --&gt; S\n    S --&gt; C1[CPU Nodes]\n    S --&gt; C2[GPU Nodes]\n    S --&gt; C3[High-Memory Nodes]\n    C1 --&gt; ST[(Shared Storage)]\n    C2 --&gt; ST\n    C3 --&gt; ST\n    \n    style U fill:#e3f2fd,stroke:#333,stroke-width:2px\n    style L1 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L2 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L3 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L4 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style S fill:#f9d71c,stroke:#333,stroke-width:2px\n    style C1 fill:#98fb98,stroke:#333,stroke-width:2px\n    style C2 fill:#ffa07a,stroke:#333,stroke-width:2px\n    style C3 fill:#dda0dd,stroke:#333,stroke-width:2px\n    style ST fill:#c8e6c9,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 15.1: Talapas cluster architecture showing login nodes, scheduler, and compute nodes\n\n\n\n\nLogin Nodes vs. Compute Nodes\n\n\n\n\n\n\nNever Run Jobs on Login Nodes\n\n\n\nLogin nodes are where you:\n\nLog in and manage files\nEdit scripts\nSubmit jobs\nCheck job status\n\nDo NOT run computationally intensive work on login nodes! They’re shared by everyone, and heavy processes will be terminated.\nCompute nodes are where your actual work runs, accessed through the job scheduler.\n\n\nNode Types\n\n\nTable 15.1: Talapas node types\n\n\n\nNode Type\nCores\nMemory\nPurpose\n\n\n\nStandard CPU\n28-128\n128-512 GB\nGeneral computation\n\n\nLarge Memory\n56\n1-4 TB\nMemory-intensive work\n\n\nGPU\n28-48\n256-512 GB\nMachine learning, CUDA\n\n\n\n\n\n\nPartitions (Queues)\nJobs are submitted to partitions with different characteristics:\n\n\nTable 15.2: Talapas partitions\n\n\n\nPartition\nTime Limit\nNotes\n\n\n\ncompute\n24 hours\nGeneral purpose (default)\n\n\nshort\n24 hours\nShort jobs\n\n\nlong\n14 days\nExtended jobs\n\n\n\ngpu / longgpu\n\n24h / 14d\nGPU-enabled nodes\n\n\n\nfat / longfat\n\n24h / 14d\nHigh-memory nodes\n\n\npreempt\nvaries\nCan use idle resources (may be interrupted)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-getting-access",
    "href": "chapters/11-hpc-talapas.html#sec-getting-access",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.3 Getting Access",
    "text": "15.3 Getting Access\nStep 1: Join a PIRG\nA PIRG (Principal Investigator Research Group) is your access group. Your PI must add you to their PIRG, or contact RACS to create a new one.\nStep 2: Request Access\nVisit racs.uoregon.edu/request-access\nYour credentials:\n\n\nUsername: Your Duck ID\n\nPassword: Your UO password",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-connecting-talapas",
    "href": "chapters/11-hpc-talapas.html#sec-connecting-talapas",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.4 Connecting to Talapas",
    "text": "15.4 Connecting to Talapas\nOption 1: SSH (Command Line)\nFrom your terminal:\n# Connect to Talapas\n$ ssh yourduckid@login.talapas.uoregon.edu\n\n# Or connect to a specific login node\n$ ssh yourduckid@login1.talapas.uoregon.edu\nOff campus? Connect to the UO VPN first.\nWindows users: Use PuTTY, MobaXterm, Windows Terminal, or VS Code with the Remote-SSH extension.\nOption 2: Open OnDemand (Web Browser)\nFor a graphical interface, use Open OnDemand:\nondemand.talapas.uoregon.edu\nOpen OnDemand provides:\n\nFile browser for uploads/downloads\nInteractive applications (RStudio, JupyterLab, MATLAB)\nJob composer and monitoring\nTerminal in browser\n\nThis is excellent for beginners and interactive data analysis.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-talapas-storage",
    "href": "chapters/11-hpc-talapas.html#sec-talapas-storage",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.5 Storage on Talapas",
    "text": "15.5 Storage on Talapas\n\n\n\n\n\nflowchart LR\n    R[\"/\"] --&gt; H[\"/home\"]\n    R --&gt; P[\"/projects\"]\n    H --&gt; HD[\"/home/duckid&lt;br/&gt;250 GB\"]\n    P --&gt; PG[\"/projects/PIRG\"]\n    PG --&gt; PGU[\"/projects/PIRG/duckid&lt;br/&gt;Your workspace\"]\n    PG --&gt; PGS[\"/projects/PIRG/shared&lt;br/&gt;Shared with group\"]\n    \n    style HD fill:#87ceeb,stroke:#333,stroke-width:2px\n    style PGU fill:#98fb98,stroke:#333,stroke-width:2px\n    style PGS fill:#f9d71c,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 15.2: Talapas storage hierarchy\n\n\n\n\nDirectory Structure\n\n\nTable 15.3: Talapas storage locations\n\n\n\n\n\n\n\n\nLocation\nQuota\nPurpose\n\n\n\n/home/duckid\n250 GB\nConfig files, small scripts\n\n\n/projects/PIRG/duckid\nShared (default 2 TB)\nLarge data, analysis\n\n\n/projects/PIRG/shared\nShared\nFiles shared with group\n\n\n\n/tmp (on compute nodes)\nLocal to node\nTemporary scratch space\n\n\n\n\n\n\nCheck Your Storage\n# Home directory usage\n$ df -h ~\n\n# Project directory usage\n$ du -sh /projects/myPIRG/myusername\n\n# Detailed breakdown\n$ du -h --max-depth=1 /projects/myPIRG/myusername\n\n\n\n\n\n\nImportant\n\n\n\nData is NOT automatically backed up! You are responsible for backing up important files. Consider using Git for code and external storage for irreplaceable data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-slurm",
    "href": "chapters/11-hpc-talapas.html#sec-slurm",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.6 SLURM: The Job Scheduler",
    "text": "15.6 SLURM: The Job Scheduler\nSLURM (Simple Linux Utility for Resource Management) is the software that manages the cluster (Yoo et al., 2003). It:\n\nAllocates resources fairly among users\nQueues jobs when resources are busy\nTracks resource usage\n\n\n\n\n\n\nflowchart LR\n    A[Submit Job] --&gt; SB([sbatch])\n    SB --&gt; B[Queue]\n    B --&gt; W([Wait])\n    W --&gt; C[Run on Compute Node]\n    C --&gt; CO([Complete])\n    CO --&gt; D[Output Files]\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 15.3: SLURM job lifecycle from submission to completion",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-slurm-scripts",
    "href": "chapters/11-hpc-talapas.html#sec-slurm-scripts",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.7 Writing SLURM Batch Scripts",
    "text": "15.7 Writing SLURM Batch Scripts\nA batch script combines SLURM directives with your commands:\n#!/bin/bash\n#SBATCH --job-name=my_analysis      # Job name (shows in queue)\n#SBATCH --partition=compute         # Partition (queue)\n#SBATCH --account=myPIRG            # Your PIRG name\n#SBATCH --output=output_%j.log      # Standard output (%j = job ID)\n#SBATCH --error=error_%j.log        # Standard error\n#SBATCH --time=04:00:00             # Time limit (HH:MM:SS)\n#SBATCH --nodes=1                   # Number of nodes\n#SBATCH --ntasks-per-node=1         # Tasks per node\n#SBATCH --cpus-per-task=4           # CPUs per task\n#SBATCH --mem=16G                   # Memory per node\n\n# Load required modules\nmodule load R/4.3.3\n\n# Change to working directory\ncd /projects/myPIRG/myusername/analysis\n\n# Run your program\nRscript my_analysis.R\nUnderstanding SLURM Directives\n\n\nTable 15.4: SLURM directives reference\n\n\n\nDirective\nDescription\nExample\n\n\n\n--partition\nQueue to use\n\ncompute, gpu, fat\n\n\n\n--account\nPIRG for accounting\nmyPIRG\n\n\n--time\nMaximum runtime\n\n1-12:00:00 (1 day, 12 hrs)\n\n\n--nodes\nNumber of nodes\n1\n\n\n--cpus-per-task\nCores per task\n8\n\n\n--mem\nTotal memory per node\n32G\n\n\n--mem-per-cpu\nMemory per CPU\n4G\n\n\n--gres\nGeneric resources\ngpu:1\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRequest only what you need! Smaller resource requests often run sooner. Start conservative and increase if jobs fail.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-job-management",
    "href": "chapters/11-hpc-talapas.html#sec-job-management",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.8 Submitting and Managing Jobs",
    "text": "15.8 Submitting and Managing Jobs\nSubmit a Job\n# Submit batch job\n$ sbatch my_script.sh\nSubmitted batch job 12345678\nCheck Job Status\n# Your jobs\n$ squeue -u $USER\n\n# All jobs on a partition\n$ squeue -p compute\n\n# Detailed job info\n$ scontrol show job 12345678\nJob Status Codes\n\n\nTable 15.5: SLURM job status codes\n\n\n\nCode\nMeaning\n\n\n\nPD\nPending (waiting in queue)\n\n\nR\nRunning\n\n\nCG\nCompleting\n\n\nCD\nCompleted\n\n\nF\nFailed\n\n\nCA\nCancelled\n\n\n\n\n\n\nCancel a Job\n# Cancel specific job\n$ scancel 12345678\n\n# Cancel all your jobs\n$ scancel -u $USER\nCheck Resource Usage\nAfter a job completes:\n$ sacct -j 12345678 --format=JobID,JobName,Elapsed,MaxRSS,State",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-interactive-jobs",
    "href": "chapters/11-hpc-talapas.html#sec-interactive-jobs",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.9 Interactive Jobs",
    "text": "15.9 Interactive Jobs\nFor testing and development, use interactive jobs instead of batch:\n# Start interactive session\n$ srun --pty --partition=compute --account=myPIRG \\\n       --time=2:00:00 --cpus-per-task=4 --mem=8G /bin/bash\nYour prompt will change from login1 to something like n049, indicating you’re on a compute node.\nInteractive jobs are ideal for:\n\nTesting workflows before batch submission\nDebugging code\nQuick data exploration\nRunning GUI applications via Open OnDemand",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-modules",
    "href": "chapters/11-hpc-talapas.html#sec-modules",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.10 Software Modules",
    "text": "15.10 Software Modules\nTalapas uses the Lmod module system to manage software. This lets multiple versions coexist without conflicts.\nBasic Module Commands\n# List available modules\n$ module avail\n\n# Search for specific software\n$ module spider R\n$ module spider python\n\n# Load a module\n$ module load R/4.3.3\n$ module load miniconda3/20240410\n\n# See loaded modules\n$ module list\n\n# Unload a module\n$ module unload R\n\n# Reset to default state\n$ module purge\nIn Batch Scripts\n\n\n\n\n\n\nImportant\n\n\n\nAlways load modules after the #SBATCH directives in your batch scripts:\n#!/bin/bash\n#SBATCH --job-name=analysis\n#SBATCH --partition=compute\n#SBATCH --account=myPIRG\n# ... other directives ...\n\n# Load modules here\nmodule load R/4.3.3\n\n# Now run your code\nRscript analysis.R",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-gpu-jobs",
    "href": "chapters/11-hpc-talapas.html#sec-gpu-jobs",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.11 GPU Jobs",
    "text": "15.11 GPU Jobs\nFor machine learning and GPU-accelerated computing:\n#!/bin/bash\n#SBATCH --job-name=gpu_job\n#SBATCH --partition=gpu             # GPU partition\n#SBATCH --account=myPIRG\n#SBATCH --time=1-00:00:00           # 1 day\n#SBATCH --nodes=1\n#SBATCH --gres=gpu:1                # Request 1 GPU\n#SBATCH --mem=32G\n\n# Load CUDA and your software\nmodule load cuda/12.4\nmodule load miniconda3/20240410\n\n# Activate your environment\nconda activate ml_env\n\n# Run GPU code\npython train_model.py\nCheck available GPUs:\n$ /packages/racs/bin/slurm-show-gpus",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-hpc-example",
    "href": "chapters/11-hpc-talapas.html#sec-hpc-example",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.12 A Complete Example",
    "text": "15.12 A Complete Example\nLet’s walk through a realistic bioinformatics workflow:\n1. Prepare Your Script\nCreate genome_analysis.sh:\n#!/bin/bash\n#SBATCH --job-name=genome_analysis\n#SBATCH --partition=compute\n#SBATCH --account=myPIRG\n#SBATCH --output=logs/genome_%j.out\n#SBATCH --error=logs/genome_%j.err\n#SBATCH --time=08:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n\n# Exit on error\nset -e\n\n# Load modules\nmodule load bwa/0.7.17\nmodule load samtools/1.17\n\n# Set variables\nGENOME=\"/projects/myPIRG/shared/reference/human_GRCh38.fa\"\nREADS_R1=\"/projects/myPIRG/myuser/data/sample_R1.fastq.gz\"\nREADS_R2=\"/projects/myPIRG/myuser/data/sample_R2.fastq.gz\"\nOUTPUT=\"/projects/myPIRG/myuser/results\"\n\n# Create output directory\nmkdir -p $OUTPUT\n\n# Log start time\necho \"Starting analysis at $(date)\"\n\n# Run alignment\nbwa mem -t $SLURM_CPUS_PER_TASK $GENOME $READS_R1 $READS_R2 | \\\n    samtools sort -@ 4 -o $OUTPUT/aligned.bam\n\n# Index BAM file\nsamtools index $OUTPUT/aligned.bam\n\n# Log completion\necho \"Analysis completed at $(date)\"\n2. Submit and Monitor\n# Create logs directory\n$ mkdir -p logs\n\n# Submit job\n$ sbatch genome_analysis.sh\nSubmitted batch job 12345678\n\n# Monitor status\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n          12345678   compute genome_a   duckid   R       5:32      1 n042\n\n# Check output in real-time\n$ tail -f logs/genome_12345678.out",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-hpc-best-practices",
    "href": "chapters/11-hpc-talapas.html#sec-hpc-best-practices",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.13 Best Practices",
    "text": "15.13 Best Practices\nDo\n\nRequest only the resources you need\nTest with small datasets first\nUse --time conservatively (but not too short)\nClean up old files regularly\nUse /projects for large data\nDocument your workflows\nAcknowledge RACS in publications\nDon’t\n\nRun heavy computation on login nodes\nLeave jobs running indefinitely\nStore sensitive data without encryption\nIgnore error messages\nRequest maximum resources “just in case”\nAcknowledgment\nWhen publishing research using Talapas, include this acknowledgment:\n\n“The authors acknowledge Research Advanced Computing Services (RACS) at the University of Oregon for providing computing resources that have contributed to the research results reported within this publication.”",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-hpc-help",
    "href": "chapters/11-hpc-talapas.html#sec-hpc-help",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.14 Getting Help",
    "text": "15.14 Getting Help\nResources\n\n\nDocumentation: uoracs.github.io/talapas2-knowledge-base\n\n\nService Desk: hpcrcf.atlassian.net/servicedesk\n\n\nEmail: racs@uoregon.edu\n\nOffice Hours: Knight Library, Dream Lab 122E (Wednesdays 1-3pm)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#sec-hpc-summary",
    "href": "chapters/11-hpc-talapas.html#sec-hpc-summary",
    "title": "15  High-Performance Computing with Talapas",
    "section": "\n15.15 Summary",
    "text": "15.15 Summary\nHigh-performance computing extends your analytical capabilities beyond personal computers:\n\nTalapas provides thousands of CPUs, GPUs, and terabytes of storage\nConnect via SSH or the Open OnDemand web interface\nSLURM manages job scheduling and resource allocation\nBatch scripts combine resource requests with your commands\nSoftware modules manage different software versions\nResponsible use ensures fair access for everyone\n\nStart simple—submit small test jobs, verify they work, then scale up. The HPC learning curve is real, but the capability it provides is essential for modern computational research.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#additional-reading",
    "href": "chapters/11-hpc-talapas.html#additional-reading",
    "title": "15  High-Performance Computing with Talapas",
    "section": "Additional Reading",
    "text": "Additional Reading\nFor more on HPC and cluster computing:\n\n\nTalapas Knowledge Base — Official UO documentation for Talapas\n\nSLURM Documentation — The official SLURM workload manager documentation\n\nHPC Carpentry — Introductory HPC lessons for researchers\n\nIntroduction to High-Performance Scientific Computing — Comprehensive textbook on HPC concepts\n\nFor workflow management at scale:\n\n\nSnakemake for HPC — Running Snakemake workflows on clusters\n\nNextflow Executors — Running Nextflow on SLURM and other schedulers",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/11-hpc-talapas.html#exercises",
    "href": "chapters/11-hpc-talapas.html#exercises",
    "title": "15  High-Performance Computing with Talapas",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: First Connection\n\nLog into Talapas via SSH\nCheck which login node you’re on with hostname\n\nExplore your home directory and check your quota\nFind your project directory\n\nExercise 2: Module Practice\n\nSearch for available R versions\nLoad R and verify with R --version\n\nSearch for Python versions\nCreate a script that loads both R and Python\n\nExercise 3: Interactive Job\n\nStart an interactive session with 2 CPUs and 4GB memory\nNote how your prompt changes\nRun a simple R command\nExit the interactive session\n\nExercise 4: Batch Job\n\nWrite a batch script that:\n\nRequests 1 hour, 2 CPUs, 8GB memory\nLoads R\nRuns a simple R script that generates a plot\n\n\nSubmit the job\nMonitor its progress\nExamine the output files\n\n\n\n\n\n\n\nYoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple linux utility for resource management.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blischak, J. D., Davenport, E. R., & Wilson, G. (2016). A quick introduction\nto version control with git and GitHub.\n\n\nChacon, S., & Straub, B. (2014). Pro git.\n\n\nKernighan, B. W., & Pike, R. (1984). The UNIX programming\nenvironment.\n\n\nPosit Software, PBC (2024). Quarto:\nOpen-source scientific and technical publishing system.\n\n\nR Core Team (2024). R: A language\nand environment for statistical computing.\n\n\nSandve, G. K. et al. (2013). Ten simple rules for\nreproducible computational research.\n\n\nSoftware Carpentry (2024b). The unix shell.\n\n\nSoftware Carpentry (2024a). Version control with\ngit.\n\n\nWickham, H. (2014). Tidy\ndata.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy,\ntransform, visualize, and model data.\n\n\nYoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple linux utility\nfor resource management.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html",
    "href": "chapters/appendix-commands.html",
    "title": "Appendix A — Command Reference",
    "section": "",
    "text": "A.1 Unix Navigation Commands\nThis appendix provides a quick reference for commands covered throughout the book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#unix-navigation-commands",
    "href": "chapters/appendix-commands.html#unix-navigation-commands",
    "title": "Appendix A — Command Reference",
    "section": "",
    "text": "Table A.1: Navigation commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\npwd\nPrint working directory\npwd\n\n\nls\nList directory contents\nls -la\n\n\ncd\nChange directory\ncd ~/projects\n\n\nmkdir\nCreate directory\nmkdir -p data/raw\n\n\nrmdir\nRemove empty directory\nrmdir old_folder",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#file-operations",
    "href": "chapters/appendix-commands.html#file-operations",
    "title": "Appendix A — Command Reference",
    "section": "A.2 File Operations",
    "text": "A.2 File Operations\n\n\n\nTable A.2: File operation commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ncp\nCopy files/directories\ncp file.txt backup/\n\n\nmv\nMove or rename\nmv old.txt new.txt\n\n\nrm\nRemove files\nrm -r old_directory/\n\n\ntouch\nCreate empty file\ntouch notes.txt\n\n\ncat\nDisplay file contents\ncat data.csv\n\n\nhead\nShow first lines\nhead -n 20 file.txt\n\n\ntail\nShow last lines\ntail -f log.txt\n\n\nless\nPage through file\nless huge_file.txt\n\n\nwc\nCount lines/words/bytes\nwc -l data.csv",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#text-processing",
    "href": "chapters/appendix-commands.html#text-processing",
    "title": "Appendix A — Command Reference",
    "section": "A.3 Text Processing",
    "text": "A.3 Text Processing\n\n\n\nTable A.3: Text processing commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngrep\nSearch for patterns\ngrep \"error\" log.txt\n\n\ngrep -E\nExtended regex\ngrep -E \"gene[0-9]+\"\n\n\ngrep -v\nInvert match\ngrep -v \"^#\" data.txt\n\n\ngrep -c\nCount matches\ngrep -c \"&gt;\" seqs.fa\n\n\ngrep -i\nCase insensitive\ngrep -i \"warning\"\n\n\nsort\nSort lines\nsort -n numbers.txt\n\n\nuniq\nRemove duplicates\nsort \\| uniq -c\n\n\ncut\nExtract columns\ncut -f2 data.tsv\n\n\ntr\nTranslate characters\ntr 'a-z' 'A-Z'\n\n\nsed\nStream editor\nsed 's/old/new/g'\n\n\nawk\nPattern processing\nawk '{print $1}'",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#redirection-and-pipes",
    "href": "chapters/appendix-commands.html#redirection-and-pipes",
    "title": "Appendix A — Command Reference",
    "section": "A.4 Redirection and Pipes",
    "text": "A.4 Redirection and Pipes\n\n\n\nTable A.4: Redirection operators\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&gt;\nRedirect output (overwrite)\nls &gt; files.txt\n\n\n&gt;&gt;\nRedirect output (append)\necho \"done\" &gt;&gt; log.txt\n\n\n&lt;\nRedirect input\nwc -l &lt; data.txt\n\n\n2&gt;\nRedirect stderr\ncmd 2&gt; errors.txt\n\n\n&&gt;\nRedirect both stdout/stderr\ncmd &&gt; all.txt\n\n\n\\|\nPipe to next command\ncat file \\| sort \\| uniq\n\n\n\\|&\nPipe stdout and stderr\ncmd \\|& less",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#file-compression",
    "href": "chapters/appendix-commands.html#file-compression",
    "title": "Appendix A — Command Reference",
    "section": "A.5 File Compression",
    "text": "A.5 File Compression\n\n\n\nTable A.5: Compression commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngzip\nCompress file\ngzip large_file.txt\n\n\ngunzip\nDecompress\ngunzip file.txt.gz\n\n\nzcat\nView compressed\nzcat data.gz \\| head\n\n\nzgrep\nSearch compressed\nzgrep \"pattern\" file.gz\n\n\ntar\nArchive files\ntar -czvf archive.tar.gz dir/\n\n\ntar\nExtract archive\ntar -xzvf archive.tar.gz",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#file-permissions",
    "href": "chapters/appendix-commands.html#file-permissions",
    "title": "Appendix A — Command Reference",
    "section": "A.6 File Permissions",
    "text": "A.6 File Permissions\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nchmod\nChange permissions\nchmod +x script.sh\n\n\nchmod\nNumeric permissions\nchmod 755 script.sh\n\n\nchown\nChange owner\nchown user:group file\n\n\n\n\nPermission Codes\n\n\n\nTable A.6: Permission codes\n\n\n\n\n\nMode\nMeaning\n\n\n\n\nr (4)\nRead\n\n\nw (2)\nWrite\n\n\nx (1)\nExecute\n\n\n755\nrwx for owner, rx for others\n\n\n644\nrw for owner, r for others",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#system-information",
    "href": "chapters/appendix-commands.html#system-information",
    "title": "Appendix A — Command Reference",
    "section": "A.7 System Information",
    "text": "A.7 System Information\n\n\n\nTable A.7: System information commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nwhoami\nCurrent username\nwhoami\n\n\nhostname\nComputer name\nhostname\n\n\ndate\nCurrent date/time\ndate +%Y-%m-%d\n\n\ndf\nDisk space\ndf -h\n\n\ndu\nDirectory size\ndu -sh folder/\n\n\ntop\nRunning processes\ntop\n\n\nps\nProcess status\nps aux\n\n\nkill\nTerminate process\nkill -9 PID",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#network-and-remote",
    "href": "chapters/appendix-commands.html#network-and-remote",
    "title": "Appendix A — Command Reference",
    "section": "A.8 Network and Remote",
    "text": "A.8 Network and Remote\n\n\n\nTable A.8: Network commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nssh\nSecure shell\nssh user@host\n\n\nscp\nSecure copy\nscp file user@host:path/\n\n\nrsync\nSync files\nrsync -avz src/ dest/\n\n\nwget\nDownload file\nwget URL\n\n\ncurl\nTransfer data\ncurl -O URL",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#git-commands",
    "href": "chapters/appendix-commands.html#git-commands",
    "title": "Appendix A — Command Reference",
    "section": "A.9 Git Commands",
    "text": "A.9 Git Commands\n\nSetup and Configuration\n\n\n\nCommand\nDescription\n\n\n\n\ngit config --global user.name \"Name\"\nSet username\n\n\ngit config --global user.email \"email\"\nSet email\n\n\ngit config --list\nShow configuration\n\n\n\n\n\nRepository Operations\n\n\n\nCommand\nDescription\n\n\n\n\ngit init\nInitialize repository\n\n\ngit clone URL\nClone remote repository\n\n\ngit status\nShow current state\n\n\ngit log --oneline\nView commit history\n\n\n\n\n\nMaking Changes\n\n\n\nCommand\nDescription\n\n\n\n\ngit add file\nStage file\n\n\ngit add .\nStage all changes\n\n\ngit commit -m \"message\"\nCommit staged changes\n\n\ngit diff\nShow unstaged changes\n\n\ngit diff --staged\nShow staged changes\n\n\n\n\n\nBranching\n\n\n\nCommand\nDescription\n\n\n\n\ngit branch\nList branches\n\n\ngit branch name\nCreate branch\n\n\ngit checkout name\nSwitch branch\n\n\ngit checkout -b name\nCreate and switch\n\n\ngit merge name\nMerge branch\n\n\ngit branch -d name\nDelete branch\n\n\n\n\n\nRemote Operations\n\n\n\nCommand\nDescription\n\n\n\n\ngit remote -v\nShow remotes\n\n\ngit remote add origin URL\nAdd remote\n\n\ngit push origin branch\nPush to remote\n\n\ngit pull origin branch\nPull from remote\n\n\ngit fetch origin\nFetch without merge",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#slurm-commands",
    "href": "chapters/appendix-commands.html#slurm-commands",
    "title": "Appendix A — Command Reference",
    "section": "A.10 SLURM Commands",
    "text": "A.10 SLURM Commands\n\nJob Submission\n\n\n\nCommand\nDescription\n\n\n\n\nsbatch script.sh\nSubmit batch job\n\n\nsrun --pty bash\nStart interactive job\n\n\n\n\n\nJob Management\n\n\n\nCommand\nDescription\n\n\n\n\nsqueue -u $USER\nShow your jobs\n\n\nsqueue -p partition\nShow partition jobs\n\n\nscancel JOBID\nCancel job\n\n\nscancel -u $USER\nCancel all your jobs\n\n\nscontrol show job JOBID\nJob details\n\n\nsacct -j JOBID\nJob accounting\n\n\n\n\n\nInformation\n\n\n\nCommand\nDescription\n\n\n\n\nsinfo\nPartition information\n\n\nsinfo -p partition\nSpecific partition\n\n\n\n\n\nCommon SBATCH Directives\n#SBATCH --job-name=name       # Job name\n#SBATCH --partition=compute   # Partition\n#SBATCH --account=PIRG        # Account\n#SBATCH --time=HH:MM:SS       # Time limit\n#SBATCH --nodes=1             # Number of nodes\n#SBATCH --cpus-per-task=4     # CPUs per task\n#SBATCH --mem=16G             # Memory\n#SBATCH --gres=gpu:1          # GPUs\n#SBATCH --output=out_%j.log   # Output file\n#SBATCH --error=err_%j.log    # Error file",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#module-commands",
    "href": "chapters/appendix-commands.html#module-commands",
    "title": "Appendix A — Command Reference",
    "section": "A.11 Module Commands",
    "text": "A.11 Module Commands\n\n\n\nCommand\nDescription\n\n\n\n\nmodule avail\nList available modules\n\n\nmodule spider name\nSearch for module\n\n\nmodule load name\nLoad module\n\n\nmodule unload name\nUnload module\n\n\nmodule list\nShow loaded modules\n\n\nmodule purge\nUnload all modules",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#r-quick-reference",
    "href": "chapters/appendix-commands.html#r-quick-reference",
    "title": "Appendix A — Command Reference",
    "section": "A.12 R Quick Reference",
    "text": "A.12 R Quick Reference\n\nBasic Operations\n# Assignment\nx &lt;- 5\n\n# Vectors\nv &lt;- c(1, 2, 3, 4, 5)\n\n# Sequences\n1:10\nseq(0, 1, by = 0.1)\n\n# Basic stats\nmean(v)\nsd(v)\nsummary(v)\n\n\nData Frames\n# Create\ndf &lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n# Access\ndf$x\ndf[1, ]\ndf[, \"x\"]\n\n# Filter\ndf[df$x &gt; 1, ]\n\n# Read/Write\nread.csv(\"file.csv\")\nwrite.csv(df, \"file.csv\", row.names = FALSE)\n\n\nGetting Help\n?function_name\nhelp(function_name)\nexample(function_name)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#regular-expression-metacharacters",
    "href": "chapters/appendix-commands.html#regular-expression-metacharacters",
    "title": "Appendix A — Command Reference",
    "section": "A.13 Regular Expression Metacharacters",
    "text": "A.13 Regular Expression Metacharacters\n\n\n\nTable A.9: Regular expression metacharacters\n\n\n\n\n\nCharacter\nMeaning\n\n\n\n\n.\nAny single character\n\n\n*\nZero or more of preceding\n\n\n+\nOne or more of preceding\n\n\n?\nZero or one of preceding\n\n\n^\nStart of line\n\n\n$\nEnd of line\n\n\n[]\nCharacter class\n\n\n[^]\nNegated class\n\n\n\\|\nAlternation (OR)\n\n\n()\nGrouping\n\n\n{n}\nExactly n repetitions\n\n\n{n,m}\nBetween n and m repetitions\n\n\n\\\nEscape special character",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#useful-shortcuts",
    "href": "chapters/appendix-commands.html#useful-shortcuts",
    "title": "Appendix A — Command Reference",
    "section": "A.14 Useful Shortcuts",
    "text": "A.14 Useful Shortcuts\n\nBash Keyboard Shortcuts\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+C\nCancel current command\n\n\nCtrl+D\nEnd of input / logout\n\n\nCtrl+L\nClear screen\n\n\nCtrl+A\nMove to start of line\n\n\nCtrl+E\nMove to end of line\n\n\nCtrl+R\nSearch command history\n\n\nTab\nAutocomplete\n\n\n↑ / ↓\nPrevious/next command\n\n\n\n\n\nRStudio Keyboard Shortcuts\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+Enter\nRun current line\n\n\nCtrl+Shift+Enter\nRun current chunk\n\n\nCtrl+Shift+S\nRun entire script\n\n\nCtrl+Shift+C\nComment/uncomment\n\n\nCtrl+Shift+M\nInsert pipe %&gt;%\n\n\nAlt+-\nInsert assignment &lt;-\n\n\nCtrl+Shift+K\nKnit document",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  }
]