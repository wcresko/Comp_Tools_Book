[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "",
    "text": "Preface\nWelcome to Foundational Computational Tools for Bioengineers, a comprehensive guide designed to equip graduate students with essential computational skills for modern research in biology, bioengineering, and related life sciences.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Why This Book?",
    "text": "Why This Book?\nModern scientific research is fundamentally computational. Whether you’re analyzing genomic sequences, processing microscopy images, modeling biological systems, or managing experimental data, computational literacy has become as essential as laboratory technique. Yet many graduate students enter their programs without formal training in these foundational skills.\nThis book bridges that gap by providing a practical, hands-on introduction to the core computational tools you’ll use throughout your scientific career.\n\n\n\n\n\n\nLearning Philosophy\n\n\n\nThis is a practical course, and we will learn by doing. Each chapter includes worked examples, exercises, and real-world applications drawn from bioengineering and life sciences research.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nBy the end of this book, you will be able to:\n\nUnderstand computer architecture and choose appropriate computing resources for your analyses\nNavigate file systems and automate tasks using Unix/Linux commands\nWrite shell scripts to create reproducible analysis pipelines\nProcess and analyze large biological datasets using command-line tools\nProgram in R for statistical analysis and data visualization\nApply tidy data principles for effective data management\nUse Git and GitHub for version control and collaboration\nSubmit and manage jobs on high-performance computing clusters",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Prerequisites",
    "text": "Prerequisites\nNo prior programming experience is required! However, you should have:\n\nAccess to a computer running Windows, macOS, or Linux\nWillingness to learn through practice and experimentation\nPatience with yourself as you develop new skills",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nEach chapter builds on previous material, so I recommend reading them in order, at least initially. Key features include:\n\n\nCallout boxes highlighting important concepts, tips, and warnings\n\nCode examples that you can copy and modify for your own work\n\nPractice exercises to reinforce your learning\n\nCross-references connecting related concepts across chapters\n\n\n\n\n\n\n\nHands-On Practice\n\n\n\nThe best way to learn computational skills is to type the commands yourself. Avoid copying and pasting when possible—the muscle memory of typing commands is part of the learning process.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Course Materials",
    "text": "Course Materials\nThis book accompanies a graduate-level course taught at the University of Oregon’s Phil and Penny Knight Campus for Accelerating Scientific Impact. Additional materials, including lecture slides and homework assignments, are available on the course website.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book draws on many excellent resources, including Software Carpentry workshops (Software Carpentry, 2024a, 2024b), the tidyverse documentation (Wickham et al., 2023), and the experiences of countless students who have taken this course and provided valuable feedback.\nSpecial thanks to the Research Advanced Computing Services (RACS) team at the University of Oregon for their support with Talapas computing resources, and to the Knight Campus community for fostering an environment of interdisciplinary learning.\n\nLast updated: ?var:date",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Foundational Computational Tools for Bioengineers",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0). You are free to share and adapt this material for non-commercial purposes, provided you give appropriate credit and distribute your contributions under the same license.\n\n\n\n\nSoftware Carpentry (2024a). The unix shell.\n\n\nSoftware Carpentry (2024b). Version control with git.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction to Computational Tools",
    "section": "",
    "text": "1.1 The Computational Revolution in Biology\nModern biological research has undergone a profound transformation. The advent of high-throughput technologies—from next-generation sequencing to automated microscopy to mass spectrometry—has generated an explosion of data that far exceeds what can be analyzed manually. Consider these examples:\nTo make sense of this data deluge, researchers must be fluent in computational tools. This fluency goes beyond simply running pre-made software—it requires understanding how to manipulate data, automate analyses, and ensure reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#the-computational-revolution-in-biology",
    "href": "chapters/01-introduction.html#the-computational-revolution-in-biology",
    "title": "1  Introduction to Computational Tools",
    "section": "",
    "text": "A single human genome sequence contains approximately 6 billion base pairs\nA typical RNA-seq experiment generates tens of millions of short reads\nProteomics studies can identify and quantify thousands of proteins simultaneously\nHigh-content imaging screens produce terabytes of image data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-learn-computational-skills",
    "href": "chapters/01-introduction.html#why-learn-computational-skills",
    "title": "1  Introduction to Computational Tools",
    "section": "1.2 Why Learn Computational Skills?",
    "text": "1.2 Why Learn Computational Skills?\nThere are compelling reasons to invest time in developing computational proficiency:\n\nSpeed and Scale\n\nComputational tools allow you to perform thousands of operations with single commands. Tasks that would take days by hand can be completed in seconds.\n\nReproducibility\n\nScripts and code serve as a complete record of your analysis. Unlike clicking through graphical interfaces, command-line workflows can be exactly reproduced, shared, and verified (Sandve et al., 2013).\n\nFlexibility\n\nGeneral-purpose programming languages can handle virtually any data format or analysis type. You’re not limited to what software developers anticipated.\n\nAccess to Specialized Tools\n\nMany cutting-edge analysis tools in bioinformatics are only available through the command line. Learning to use the shell opens doors to thousands of free programs developed by scientists for scientists.\n\nHigh-Performance Computing\n\nLarge-scale analyses require computing clusters. These systems are accessed and controlled through command-line interfaces.\n\n\n\n\n\n\n\n\nCareer Impact\n\n\n\nComputational skills are increasingly valued in academic and industry positions. Investing in these skills now will pay dividends throughout your career.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#coding-vs.-scripting-whats-the-difference",
    "href": "chapters/01-introduction.html#coding-vs.-scripting-whats-the-difference",
    "title": "1  Introduction to Computational Tools",
    "section": "1.3 Coding vs. Scripting: What’s the Difference?",
    "text": "1.3 Coding vs. Scripting: What’s the Difference?\nYou’ll often hear the terms “coding” and “scripting” used interchangeably, but there are technical distinctions worth understanding.\n\n1.3.1 Compiled Languages (Coding)\nCompiled languages require a separate compilation step that translates human-readable source code into machine code before execution. Examples include:\n\nC/C++\nFortran\nRust\nGo\n\nThe compilation process produces optimized executable files that run very quickly. However, compiled languages typically require more setup and are less forgiving of errors during development.\n\n\n1.3.2 Interpreted Languages (Scripting)\nInterpreted languages (or scripting languages) execute code line-by-line at runtime, without a separate compilation step. Examples include:\n\nBash/Shell\nPython\nR\nJulia\nPerl\n\nInterpreted languages offer greater flexibility and faster development cycles at the cost of some execution speed. They’re ideal for data analysis, automation, and interactive exploration.\n\n\n\n\n\n\nNote\n\n\n\nThe distinction between compiled and interpreted languages has become increasingly blurred. Modern systems like Julia use just-in-time (JIT) compilation to achieve near-compiled performance with scripting-language convenience. Python and R can also interface with compiled C/C++ code for performance-critical operations.\n\n\n\n\n1.3.3 Which Should You Learn?\nFor most biological research, interpreted languages are the right choice. They offer:\n\nRapid prototyping and experimentation\nInteractive data exploration\nExtensive libraries for scientific computing\nLower barriers to entry for beginners\n\nIn this course, we focus primarily on Bash (the Unix shell) and R, with exposure to high-performance computing concepts that may eventually lead you to compiled languages if your research requires it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#tools-we-will-cover",
    "href": "chapters/01-introduction.html#tools-we-will-cover",
    "title": "1  Introduction to Computational Tools",
    "section": "1.4 Tools We Will Cover",
    "text": "1.4 Tools We Will Cover\nThis book and accompanying course introduce you to a carefully selected set of tools that form the foundation of computational research in the life sciences.\n\n1.4.1 Unix/Linux and the Shell\nThe Unix shell (specifically Bash) provides:\n\nA powerful interface for file manipulation\nTools for processing large text-based data files\nThe ability to combine simple programs into complex workflows\nAccess to remote computing systems\n\nUnderstanding Unix is essential because it underlies most scientific computing infrastructure, from laptops to supercomputers.\n\n\n1.4.2 R Statistical Programming\nR is a programming language and environment specifically designed for statistical computing and graphics (R Core Team, 2024). It offers:\n\nComprehensive statistical analysis capabilities\nPublication-quality graphics\nThousands of packages for specialized analyses\nIntegration with reproducible document systems (Quarto, R Markdown)\nAn excellent IDE (RStudio)\n\n\n\n1.4.3 Quarto and Reproducible Documents\nQuarto is an open-source scientific publishing system (Posit Software, PBC, 2024) that allows you to:\n\nCombine narrative text with executable code\nGenerate reports, presentations, websites, and books\nEnsure your analyses are fully reproducible\nShare your work in multiple formats\n\nThis book itself was written using Quarto!\n\n\n1.4.4 Git and GitHub\nGit is a version control system that tracks changes to your files over time (Chacon & Straub, 2014). GitHub is a web-based platform for hosting Git repositories. Together, they enable:\n\nComplete history of your project’s development\nSafe experimentation with new ideas\nCollaboration with colleagues worldwide\nSharing code and data publicly\n\n\n\n1.4.5 High-Performance Computing\nTalapas is the University of Oregon’s computing cluster. You’ll learn to:\n\nAccess and navigate the cluster\nSubmit and manage batch jobs\nUse software modules\nScale your analyses beyond your laptop’s capabilities",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#setting-up-your-computing-environment",
    "href": "chapters/01-introduction.html#setting-up-your-computing-environment",
    "title": "1  Introduction to Computational Tools",
    "section": "1.5 Setting Up Your Computing Environment",
    "text": "1.5 Setting Up Your Computing Environment\nBefore diving into the technical content, you’ll need to set up your computing environment. The required software depends on your operating system.\n\n1.5.1 All Operating Systems\nThese tools work across Windows, macOS, and Linux:\n\nR — Download from r-project.org\nRStudio — Download from posit.co/download/rstudio-desktop\nVisual Studio Code — Download from code.visualstudio.com\nGit — Download from git-scm.com\n\n\n\n1.5.2 macOS Users\nmacOS includes a Unix-based operating system. To access the shell:\n\nOpen Terminal (found in Applications → Utilities) or\nInstall iTerm2 for a more feature-rich terminal experience\n\n\n\n1.5.3 Linux Users\nLinux systems include multiple terminal applications. Open your distribution’s default terminal emulator, often called “Terminal” or “Console.”\n\n\n1.5.4 Windows Users\nWindows requires additional setup to access a Unix-like environment. The recommended approach uses Windows Subsystem for Linux (WSL):\n\n\n\n\n\n\nWindows Users: Additional Steps Required\n\n\n\nFollow these steps to install Ubuntu on Windows:\n\nRun Windows PowerShell as administrator\nType wsl --install and press Enter\nRestart your computer\nSearch for and install Ubuntu from the Microsoft Store, or type wsl --install -d ubuntu in PowerShell\nOpen Ubuntu and set up a username and password (doesn’t need to match your Windows login)\nRun sudo apt update followed by sudo apt upgrade to update packages\n\n\n\nA detailed guide is available at ubuntu.com/tutorials/install-ubuntu-on-wsl2-on-windows-10.\n\n\n1.5.5 Talapas Access\nTo use the university’s computing cluster:\n\nEnsure you’re a member of a PIRG (Principal Investigator Research Group)\nYour login credentials are your Duck ID and password\nOff-campus access requires the UO VPN\n\nWe’ll cover Talapas access in detail in Chapter 10.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#course-structure-and-expectations",
    "href": "chapters/01-introduction.html#course-structure-and-expectations",
    "title": "1  Introduction to Computational Tools",
    "section": "1.6 Course Structure and Expectations",
    "text": "1.6 Course Structure and Expectations\nThis book is designed to accompany a hands-on course where most of class time is devoted to coding practice rather than lectures. Expect to:\n\nFollow along with examples during class sessions\nComplete practice exercises to reinforce concepts\nSubmit homework assignments demonstrating your skills\nDevelop a final project applying what you’ve learned to your own research\n\n\n\n\n\n\n\nLearning Takes Practice\n\n\n\nComputational skills develop through repeated practice. Don’t be discouraged if concepts don’t click immediately—everyone struggles at first. The key is persistence and willingness to experiment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#getting-help",
    "href": "chapters/01-introduction.html#getting-help",
    "title": "1  Introduction to Computational Tools",
    "section": "1.7 Getting Help",
    "text": "1.7 Getting Help\nWhen you encounter problems (and you will!), several resources are available:\n\nManual Pages\n\nUnix commands have built-in documentation accessible via man command_name. Type q to exit.\n\nHelp Functions\n\nIn R, use ?function_name or help(function_name) to access documentation.\n\nThe Internet\n\nSites like Stack Overflow contain solutions to virtually every common error message. Learning to search effectively is a crucial skill.\n\nGenerative AI\n\nTools like ChatGPT and Claude can explain concepts, debug code, and suggest solutions. However, always verify AI-generated code and understand what it does before using it.\n\nEach Other\n\nYour classmates are experiencing the same learning curve. Collaboration and discussion accelerate everyone’s learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#summary",
    "href": "chapters/01-introduction.html#summary",
    "title": "1  Introduction to Computational Tools",
    "section": "1.8 Summary",
    "text": "1.8 Summary\nThis chapter introduced the rationale for learning computational tools and provided an overview of the skills you’ll develop. Key takeaways:\n\nComputational literacy is essential for modern biological research\nScripting languages (Bash, R, Python) are ideal for data analysis\nVersion control and reproducibility are professional best practices\nLearning requires patience, practice, and willingness to make mistakes\n\nIn the next chapter, we’ll explore computer systems architecture to build your mental model of how computers work—knowledge that will inform everything that follows.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#exercises",
    "href": "chapters/01-introduction.html#exercises",
    "title": "1  Introduction to Computational Tools",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\n\nEnvironment Check: Verify that you can open a terminal on your computer. What appears when you type echo \"Hello World\" and press Enter?\nSoftware Installation: Install R and RStudio if you haven’t already. Open RStudio and type 1 + 1 in the console. What happens?\nReflection: Write a paragraph describing one computational challenge in your current or planned research. What skills from this course might help address it?\n\n\n\n\n\n\n\nChacon, S., & Straub, B. (2014). Pro git.\n\n\nPosit Software, PBC (2024). Quarto: Open-source scientific and technical publishing system.\n\n\nR Core Team (2024). R: A language and environment for statistical computing.\n\n\nSandve, G. K. et al. (2013). Ten simple rules for reproducible computational research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Computational Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html",
    "href": "chapters/02-computer-systems.html",
    "title": "2  Computer Systems Architecture",
    "section": "",
    "text": "2.1 What Is a Computer System?\nUnderstanding how computers work at a fundamental level will help you make better decisions about computational resources, write more efficient code, and troubleshoot problems effectively. This chapter provides the conceptual foundation for everything that follows.\nA computer system consists of four major components working together:\ngraph TD\n    A[Input Devices] --&gt; B[CPU]\n    B --&gt; C[Memory]\n    C --&gt; B\n    B --&gt; D[Output Devices]\n    E[Storage] --&gt; C\n    C --&gt; E\n\n\n\n\n\ngraph TD\n    A[Input Devices] --&gt; B[CPU]\n    B --&gt; C[Memory]\n    C --&gt; B\n    B --&gt; D[Output Devices]\n    E[Storage] --&gt; C\n    C --&gt; E\n\n\n\n\nFigure 2.1: Basic computer system architecture showing the flow of data between components\nAt the heart of every computation is the fetch-decode-execute cycle, which the CPU performs billions of times per second.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#what-is-a-computer-system",
    "href": "chapters/02-computer-systems.html#what-is-a-computer-system",
    "title": "2  Computer Systems Architecture",
    "section": "",
    "text": "Hardware\n\nThe physical components you can touch—processors, memory chips, storage devices, and input/output peripherals.\n\nSoftware\n\nPrograms and instructions that tell the hardware what to do. This includes operating systems, applications, and your own scripts.\n\nFirmware\n\nLow-level software embedded directly in hardware components, providing basic control functions.\n\nData\n\nThe information being processed, stored, and transmitted by the system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#the-fetch-decode-execute-cycle",
    "href": "chapters/02-computer-systems.html#the-fetch-decode-execute-cycle",
    "title": "2  Computer Systems Architecture",
    "section": "2.2 The Fetch-Decode-Execute Cycle",
    "text": "2.2 The Fetch-Decode-Execute Cycle\nEvery instruction your programs execute follows the same fundamental pattern:\n\ngraph LR\n    A[Fetch Instruction] --&gt; B[Decode Instruction]\n    B --&gt; C[Execute Instruction]\n    C --&gt; D[Store Results]\n    D --&gt; A\n\n\n\n\n\ngraph LR\n    A[Fetch Instruction] --&gt; B[Decode Instruction]\n    B --&gt; C[Execute Instruction]\n    C --&gt; D[Store Results]\n    D --&gt; A\n\n\n\n\nFigure 2.2: The fundamental cycle underlying all computer operations\n\n\n\n\n\n\nFetch: The CPU retrieves the next instruction from memory\nDecode: The CPU interprets what the instruction means\nExecute: The CPU performs the specified operation\nStore: Results are saved to memory or a register\n\nThis cycle repeats continuously, with modern processors executing billions of cycles per second. Understanding this cycle helps explain why some operations are fast (data already in registers) while others are slow (waiting for data from disk).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#the-central-processing-unit-cpu",
    "href": "chapters/02-computer-systems.html#the-central-processing-unit-cpu",
    "title": "2  Computer Systems Architecture",
    "section": "2.3 The Central Processing Unit (CPU)",
    "text": "2.3 The Central Processing Unit (CPU)\nThe CPU is often called the “brain” of the computer, though this metaphor oversimplifies its function. The CPU is really a very fast calculator that follows instructions precisely.\n\n2.3.1 CPU Components\nThe CPU contains several critical components:\n\nControl Unit (CU)\n\nDirects the operation of the processor, managing the fetch-decode-execute cycle.\n\nArithmetic Logic Unit (ALU)\n\nPerforms mathematical calculations and logical comparisons.\n\nRegisters\n\nUltra-fast temporary storage locations (typically 64 bits each) for data being actively processed.\n\nCache\n\nHigh-speed memory buffer that stores frequently accessed data to reduce trips to main memory.\n\n\n\n\n2.3.2 Key CPU Concepts\nSeveral metrics describe CPU performance:\n\nClock Speed\n\nMeasured in gigahertz (GHz), this indicates how many cycles the CPU can perform per second. A 3.5 GHz processor performs 3.5 billion cycles per second.\n\nCores\n\nModern CPUs contain multiple independent processing units (cores). An 8-core processor can theoretically perform 8 operations simultaneously.\n\nThreads\n\nVirtual cores that allow better utilization of physical cores through simultaneous multithreading (SMT) or hyper-threading.\n\nInstruction Set\n\nThe low-level language the CPU understands. Common architectures include x86-64 (Intel/AMD) and ARM (Apple Silicon, most phones).\n\n\n\n\n\n\n\n\nNote\n\n\n\nMore cores and higher clock speeds don’t always mean faster performance for your specific task. Many analyses are limited by memory access speed or storage I/O rather than raw CPU power.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#graphics-processing-units-gpus",
    "href": "chapters/02-computer-systems.html#graphics-processing-units-gpus",
    "title": "2  Computer Systems Architecture",
    "section": "2.4 Graphics Processing Units (GPUs)",
    "text": "2.4 Graphics Processing Units (GPUs)\nWhile CPUs are designed for complex sequential tasks, GPUs excel at simple parallel operations. Originally designed for rendering graphics (where millions of pixels need similar calculations), GPUs have become essential for:\n\nMachine learning and deep neural networks\nScientific simulations\nCryptocurrency mining\nImage and video processing\n\n\n2.4.1 GPU Architecture\nGPUs differ fundamentally from CPUs:\n\nStreaming Multiprocessors (SMs)\n\nProcessing clusters containing many smaller cores.\n\nCUDA Cores / Stream Processors\n\nThousands of simple processing units optimized for parallel computation.\n\nVideo Memory (VRAM)\n\nDedicated high-bandwidth memory separate from system RAM.\n\n\n\n\n2.4.2 CPU vs. GPU: When to Use Each\n\n\n\nTable 2.1: Comparison of CPU and GPU characteristics\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nCPU\nGPU\n\n\n\n\nDesign Focus\nSequential, complex tasks\nParallel, simple tasks\n\n\nCore Count\n4-64 powerful cores\nThousands of smaller cores\n\n\nMemory\nLarge cache, lower bandwidth\nSmaller cache, higher bandwidth\n\n\nBest For\nOperating systems, complex logic, serial tasks\nGraphics, ML/AI, simulations\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf your computation involves applying the same operation to millions of data points (like image processing or matrix multiplication), GPUs can provide dramatic speedups. If your computation requires complex branching logic and sequential dependencies, CPUs are more appropriate.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#types-of-computer-memory",
    "href": "chapters/02-computer-systems.html#types-of-computer-memory",
    "title": "2  Computer Systems Architecture",
    "section": "2.5 Types of Computer Memory",
    "text": "2.5 Types of Computer Memory\nUnderstanding the memory hierarchy helps explain performance characteristics of different operations.\n\n2.5.1 Random Access Memory (RAM)\nRAM provides temporary working space for programs and data currently in use. Key characteristics:\n\nVolatile\n\nContents are lost when power is removed.\n\nRandom Access\n\nAny memory location can be accessed with equal speed (unlike sequential access).\n\nTypes\n\n\nDRAM (Dynamic RAM): Main system memory, requires constant refresh\nSRAM (Static RAM): Used in CPU caches, faster but more expensive\n\n\n\nModern systems typically have 8-64 GB of RAM, though scientific workstations and servers may have much more.\n\n\n2.5.2 Persistent Storage\nUnlike RAM, storage devices retain data without power:\nHard Disk Drives (HDDs)\n\nMechanical spinning platters with magnetic storage\nLarge capacities at low cost (many terabytes)\nSlower access (~10ms latency, 100-200 MB/s throughput)\nSusceptible to physical damage\n\nSolid State Drives (SSDs)\n\nNo moving parts (NAND flash memory)\nMuch faster access (~0.1ms latency, 500-7000 MB/s throughput)\nMore expensive per gigabyte than HDDs\nMore durable and energy-efficient\n\n\n\n\n\n\n\nImportant\n\n\n\nThe memory hierarchy creates significant performance differences. Data in CPU registers can be accessed in nanoseconds, data in RAM in tens of nanoseconds, but data on disk takes milliseconds—a difference of six orders of magnitude!\n\n\n\n\n2.5.3 The Memory Hierarchy\nFigure 2.3 illustrates the tradeoffs between speed, capacity, and cost:\n\n\n\n\n\n\n\n\nFigure 2.3: The memory hierarchy: faster storage is smaller and more expensive",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#operating-systems",
    "href": "chapters/02-computer-systems.html#operating-systems",
    "title": "2  Computer Systems Architecture",
    "section": "2.6 Operating Systems",
    "text": "2.6 Operating Systems\nAn operating system (OS) is the software layer that manages hardware resources and provides services to applications. Think of it as a translator between your programs and the physical computer.\n\n2.6.1 Core Functions\nOperating systems handle:\n\nProcess Management\n\nCreating, scheduling, and terminating programs. Managing how CPU time is shared among competing processes.\n\nMemory Management\n\nAllocating RAM to programs, implementing virtual memory, protecting programs from interfering with each other’s memory.\n\nFile System Management\n\nOrganizing data on storage devices, managing directories and permissions, handling read/write operations.\n\nDevice Management\n\nProviding standardized interfaces to diverse hardware through device drivers.\n\nSecurity\n\nUser authentication, access control, protection against malicious software.\n\n\n\n\n2.6.2 Common Operating Systems\nFor scientific computing, you’ll encounter:\n\nLinux\n\nOpen-source Unix-like OS dominant in servers and clusters. Most bioinformatics tools assume Linux.\n\nmacOS\n\nApple’s Unix-based desktop OS. Provides native access to Unix tools.\n\nWindows\n\nMicrosoft’s desktop OS. Now supports Linux tools through WSL (see Section 1.5.4).\n\n\n\n\n\n\n\n\nThe Principle of Least Privilege\n\n\n\nA fundamental security concept: every program and user should operate using the minimum permissions necessary to accomplish their task. This limits the damage that can occur from mistakes or malicious actions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#computing-environments",
    "href": "chapters/02-computer-systems.html#computing-environments",
    "title": "2  Computer Systems Architecture",
    "section": "2.7 Computing Environments",
    "text": "2.7 Computing Environments\nResearch computing happens across a spectrum of environments, from your laptop to massive cloud data centers.\n\n2.7.1 Local Computing\nLocal computing refers to resources physically present and directly controlled by you—your laptop, desktop, or lab workstation.\nAdvantages:\n\nComplete control over hardware and software\nData stays on your premises\nNo network dependency for computation\nPredictable performance\nOne-time purchase cost\n\nDisadvantages:\n\nLimited scale (constrained by hardware)\nYou’re responsible for maintenance and backups\nHigh upfront cost for powerful machines\nResources sit idle during off-hours\n\n\n\n2.7.2 Cluster Computing\nA computing cluster is a collection of interconnected computers (nodes) working together as a single system. The University of Oregon’s Talapas is an example.\n\ngraph TB\n    M[Head/Login Node] --&gt; N1[Compute Node 1]\n    M --&gt; N2[Compute Node 2]\n    M --&gt; N3[Compute Node 3]\n    M --&gt; N4[Compute Node N]\n    N1 -.-&gt; S[(Shared Storage)]\n    N2 -.-&gt; S\n    N3 -.-&gt; S\n    N4 -.-&gt; S\n    U[Users] --&gt; M\n\n\n\n\n\ngraph TB\n    M[Head/Login Node] --&gt; N1[Compute Node 1]\n    M --&gt; N2[Compute Node 2]\n    M --&gt; N3[Compute Node 3]\n    M --&gt; N4[Compute Node N]\n    N1 -.-&gt; S[(Shared Storage)]\n    N2 -.-&gt; S\n    N3 -.-&gt; S\n    N4 -.-&gt; S\n    U[Users] --&gt; M\n\n\n\n\nFigure 2.4: Simplified cluster architecture with head node, compute nodes, and shared storage\n\n\n\n\n\nKey components include:\n\nHead/Login Node\n\nWhere users log in and submit jobs. Not for running computations!\n\nCompute Nodes\n\nWorker machines that actually run your analyses.\n\nResource Manager\n\nSoftware (like SLURM) that schedules jobs and allocates resources fairly.\n\nShared Storage\n\nParallel file systems accessible from all nodes.\n\n\nAdvantages:\n\nMassive scalability (add nodes as needed)\nSpecialized hardware (GPUs, high-memory nodes)\nShared among many users and projects\nProfessional management and maintenance\n\nDisadvantages:\n\nRequires learning job scheduling systems\nJobs may wait in queue\nShared resources mean competition for access\nLess control over software environment\n\n\n\n2.7.3 Cloud Computing\nCloud computing provides on-demand computing resources over the internet, with pay-as-you-go pricing. Major providers include Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure.\n\ngraph TB\n    R[Region] --&gt; AZ1[Availability Zone 1]\n    R --&gt; AZ2[Availability Zone 2]\n    AZ1 --&gt; C[Compute]\n    AZ1 --&gt; S[Storage]\n    AZ2 --&gt; C2[Compute]\n    AZ2 --&gt; S2[Storage]\n    U[Users] -.Internet.-&gt; R\n\n\n\n\n\ngraph TB\n    R[Region] --&gt; AZ1[Availability Zone 1]\n    R --&gt; AZ2[Availability Zone 2]\n    AZ1 --&gt; C[Compute]\n    AZ1 --&gt; S[Storage]\n    AZ2 --&gt; C2[Compute]\n    AZ2 --&gt; S2[Storage]\n    U[Users] -.Internet.-&gt; R\n\n\n\n\nFigure 2.5: Cloud computing architecture with multiple availability zones\n\n\n\n\n\nAdvantages:\n\nElastic scaling (instantly add or remove resources)\nNo upfront hardware investment\nGlobal availability\nManaged services reduce operational burden\nBuilt-in redundancy and disaster recovery\n\nDisadvantages:\n\nOngoing costs can exceed on-premises solutions\nVendor lock-in with proprietary services\nRequires reliable internet connectivity\nData transfer costs (especially egress fees)\nLess control compared to local systems\n\n\n\n2.7.4 Choosing the Right Environment\n\n\n\nTable 2.2: Comparison of computing environments\n\n\n\n\n\n\n\n\n\n\n\nFactor\nLocal\nCluster\nCloud\n\n\n\n\nScale\nSmall\nLarge\nUnlimited\n\n\nControl\nComplete\nModerate\nLimited\n\n\nCost Model\nCapital expense\nShared allocation\nPay-per-use\n\n\nLearning Curve\nLow\nModerate\nModerate-High\n\n\nBest For\nDevelopment, small analyses\nLarge batch jobs\nBurst computing, web services\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA common workflow: develop and test locally, run production analyses on a cluster, use cloud resources for specialized needs or when cluster queues are full.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#the-evolution-of-scientific-computing",
    "href": "chapters/02-computer-systems.html#the-evolution-of-scientific-computing",
    "title": "2  Computer Systems Architecture",
    "section": "2.8 The Evolution of Scientific Computing",
    "text": "2.8 The Evolution of Scientific Computing\nComputing infrastructure has evolved dramatically over the past several decades:\n\ngraph LR\n    A[1960s-70s&lt;br/&gt;Mainframes] --&gt; B[1980s-90s&lt;br/&gt;Personal Computers]\n    B --&gt; C[1990s-2000s&lt;br/&gt;Client-Server]\n    C --&gt; D[2000s&lt;br/&gt;Clusters/Grids]\n    D --&gt; E[2006+&lt;br/&gt;Cloud Computing]\n    E --&gt; F[2020s&lt;br/&gt;Edge/Hybrid]\n\n\n\n\n\ngraph LR\n    A[1960s-70s&lt;br/&gt;Mainframes] --&gt; B[1980s-90s&lt;br/&gt;Personal Computers]\n    B --&gt; C[1990s-2000s&lt;br/&gt;Client-Server]\n    C --&gt; D[2000s&lt;br/&gt;Clusters/Grids]\n    D --&gt; E[2006+&lt;br/&gt;Cloud Computing]\n    E --&gt; F[2020s&lt;br/&gt;Edge/Hybrid]\n\n\n\n\nFigure 2.6: Evolution of computing paradigms from mainframes to modern hybrid approaches\n\n\n\n\n\nKey drivers of this evolution include:\n\nPerformance Needs: Ever-increasing computational demands from research\nEconomics: Pursuit of cost optimization and economies of scale\nConnectivity: Internet bandwidth improvements enabling remote computing\nVirtualization: Technologies that abstract hardware from software\n\nModern research computing increasingly uses hybrid approaches, combining local development with cluster and cloud resources based on the specific needs of each project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#summary",
    "href": "chapters/02-computer-systems.html#summary",
    "title": "2  Computer Systems Architecture",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nThis chapter provided foundational knowledge about computer systems:\n\nComputers consist of hardware, software, firmware, and data working together\nThe CPU executes instructions through the fetch-decode-execute cycle\nGPUs excel at parallel tasks; CPUs excel at sequential, complex operations\nMemory hierarchy creates significant performance differences\nOperating systems manage resources and provide services to applications\nComputing environments range from local to cluster to cloud, each with tradeoffs\n\nUnderstanding these concepts will help you choose appropriate resources for your analyses, interpret performance characteristics, and communicate effectively with system administrators.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/02-computer-systems.html#exercises",
    "href": "chapters/02-computer-systems.html#exercises",
    "title": "2  Computer Systems Architecture",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\n\nSystem Exploration: On your computer, find out:\n\nHow many CPU cores does your system have?\nHow much RAM is installed?\nWhat type of storage (SSD or HDD) do you have?\nWhat operating system and version are you running?\n\nMemory Hierarchy: Explain why reading data from disk is so much slower than reading from RAM. What implications does this have for analyzing large datasets?\nComputing Environment Selection: For each scenario, which computing environment (local, cluster, or cloud) would you recommend and why?\n\nTesting a new analysis script on a small dataset\nRunning a genome assembly requiring 256 GB of RAM\nHosting a web application for your lab’s research tools\nTraining a deep learning model requiring multiple GPUs for two weeks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Computer Systems Architecture</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html",
    "href": "chapters/03-unix-fundamentals.html",
    "title": "3  Unix Fundamentals",
    "section": "",
    "text": "3.1 What Is Unix?\nUnix is a family of operating systems originally developed at Bell Labs in 1969 and publicly released in 1973 (Kernighan & Pike, 1984). What began as a project for a single computer has become the foundation of modern computing infrastructure.\nToday, Unix and its derivatives power:\nLinux is an open-source implementation of Unix principles that has become the dominant operating system for scientific computing. When we refer to “Unix commands” in this book, the same commands work on Linux, macOS, and Windows Subsystem for Linux (WSL).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#what-is-unix",
    "href": "chapters/03-unix-fundamentals.html#what-is-unix",
    "title": "3  Unix Fundamentals",
    "section": "",
    "text": "Most web servers on the internet\nScientific computing clusters (including Talapas)\nmacOS (Apple’s desktop operating system)\nAndroid phones (via the Linux kernel)\nEmbedded systems in everything from routers to cars\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearning Unix is an investment that pays dividends throughout your career. The commands you learn today will work on systems you encounter decades from now—Unix’s longevity is remarkable in the fast-changing world of technology.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#the-shell-your-interface-to-unix",
    "href": "chapters/03-unix-fundamentals.html#the-shell-your-interface-to-unix",
    "title": "3  Unix Fundamentals",
    "section": "3.2 The Shell: Your Interface to Unix",
    "text": "3.2 The Shell: Your Interface to Unix\nThe shell is a program that interprets your commands and communicates with the operating system. Think of it as a translator between human-readable instructions and the computer’s internal operations.\nWhile graphical user interfaces (GUIs) like Finder on Mac or File Explorer on Windows are intuitive for casual use, the shell offers:\n\nSpeed: Type commands faster than clicking through menus\nPower: Access to thousands of specialized tools\nAutomation: Script repetitive tasks\nRemote Access: Work on distant computers identically to local ones\nReproducibility: Document exactly what you did\n\n\n3.2.1 Bash: The Default Shell\nBash (Bourne Again SHell) is the default shell on most Unix systems and is what you’ll use throughout this course. Other shells exist (zsh, fish, tcsh), but Bash is the most widely used and what you’ll encounter on computing clusters.\n\n\n3.2.2 Accessing the Shell\nHow you access the shell depends on your operating system:\n\nmacOS\n\nOpen the Terminal application (Applications → Utilities → Terminal) or use a third-party terminal like iTerm2.\n\nLinux\n\nOpen your distribution’s terminal application, often called “Terminal” or “Console.”\n\nWindows\n\nAfter installing WSL (see Section 1.5.4), search for “Ubuntu” or your installed Linux distribution in the Start menu.\n\n\nWhen you open the terminal, you’ll see a prompt indicating the system is ready for your command:\nusername@computer:~$\nThe prompt typically shows your username, the computer name, your current directory (~ means your home directory), and a $ symbol indicating you’re a regular user (a # would indicate administrator/root access).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#anatomy-of-a-shell-command",
    "href": "chapters/03-unix-fundamentals.html#anatomy-of-a-shell-command",
    "title": "3  Unix Fundamentals",
    "section": "3.3 Anatomy of a Shell Command",
    "text": "3.3 Anatomy of a Shell Command\nEvery shell command follows a consistent structure:\ncommand -options arguments\nLet’s break this down:\n\nCommand\n\nThe program or built-in function you want to run. This is required.\n\nOptions (also called flags)\n\nModify the command’s behavior. Usually preceded by - (single letter) or -- (full word). Optional.\n\nArguments\n\nWhat the command should operate on—typically file or directory names. Sometimes optional.\n\n\nFor example:\nls -l Documents\nHere, ls is the command (list directory contents), -l is an option (use long format with details), and Documents is the argument (which directory to list).\n\n\n\n\n\n\nNote\n\n\n\nOptions can often be combined. For example, ls -l -h -a can be written as ls -lha. The order of combined options usually doesn’t matter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#understanding-the-file-system",
    "href": "chapters/03-unix-fundamentals.html#understanding-the-file-system",
    "title": "3  Unix Fundamentals",
    "section": "3.4 Understanding the File System",
    "text": "3.4 Understanding the File System\nUnix organizes everything as files within a hierarchical directory structure. Understanding this structure is essential for navigation.\n\n3.4.1 The Directory Tree\nThe entire file system branches from a single root directory, represented by /:\n/\n├── Users/\n│   └── wcresko/\n│       ├── Documents/\n│       ├── Downloads/\n│       └── projects/\n├── Applications/\n├── Library/\n└── tmp/\nKey directories include:\n\n/\n\nThe root directory. Everything else is inside this.\n\n/home or /Users\n\nContains user home directories. Your files live here.\n\n~\n\nShorthand for your home directory (e.g., /home/wcresko).\n\n/tmp\n\nTemporary files that may be deleted on reboot.\n\n\n\n\n3.4.2 Absolute vs. Relative Paths\nA path specifies the location of a file or directory. Paths come in two forms:\n\nAbsolute Paths\n\nStart from the root directory (/). They work from anywhere because they specify the complete location.\n/Users/wcresko/Documents/analysis.R\n\nRelative Paths\n\nStart from your current directory. Shorter but depend on where you are.\nDocuments/analysis.R\n\n\nSpecial directory references:\n\n. (single dot) — current directory\n.. (double dot) — parent directory (one level up)\n~ — your home directory\n\n\n\n\n\n\n\nTip\n\n\n\nUse Tab completion! Start typing a file or directory name and press Tab to autocomplete. This saves typing and prevents errors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#essential-navigation-commands",
    "href": "chapters/03-unix-fundamentals.html#essential-navigation-commands",
    "title": "3  Unix Fundamentals",
    "section": "3.5 Essential Navigation Commands",
    "text": "3.5 Essential Navigation Commands\n\n3.5.1 Where Am I? pwd\nThe pwd command (print working directory) shows your current location:\n$ pwd\n/Users/wcresko/projects/analysis\nAlways know where you are before running commands—especially destructive ones!\n\n\n3.5.2 What’s Here? ls\nThe ls command (list) shows directory contents:\n$ ls\ndata/  results/  analysis.R  README.md\nCommon options make ls more informative:\n\n\n\nTable 3.1: Common options for the ls command\n\n\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n-l\nLong format (permissions, size, date)\n\n\n-a\nShow hidden files (those starting with .)\n\n\n-h\nHuman-readable file sizes (KB, MB, GB)\n\n\n-t\nSort by modification time (newest first)\n\n\n-S\nSort by file size (largest first)\n\n\n-r\nReverse sort order\n\n\n-F\nAdd symbols to show file types (/ for directories, * for executables)\n\n\n\n\n\n\nExample with multiple options:\n$ ls -lah\ntotal 32K\ndrwxr-xr-x  4 wcresko staff 128B Nov 15 14:30 .\ndrwxr-xr-x 12 wcresko staff 384B Nov 14 09:15 ..\n-rw-r--r--  1 wcresko staff 2.1K Nov 15 14:30 analysis.R\ndrwxr-xr-x  3 wcresko staff  96B Nov 15 10:00 data\n-rw-r--r--  1 wcresko staff  512 Nov 14 09:15 README.md\ndrwxr-xr-x  2 wcresko staff  64B Nov 15 14:00 results\nThe long format shows: permissions, links, owner, group, size, modification date, and name.\n\n\n3.5.3 Moving Around: cd\nThe cd command (change directory) moves you to a different location:\n# Go to a specific directory\n$ cd Documents\n\n# Go up one directory\n$ cd ..\n\n# Go to home directory (three equivalent ways)\n$ cd ~\n$ cd $HOME\n$ cd\n\n# Go to the previous directory (like \"back\" in a browser)\n$ cd -\n\n# Go to an absolute path\n$ cd /Users/wcresko/projects\n\n\n\n\n\n\nCaution\n\n\n\nUnlike GUIs, cd gives no feedback when successful. If you don’t see an error message, the command worked! Use pwd to verify where you ended up.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#working-with-files-and-directories",
    "href": "chapters/03-unix-fundamentals.html#working-with-files-and-directories",
    "title": "3  Unix Fundamentals",
    "section": "3.6 Working with Files and Directories",
    "text": "3.6 Working with Files and Directories\nNow that you can navigate, let’s learn to manipulate files and directories.\n\n3.6.1 Creating Directories: mkdir\n# Create a single directory\n$ mkdir analysis\n\n# Create nested directories (use -p for parents)\n$ mkdir -p project/data/raw\n\n# Create multiple directories at once\n$ mkdir results figures tables\n\n\n3.6.2 Creating Files: touch and nano\nThe touch command creates an empty file (or updates the timestamp of an existing file):\n$ touch notes.txt\nFor files with content, use a text editor. nano is a simple terminal-based editor:\n$ nano notes.txt\nIn nano: - Type your content normally - Ctrl+O saves the file (press Enter to confirm) - Ctrl+X exits nano\n\n\n3.6.3 Copying Files: cp\n# Copy a file\n$ cp original.txt backup.txt\n\n# Copy a file to a directory\n$ cp analysis.R scripts/\n\n# Copy a directory (requires -r for recursive)\n$ cp -r data/ data_backup/\n\n# Copy multiple files to a directory\n$ cp *.txt documents/\n\n\n3.6.4 Moving and Renaming: mv\nThe mv command both moves and renames files:\n# Rename a file\n$ mv oldname.txt newname.txt\n\n# Move a file to a directory\n$ mv analysis.R scripts/\n\n# Move and rename simultaneously\n$ mv data/raw.csv backup/raw_2024.csv\n\n# Move multiple files\n$ mv *.R scripts/\n\n\n3.6.5 Deleting Files: rm and rmdir\n\n\n\n\n\n\nDanger Zone\n\n\n\nThe shell has no trash can! Deleted files are gone forever. Always double-check before using rm, especially with wildcards.\n\n\n# Remove a file\n$ rm unnecessary.txt\n\n# Remove multiple files\n$ rm *.tmp\n\n# Remove a directory and all contents (DANGEROUS!)\n$ rm -r old_project/\n\n# Interactive mode (asks for confirmation)\n$ rm -i important.txt\n\n# Remove empty directories only\n$ rmdir empty_folder/\nThe -r (recursive) option with rm is powerful but dangerous. Consider using -i (interactive) to confirm each deletion when learning.\n\n\n3.6.6 Viewing Files: cat, head, tail, less\n# Display entire file\n$ cat README.md\n\n# First 10 lines (or specify -n NUMBER)\n$ head data.csv\n$ head -n 20 data.csv\n\n# Last 10 lines\n$ tail data.csv\n$ tail -n 5 data.csv\n\n# Follow a file as it grows (great for logs)\n$ tail -f analysis.log\n\n# Page through a large file\n$ less huge_data.txt\nIn less: - Space or f — next page - b — previous page - /pattern — search forward - n — next search result - q — quit\n\n\n3.6.7 File Information: wc\nThe wc (word count) command provides file statistics:\n$ wc document.txt\n  100   850  5420 document.txt\nThis shows: lines, words, and bytes. Useful options:\n$ wc -l data.csv   # Lines only\n$ wc -w essay.txt  # Words only\n$ wc -c file.bin   # Bytes only",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#naming-conventions",
    "href": "chapters/03-unix-fundamentals.html#naming-conventions",
    "title": "3  Unix Fundamentals",
    "section": "3.7 Naming Conventions",
    "text": "3.7 Naming Conventions\nGood naming practices prevent countless problems:\n\n\n\n\n\n\nFile Naming Rules\n\n\n\nDO: - Use lowercase letters, numbers, underscores (_), and hyphens (-) - Use meaningful, descriptive names - Include dates in ISO format if relevant: data_2024-01-15.csv - Use appropriate file extensions\nDON’T: - Use spaces (use _ or - instead) - Start names with - (confused with options) - Use special characters: * ? [ ] ' \" \\ / &lt; &gt; | - Use extremely long names\n\n\nExamples:\n\n\n\nBad\nGood\n\n\n\n\nmy data.csv\nmy_data.csv\n\n\n-results.txt\nresults.txt\n\n\ndata (copy).csv\ndata_copy.csv\n\n\nJoe's file.txt\njoes_file.txt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#getting-help",
    "href": "chapters/03-unix-fundamentals.html#getting-help",
    "title": "3  Unix Fundamentals",
    "section": "3.8 Getting Help",
    "text": "3.8 Getting Help\n\n3.8.1 Manual Pages\nEvery Unix command has a manual page accessible via man:\n$ man ls\nManual pages can be dense. Look for: - NAME: What the command does - SYNOPSIS: How to use it - DESCRIPTION: Detailed explanation - OPTIONS: Available flags - EXAMPLES: If you’re lucky!\nPress q to exit the manual.\n\n\n3.8.2 Quick Help\nMany commands support a --help option:\n$ ls --help\n\n\n3.8.3 Searching for Commands\nIf you know what you want to do but not the command:\n# Search man page descriptions\n$ man -k \"search term\"\n$ apropos \"search term\"\n\n# Find where a command is located\n$ which python",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#summary",
    "href": "chapters/03-unix-fundamentals.html#summary",
    "title": "3  Unix Fundamentals",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nThis chapter covered the fundamentals of Unix navigation and file manipulation:\n\nThe shell interprets commands and interfaces with the operating system\nCommands follow the pattern: command -options arguments\nThe file system is a hierarchical tree starting at /\nNavigation uses pwd, ls, and cd\nFile operations include cp, mv, rm, mkdir, and touch\nGood naming conventions prevent problems\nManual pages (man) provide detailed help\n\nThese commands form the foundation for everything else in Unix. Practice them until they become automatic!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/03-unix-fundamentals.html#exercises",
    "href": "chapters/03-unix-fundamentals.html#exercises",
    "title": "3  Unix Fundamentals",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Navigation Practice\nWithout using a GUI file browser:\n\nOpen your terminal\nPrint your current directory\nNavigate to your home directory\nList all files including hidden ones\nNavigate to your Documents folder (or create it if needed)\nReturn to your home directory using the shortcut\n\nExercise 2: File Operations\n\nCreate a new directory called practice\nNavigate into it\nCreate an empty file called notes.txt\nCreate a subdirectory called backup\nCopy notes.txt to the backup directory\nRename the original notes.txt to my_notes.txt\nList the contents of both directories to verify your work\nClean up by removing the entire practice directory\n\nExercise 3: Exploration\nUse manual pages to answer:\n\nWhat does the ls -R option do?\nHow can you create a directory and all necessary parent directories in one command?\nWhat command shows disk usage, and what option makes the output human-readable?\n\n\n\n\n\n\n\nKernighan, B. W., & Pike, R. (1984). The UNIX programming environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Unix Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html",
    "href": "chapters/04-files-pipes.html",
    "title": "4  Files, Pipes, and Redirection",
    "section": "",
    "text": "4.1 The Unix Philosophy\nUnix tools are designed around a simple but powerful philosophy (Kernighan & Pike, 1984):\nThis philosophy has profound implications for how you work:\nUnderstanding this philosophy transforms how you approach computational problems. Instead of seeking one program that does everything, you learn to construct pipelines from simple components.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#the-unix-philosophy",
    "href": "chapters/04-files-pipes.html#the-unix-philosophy",
    "title": "4  Files, Pipes, and Redirection",
    "section": "",
    "text": "Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.\n\n\n\nSmall, focused tools: Each command does one specific task excellently\nComposition: Complex operations are built by combining simple tools\nText as interface: Programs communicate through text, making them compatible",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#standard-streams-the-foundation",
    "href": "chapters/04-files-pipes.html#standard-streams-the-foundation",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.2 Standard Streams: The Foundation",
    "text": "4.2 Standard Streams: The Foundation\nEvery Unix program has three standard communication channels, called streams:\n         ┌──────────────┐\nstdin ───&gt;│              │───&gt; stdout\n    (0)   │   Program    │      (1)\n          │              │───&gt; stderr\n          └──────────────┘      (2)\n\nStandard Input (stdin)\n\nWhere programs read input. File descriptor 0. Default: keyboard.\n\nStandard Output (stdout)\n\nWhere programs write normal output. File descriptor 1. Default: terminal screen.\n\nStandard Error (stderr)\n\nWhere programs write error messages. File descriptor 2. Default: terminal screen.\n\n\nThese streams can be redirected—connected to files or other programs—which is the key to building powerful pipelines.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#reading-files",
    "href": "chapters/04-files-pipes.html#reading-files",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.3 Reading Files",
    "text": "4.3 Reading Files\n\n4.3.1 Choosing the Right Tool\nDifferent situations call for different tools:\n\n\n\nTable 4.1: File reading commands and their uses\n\n\n\n\n\nCommand\nBest For\n\n\n\n\ncat\nSmall files, concatenating files\n\n\nless\nLarge files, browsing, searching\n\n\nhead\nFirst few lines of a file\n\n\ntail\nLast few lines, monitoring logs\n\n\nwc\nCounting lines, words, characters\n\n\n\n\n\n\n\n\n4.3.2 cat: Concatenate and Display\n# Display a file\n$ cat data.txt\n\n# Display multiple files (concatenated)\n$ cat header.txt data.txt footer.txt\n\n# Number the output lines\n$ cat -n data.txt\n\n# Show non-printing characters (debugging whitespace)\n$ cat -A data.txt\n\n\n\n\n\n\nWarning\n\n\n\nNever use cat on large files! It will flood your terminal with text. For genomic data files, always use head, less, or process through pipes.\n\n\n\n\n4.3.3 less: The Pager\nFor large files, less lets you navigate page by page:\n$ less huge_file.txt\nKey navigation commands in less:\n\n\n\nTable 4.2: Navigation keys in less\n\n\n\n\n\nKey\nAction\n\n\n\n\nSpace, f\nForward one page\n\n\nb\nBack one page\n\n\ng\nGo to beginning\n\n\nG\nGo to end\n\n\n/pattern\nSearch forward\n\n\n?pattern\nSearch backward\n\n\nn\nNext search match\n\n\nN\nPrevious search match\n\n\nq\nQuit\n\n\n\n\n\n\n\n\n4.3.4 head and tail: Peek at Extremes\n# First 10 lines (default)\n$ head data.csv\n\n# First 20 lines\n$ head -n 20 data.csv\n\n# All but the last 5 lines\n$ head -n -5 data.csv\n\n# Last 10 lines (default)\n$ tail data.csv\n\n# Last 50 lines\n$ tail -n 50 data.csv\n\n# Everything after line 100\n$ tail -n +100 data.csv\nA special use of tail is monitoring files that are being written:\n# Follow a file as it grows (Ctrl+C to stop)\n$ tail -f analysis.log\nThis is invaluable for watching job progress on computing clusters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#output-redirection",
    "href": "chapters/04-files-pipes.html#output-redirection",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.4 Output Redirection",
    "text": "4.4 Output Redirection\nRedirection connects a program’s output to a file instead of the terminal.\n\n4.4.1 Overwrite: &gt;\n# Save command output to a file\n$ ls -l &gt; file_list.txt\n\n# Save date to a file\n$ date &gt; timestamp.txt\n\n# Redirect output from any command\n$ echo \"Hello, World!\" &gt; greeting.txt\n\n\n\n\n\n\nCaution\n\n\n\nThe &gt; operator overwrites existing files without warning! Double-check your filenames before running commands with &gt;.\n\n\n\n\n4.4.2 Append: &gt;&gt;\n# Add to an existing file\n$ echo \"New line\" &gt;&gt; notes.txt\n\n# Append today's entries to a log\n$ date &gt;&gt; daily_log.txt\n$ echo \"Analysis completed\" &gt;&gt; daily_log.txt\n\n\n4.4.3 Redirecting Errors: 2&gt;\nSometimes you want to separate normal output from error messages:\n# Send errors to a file, output to screen\n$ ls /nonexistent 2&gt; errors.txt\n\n# Save output and errors to different files\n$ command &gt; output.txt 2&gt; errors.txt\n\n# Discard errors (send to /dev/null)\n$ command 2&gt; /dev/null\n\n# Combine stdout and stderr to one file\n$ command &&gt; all_output.txt",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#input-redirection",
    "href": "chapters/04-files-pipes.html#input-redirection",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.5 Input Redirection",
    "text": "4.5 Input Redirection\nInput redirection feeds file contents to a program’s stdin:\n# Count lines in a file (two equivalent ways)\n$ wc -l &lt; data.txt\n$ wc -l data.txt\n\n# Sort input from a file\n$ sort &lt; names.txt\nWhile many commands can read files directly as arguments, input redirection is essential for programs that only read from stdin.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#pipes-connecting-commands",
    "href": "chapters/04-files-pipes.html#pipes-connecting-commands",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.6 Pipes: Connecting Commands",
    "text": "4.6 Pipes: Connecting Commands\nPipes (|) are the secret weapon of Unix. They connect the stdout of one command to the stdin of another, creating a processing pipeline:\ncommand1 | command2 | command3\nData flows through the pipeline, transformed at each step, with no intermediate files created.\n\n4.6.1 Simple Pipe Examples\n# Count files in a directory\n$ ls | wc -l\n\n# Sort and remove duplicates\n$ cat names.txt | sort | uniq\n\n# Find large files\n$ ls -l | grep \"\\.csv\"\n\n\n4.6.2 Building Complex Pipelines\nStart simple and build up:\n# Step 1: Look at raw data\n$ cat data.csv | head\n\n# Step 2: Extract a column (field 2, comma-delimited)\n$ cat data.csv | cut -d',' -f2 | head\n\n# Step 3: Remove header, sort\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort\n\n# Step 4: Count unique values\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c\n\n# Step 5: Sort by frequency\n$ cat data.csv | cut -d',' -f2 | tail -n +2 | sort | uniq -c | sort -rn\n\n\n\n\n\n\nPipeline Development Strategy\n\n\n\n\nStart with head to see sample data\nAdd one command at a time\nVerify output at each step\nBuild up to the complete pipeline\nSave complex pipelines in scripts\n\n\n\n\n\n4.6.3 The tee Command\nSometimes you want to save intermediate results while continuing the pipeline. The tee command splits output:\n# Save to file AND continue pipeline\n$ cat data.txt | sort | tee sorted.txt | uniq\nThis saves the sorted data to sorted.txt while simultaneously passing it to uniq.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#essential-pipeline-tools",
    "href": "chapters/04-files-pipes.html#essential-pipeline-tools",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.7 Essential Pipeline Tools",
    "text": "4.7 Essential Pipeline Tools\nSeveral commands are designed to work in pipelines:\n\n4.7.1 sort: Order Lines\n# Alphabetical sort\n$ sort names.txt\n\n# Numeric sort\n$ sort -n numbers.txt\n\n# Reverse sort\n$ sort -r data.txt\n\n# Sort by specific column (field 3, tab-delimited)\n$ sort -t$'\\t' -k3 data.tsv\n\n# Sort numerically by field 2\n$ sort -t',' -k2 -n data.csv\n\n\n4.7.2 uniq: Remove Duplicates\n# Remove adjacent duplicates (requires sorted input!)\n$ sort names.txt | uniq\n\n# Count occurrences\n$ sort names.txt | uniq -c\n\n# Show only duplicated lines\n$ sort names.txt | uniq -d\n\n# Show only unique lines\n$ sort names.txt | uniq -u\n\n\n4.7.3 cut: Extract Columns\n# Extract column 2 (tab-delimited default)\n$ cut -f2 data.tsv\n\n# Extract columns 1 and 3\n$ cut -f1,3 data.tsv\n\n# Comma-delimited\n$ cut -d',' -f2 data.csv\n\n# Character positions 1-10\n$ cut -c1-10 data.txt\n\n\n4.7.4 tr: Translate Characters\n# Convert lowercase to uppercase\n$ echo \"hello\" | tr 'a-z' 'A-Z'\n\n# Delete characters\n$ echo \"hello123\" | tr -d '0-9'\n\n# Squeeze repeated characters\n$ echo \"hello    world\" | tr -s ' '\n\n# Replace characters\n$ cat data.csv | tr ',' '\\t'  # CSV to TSV\n\n\n4.7.5 grep: Search for Patterns\nWe’ll cover grep extensively in Chapter 5, but here are basic uses:\n# Find lines containing \"error\"\n$ grep \"error\" log.txt\n\n# Case-insensitive search\n$ grep -i \"warning\" log.txt\n\n# Invert match (lines NOT containing pattern)\n$ grep -v \"^#\" data.txt  # Remove comment lines\n\n# Count matches\n$ grep -c \"pattern\" file.txt",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#working-with-large-genomic-data",
    "href": "chapters/04-files-pipes.html#working-with-large-genomic-data",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.8 Working with Large Genomic Data",
    "text": "4.8 Working with Large Genomic Data\nLet’s apply these tools to real bioinformatics scenarios. Genomic data files are often massive—the human genome reference is over 3 GB. You must process them efficiently.\n\n4.8.1 FASTA Format Basics\nFASTA files store sequences with headers starting with &gt;:\n&gt;sequence1 description\nATGCGATCGATCGATCGATCG\nATCGATCGATCGATCGATCGA\n&gt;sequence2 another description\nGCTAGCTAGCTAGCTAGCTAG\n\n\n4.8.2 Downloading Genomic Data\n# Download human genome from NCBI using wget\n$ wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\\\nGCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n\n# Or using curl\n$ curl -O https://example.com/genome.fa.gz\n\n\n4.8.3 Working with Compressed Files\nMost genomic data is compressed. Unix tools handle this seamlessly:\n# View compressed file without extracting\n$ zcat genome.fa.gz | head\n\n# Search compressed file\n$ zgrep \"&gt;\" genome.fa.gz | wc -l\n\n# Decompress and recompress\n$ gunzip genome.fa.gz         # Creates genome.fa\n$ gzip genome.fa              # Creates genome.fa.gz\n$ gzip -k genome.fa           # Keep original file\n\n\n4.8.4 Analyzing Sequence Data\n# Count sequences (headers start with &gt;)\n$ grep -c \"^&gt;\" sequences.fa\n\n# List all sequence headers\n$ grep \"^&gt;\" sequences.fa\n\n# Calculate total sequence length (remove headers and newlines)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | wc -c\n\n# Count nucleotides\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | fold -w1 | sort | uniq -c\n\n\n4.8.5 Example Pipeline: GC Content\nLet’s calculate the GC content (percentage of G and C bases) of a genome:\n# Extract sequence, convert to single line, count G and C\n$ grep -v \"^&gt;\" genome.fa | \\\n  tr -d '\\n' | \\\n  tr -cd 'GCgc' | \\\n  wc -c\nThis pipeline:\n\ngrep -v \"^&gt;\" — removes header lines\ntr -d '\\n' — removes newlines to get one long sequence\ntr -cd 'GCgc' — deletes everything except G and C\nwc -c — counts remaining characters\n\n\n\n\n\n\n\nNote\n\n\n\nThe backslash \\ at the end of a line continues the command on the next line, making complex pipelines more readable.\n\n\n\n\n4.8.6 Finding Restriction Sites\n# Count EcoRI sites (GAATTC)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -o \"GAATTC\" | wc -l\n\n# Find all start codons (ATG) and their positions\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -b -o \"ATG\" | head -20\n\n\n4.8.7 Stream Processing: Download and Analyze\nYou can process data without saving intermediate files:\n# Download, decompress, and analyze in one pipeline\n$ curl -s https://url/to/genome.fa.gz | \\\n  gunzip -c | \\\n  grep -v \"^&gt;\" | \\\n  tr -d '\\n' | \\\n  wc -c\nThis streams data through memory, never writing to disk—essential when working with files larger than your available storage.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#best-practices",
    "href": "chapters/04-files-pipes.html#best-practices",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.9 Best Practices",
    "text": "4.9 Best Practices\n\n4.9.1 Pipeline Efficiency\n\nFilter early: Reduce data volume as early as possible in the pipeline\nTest incrementally: Build and verify step by step\nUse appropriate tools: less for viewing, head for sampling\nSave complex pipelines: Put them in shell scripts\n\n\n\n4.9.2 Defensive Practices\n# Check file exists before processing\n$ test -f data.txt && grep \"pattern\" data.txt\n\n# Preview destructive operations\n$ ls *.tmp                    # See what will be deleted\n$ rm *.tmp                    # Then delete\n\n# Use tee to save intermediate results\n$ complex_pipeline | tee checkpoint.txt | further_processing",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#summary",
    "href": "chapters/04-files-pipes.html#summary",
    "title": "4  Files, Pipes, and Redirection",
    "section": "4.10 Summary",
    "text": "4.10 Summary\nThis chapter covered the tools for reading, writing, and connecting Unix commands:\n\nStandard streams (stdin, stdout, stderr) enable program communication\nOutput redirection (&gt;, &gt;&gt;) saves results to files\nInput redirection (&lt;) feeds file contents to commands\nPipes (|) connect commands into powerful pipelines\nTools like sort, uniq, cut, tr, and grep transform data\nGenomic data analysis leverages these tools at scale\n\nThese concepts form the foundation of Unix data processing. Master them, and you can construct solutions to virtually any text-processing challenge.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/04-files-pipes.html#exercises",
    "href": "chapters/04-files-pipes.html#exercises",
    "title": "4  Files, Pipes, and Redirection",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Redirection Practice\n\nUse ls -la to list your home directory and save the output to home_contents.txt\nAppend the current date and time to the end of home_contents.txt\nCount how many lines are in the file\nDisplay only the last 5 lines\n\nExercise 2: Pipeline Construction\nCreate a file numbers.txt with one number per line (include some duplicates):\necho -e \"5\\n3\\n8\\n3\\n1\\n5\\n9\\n3\\n7\" &gt; numbers.txt\nUsing pipes: 1. Sort the numbers numerically 2. Remove duplicates 3. Show only the top 3 numbers 4. Save the result to top_three.txt\nExercise 3: Text Processing\nGiven a CSV file with format name,score,grade, write a pipeline to: 1. Extract just the scores (column 2) 2. Sort them numerically in descending order 3. Calculate how many scores there are 4. Find the top 5 scores\nExercise 4: Genomic Data\nDownload a small bacterial genome and: 1. Count the number of sequences 2. Find the total sequence length 3. Count each nucleotide (A, T, G, C) 4. Search for a specific restriction enzyme site\n\n\n\n\n\n\nKernighan, B. W., & Pike, R. (1984). The UNIX programming environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Files, Pipes, and Redirection</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html",
    "href": "chapters/05-grep-regex.html",
    "title": "5  GREP and Regular Expressions",
    "section": "",
    "text": "5.1 Introduction to Pattern Matching\nWhile the tools in Chapter 4 can filter and transform data, they work on whole lines or fixed positions. Pattern matching lets you search based on the content of text—finding lines that contain specific sequences of characters.\nGREP (Global Regular Expression Print) is the Unix standard for pattern matching. Originally developed for the ed text editor in the 1970s, grep remains essential for bioinformatics, log analysis, and text processing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#basic-grep-usage",
    "href": "chapters/05-grep-regex.html#basic-grep-usage",
    "title": "5  GREP and Regular Expressions",
    "section": "5.2 Basic GREP Usage",
    "text": "5.2 Basic GREP Usage\nAt its simplest, grep finds lines containing a specific string:\n# Find lines containing \"error\"\n$ grep \"error\" logfile.txt\n\n# Search multiple files\n$ grep \"mutation\" *.txt\n\n# Search recursively in directories\n$ grep -r \"BRCA1\" data/\n\n5.2.1 Essential GREP Options\n\n\n\nTable 5.1: Common grep options\n\n\n\n\n\nOption\nDescription\n\n\n\n\n-i\nCase-insensitive search\n\n\n-v\nInvert match (lines NOT containing pattern)\n\n\n-c\nCount matching lines\n\n\n-n\nShow line numbers\n\n\n-l\nList only filenames with matches\n\n\n-o\nPrint only the matched part\n\n\n-w\nMatch whole words only\n\n\n-A n\nShow n lines after match\n\n\n-B n\nShow n lines before match\n\n\n-C n\nShow n lines before and after match\n\n\n\n\n\n\n\n\n5.2.2 Examples\n# Case-insensitive search\n$ grep -i \"gene\" annotations.txt\n\n# Count matches (not lines, just the count)\n$ grep -c \"&gt;\" sequences.fasta\n\n# Show line numbers\n$ grep -n \"ERROR\" application.log\n\n# Find files containing pattern\n$ grep -l \"experimental\" *.md\n\n# Remove comment lines (inverted match)\n$ grep -v \"^#\" config.txt\n\n# Show context around matches\n$ grep -C 2 \"significant\" results.txt",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#introduction-to-regular-expressions",
    "href": "chapters/05-grep-regex.html#introduction-to-regular-expressions",
    "title": "5  GREP and Regular Expressions",
    "section": "5.3 Introduction to Regular Expressions",
    "text": "5.3 Introduction to Regular Expressions\nSo far we’ve searched for literal text. Regular expressions (regex) let you describe patterns that match multiple strings.\nFor example, the pattern [ACGT]+ matches any sequence of DNA bases, while colou?r matches both “color” and “colour”.\n\n5.3.1 Metacharacters: Characters with Special Meaning\nRegular expressions use certain characters as pattern-building blocks:\n\n\n\nTable 5.2: Basic regular expression metacharacters\n\n\n\n\n\nCharacter\nMeaning\nExample\n\n\n\n\n.\nAny single character\na.c matches “abc”, “a1c”, “a-c”\n\n\n*\nZero or more of preceding\nab*c matches “ac”, “abc”, “abbc”\n\n\n^\nStart of line\n^Hello matches lines starting with “Hello”\n\n\n$\nEnd of line\nend$ matches lines ending with “end”\n\n\n[]\nCharacter class\n[aeiou] matches any vowel\n\n\n[^]\nNegated class\n[^0-9] matches non-digits\n\n\n\\\nEscape special characters\n\\. matches literal period\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe * in regular expressions is different from the * wildcard in the shell. In regex, * means “zero or more of the preceding character.” In shell globbing, * means “any characters.”\n\n\n\n\n5.3.2 Anchors: Position Matching\nAnchors don’t match characters—they match positions:\n# Lines starting with \"Chr\"\n$ grep \"^Chr\" genome.gff\n\n# Lines ending with \"protein\"\n$ grep \"protein$\" annotations.txt\n\n# Empty lines\n$ grep \"^$\" file.txt\n\n# Lines with exactly \"gene\"\n$ grep \"^gene$\" list.txt\n\n\n5.3.3 Character Classes\nCharacter classes match any single character from a set:\n# Match any vowel\n$ grep \"[aeiou]\" words.txt\n\n# Match any digit\n$ grep \"[0-9]\" data.txt\n\n# Match any uppercase letter\n$ grep \"[A-Z]\" text.txt\n\n# Match DNA bases\n$ grep \"[ACGT]\" sequence.txt\n\n# Match non-DNA characters (errors in sequence)\n$ grep \"[^ACGTN]\" sequence.txt\nPredefined character classes:\n\n[[:alpha:]] — letters\n[[:digit:]] — digits (same as [0-9])\n[[:alnum:]] — letters and digits\n[[:space:]] — whitespace\n[[:upper:]] — uppercase letters\n[[:lower:]] — lowercase letters\n\n\n\n5.3.4 Quantifiers\nQuantifiers specify how many times a pattern should repeat:\n# Match \"color\" or \"colour\" (zero or one 'u')\n$ grep \"colou\\?r\" text.txt\n\n# Three consecutive A's\n$ grep \"A\\{3\\}\" sequence.txt\n\n# Two to four T's\n$ grep \"T\\{2,4\\}\" sequence.txt\n\n# At least three G's\n$ grep \"G\\{3,\\}\" sequence.txt\n\n\n\n\n\n\nWarning\n\n\n\nIn basic grep, quantifiers like ?, +, and {} must be escaped with \\. Use grep -E or egrep to avoid escaping.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#extended-regular-expressions",
    "href": "chapters/05-grep-regex.html#extended-regular-expressions",
    "title": "5  GREP and Regular Expressions",
    "section": "5.4 Extended Regular Expressions",
    "text": "5.4 Extended Regular Expressions\nExtended regular expressions (ERE) add more metacharacters and don’t require escaping common quantifiers. Use grep -E or egrep:\n\n\n\nTable 5.3: Extended regular expression metacharacters\n\n\n\n\n\nCharacter\nMeaning\n\n\n\n\n+\nOne or more of preceding\n\n\n?\nZero or one of preceding\n\n\n{n}\nExactly n of preceding\n\n\n{n,m}\nBetween n and m of preceding\n\n\n\\|\nAlternation (OR)\n\n\n()\nGrouping\n\n\n\n\n\n\n\n5.4.1 Alternation: Matching Multiple Patterns\n# Match \"start\" OR \"stop\" OR \"pause\"\n$ grep -E \"start|stop|pause\" commands.txt\n\n# Match any start codon\n$ grep -E \"ATG|GTG|TTG\" sequences.fa\n\n# Match file extensions\n$ grep -E \"\\.(txt|csv|tsv)$\" filelist.txt\n\n\n5.4.2 Grouping\nParentheses group patterns together:\n# Match \"gray\" or \"grey\"\n$ grep -E \"gr(a|e)y\" text.txt\n\n# Match \"ab\", \"abab\", \"ababab\", etc.\n$ grep -E \"(ab)+\" data.txt\n\n# Match repeated dinucleotide\n$ grep -E \"(AT){3,}\" sequence.txt\n\n\n5.4.3 The Plus Quantifier\nUnlike * (zero or more), + requires at least one match:\n# One or more A's followed by one or more T's\n$ grep -E \"A+T+\" sequence.txt\n\n# One or more digits\n$ grep -E \"[0-9]+\" data.txt",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#practical-bioinformatics-examples",
    "href": "chapters/05-grep-regex.html#practical-bioinformatics-examples",
    "title": "5  GREP and Regular Expressions",
    "section": "5.5 Practical Bioinformatics Examples",
    "text": "5.5 Practical Bioinformatics Examples\n\n5.5.1 Working with FASTA Files\n# Count sequences (lines starting with &gt;)\n$ grep -c \"^&gt;\" sequences.fa\n\n# Extract all headers\n$ grep \"^&gt;\" sequences.fa\n\n# Find specific gene headers\n$ grep -i \"kinase\" sequences.fa\n\n# Find sequences by organism\n$ grep -E \"^&gt;.*Homo sapiens\" proteins.fa\n\n\n5.5.2 Searching for Biological Motifs\n# Find restriction enzyme sites\n$ grep -E \"GAATTC\" genome.fa         # EcoRI\n$ grep -E \"GGATCC\" genome.fa         # BamHI\n$ grep -E \"G[ACGT]ANTC\" genome.fa    # Degenerate site\n\n# Find potential ORFs (start followed by bases, then stop)\n$ grep -oE \"ATG([ACGT]{3})*?(TAA|TAG|TGA)\" cds.fa\n\n# Find poly-A tails\n$ grep -E \"A{10,}\" transcripts.fa\n\n# Find microsatellites (dinucleotide repeats)\n$ grep -E \"(AT){5,}\" genome.fa\n$ grep -E \"(CA){5,}\" genome.fa\n\n\n5.5.3 Analyzing GFF/GTF Files\n# Find all genes\n$ grep -P \"\\tgene\\t\" annotations.gff\n\n# Find genes on chromosome 1\n$ grep \"^chr1\" genes.gff | grep -P \"\\tgene\\t\"\n\n# Extract gene IDs\n$ grep -oE \"gene_id \\\"[^\\\"]+\\\"\" annotations.gtf\n\n# Find protein-coding genes\n$ grep \"protein_coding\" annotations.gtf\n\n\n5.5.4 Log File Analysis\n# Find errors in logs\n$ grep -i \"error\" application.log\n\n# Find specific error codes\n$ grep -E \"error\\s*[0-9]{3}\" application.log\n\n# Extract timestamps of errors\n$ grep -E \"^[0-9]{4}-[0-9]{2}-[0-9]{2}.*error\" log.txt\n\n# Count errors per type\n$ grep -oE \"ERROR: [A-Za-z]+\" log.txt | sort | uniq -c",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#combining-grep-with-pipes",
    "href": "chapters/05-grep-regex.html#combining-grep-with-pipes",
    "title": "5  GREP and Regular Expressions",
    "section": "5.6 Combining GREP with Pipes",
    "text": "5.6 Combining GREP with Pipes\nThe real power of grep emerges when combined with other tools:\n# Count unique patterns\n$ grep \"^&gt;\" sequences.fa | sort | uniq | wc -l\n\n# Extract sequence names without &gt; symbol\n$ grep \"^&gt;\" sequences.fa | cut -c2-\n\n# Find headers, then get organism names\n$ grep \"^&gt;\" proteins.fa | grep -oE \"\\[.*\\]\" | sort | uniq -c | sort -rn\n\n# Process FASTA: count bases per sequence\n$ grep -v \"^&gt;\" sequence.fa | while read line; do\n    echo \"$line\" | wc -c\n  done\n\n5.6.1 Extracting Matching Regions\nThe -o flag prints only the matched portion:\n# Extract all email addresses\n$ grep -oE \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\" contacts.txt\n\n# Extract all URLs\n$ grep -oE \"https?://[a-zA-Z0-9./?=_-]+\" webpage.html\n\n# Extract gene names from headers\n$ grep \"^&gt;\" genes.fa | grep -oE \"gene=[^;]+\"\n\n\n5.6.2 Context Searches\nWhen you need to see what surrounds a match:\n# Show 3 lines after each match (including the sequence)\n$ grep -A 3 \"^&gt;interesting_gene\" sequences.fa\n\n# Show lines before and after\n$ grep -C 5 \"error\" logfile.txt\n\n# Extract sequence following a header\n$ grep -A 1 \"^&gt;target_gene\" sequences.fa | grep -v \"^&gt;\"",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#processing-genomic-data-a-complete-example",
    "href": "chapters/05-grep-regex.html#processing-genomic-data-a-complete-example",
    "title": "5  GREP and Regular Expressions",
    "section": "5.7 Processing Genomic Data: A Complete Example",
    "text": "5.7 Processing Genomic Data: A Complete Example\nLet’s work through a realistic analysis pipeline for human chromosome data.\n\n5.7.1 Downloading the Data\n# Download human chromosome 21 (smaller for practice)\n$ wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/\\\nGCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz\n\n# Or work with a subset\n$ zcat genome.fa.gz | head -1000 &gt; sample.fa\n\n\n5.7.2 Analyzing Sequence Content\n# Count sequences in the file\n$ grep -c \"^&gt;\" genome.fa\n\n# View headers\n$ grep \"^&gt;\" genome.fa | head\n\n# Total bases (excluding headers and newlines)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | wc -c\n\n# Nucleotide frequencies\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | fold -w1 | sort | uniq -c\n\n\n5.7.3 Finding Restriction Enzyme Sites\n# Count EcoRI sites (GAATTC)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -o \"GAATTC\" | wc -l\n\n# Count multiple restriction sites\n$ for site in GAATTC GGATCC AAGCTT CTCGAG; do\n    count=$(grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -o \"$site\" | wc -l)\n    echo \"$site: $count\"\n  done\n\n# Find context around restriction sites (50 bases each side)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \".{50}GAATTC.{50}\" | head\n\n\n5.7.4 Finding Simple Sequence Repeats\n# Dinucleotide repeats\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"(AT){5,}\" | wc -l\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"(CA){5,}\" | wc -l\n\n# Homopolymer runs (consecutive same base)\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"A{10,}\" | wc -l\n$ grep -v \"^&gt;\" genome.fa | tr -d '\\n' | grep -oE \"T{10,}\" | wc -l\n\n\n5.7.5 Stream Processing Large Files\nProcess data without loading it entirely into memory:\n# Download, decompress, and analyze in one pipeline\n$ curl -s ftp://example.com/genome.fa.gz | \\\n  gunzip -c | \\\n  grep -v \"^&gt;\" | \\\n  tr -d '\\n' | \\\n  grep -o \"GAATTC\" | \\\n  wc -l\n\n# Process compressed file directly\n$ zgrep -c \"^&gt;\" genome.fa.gz",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#common-patterns-reference",
    "href": "chapters/05-grep-regex.html#common-patterns-reference",
    "title": "5  GREP and Regular Expressions",
    "section": "5.8 Common Patterns Reference",
    "text": "5.8 Common Patterns Reference\nHere’s a quick reference for frequently needed patterns:\n\n\n\nTable 5.4: Common regular expression patterns for bioinformatics\n\n\n\n\n\nPattern\nWhat It Matches\n\n\n\n\n^&gt;\nFASTA headers\n\n\n^#\nComment lines\n\n\n^$\nEmpty lines\n\n\n[ACGT]+\nDNA sequence\n\n\n[ACGU]+\nRNA sequence\n\n\n[A-Z]{3}\nUppercase triplet (codon)\n\n\nATG([ACGT]{3})*\nPotential ORF\n\n\n(TAA\\|TAG\\|TGA)\nStop codons\n\n\nA{n,}\nPoly-A of at least n\n\n\n\\t\nTab character\n\n\n\\s+\nOne or more whitespace\n\n\n[0-9]+\\.[0-9]+\nDecimal number",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#summary",
    "href": "chapters/05-grep-regex.html#summary",
    "title": "5  GREP and Regular Expressions",
    "section": "5.9 Summary",
    "text": "5.9 Summary\nThis chapter covered pattern matching with grep and regular expressions:\n\ngrep finds lines matching patterns in files or streams\nBasic regex uses metacharacters: ., *, ^, $, []\nExtended regex (grep -E) adds: +, ?, |, {}\nCharacter classes match sets of characters\nAnchors match positions (start/end of line)\nCombining grep with pipes enables powerful analysis\nGenomic data analysis uses these tools for motif searching\n\nRegular expressions are a foundational skill. While the syntax can seem cryptic initially, investment in learning regex pays off enormously across all aspects of computational biology.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/05-grep-regex.html#exercises",
    "href": "chapters/05-grep-regex.html#exercises",
    "title": "5  GREP and Regular Expressions",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Pattern Matching\nCreate a file with various text and practice: 1. Find all lines containing “the” (case-insensitive) 2. Find lines starting with a capital letter 3. Find lines ending with a period 4. Count empty lines\nExercise 2: Biological Patterns\nUsing a FASTA file: 1. Count the number of sequences 2. Extract headers for sequences longer than 1000 bp (hint: you’ll need multiple commands) 3. Find all sequences containing the pattern “TATAAA” (TATA box) 4. Count how many sequences have “chromosome” in their header\nExercise 3: Regular Expression Practice\nWrite grep commands to match: 1. Phone numbers in format: (555) 555-5555 2. Email addresses 3. Dates in YYYY-MM-DD format 4. DNA sequences that are exactly 20 bases long\nExercise 4: Genomic Analysis Pipeline\nDownload a bacterial genome and: 1. Calculate the genome size 2. Count all restriction sites for at least 3 different enzymes 3. Find the longest homopolymer run 4. Calculate GC content using only Unix tools 5. Document your pipeline in a shell script",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>GREP and Regular Expressions</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html",
    "href": "chapters/06-shell-scripting.html",
    "title": "6  Shell Scripting",
    "section": "",
    "text": "6.1 Why Shell Scripting?\nThroughout the previous chapters, you’ve been typing commands interactively. While this is great for exploration, real-world analyses require something more:\nShell scripts transform a series of commands into a reusable, documented program.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#why-shell-scripting",
    "href": "chapters/06-shell-scripting.html#why-shell-scripting",
    "title": "6  Shell Scripting",
    "section": "",
    "text": "Reproducibility: Record exactly what you did\nAutomation: Run the same analysis on many files\nEfficiency: Avoid repetitive typing\nDocumentation: Explain why you did each step\nSharing: Let others reproduce your work",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#your-first-shell-script",
    "href": "chapters/06-shell-scripting.html#your-first-shell-script",
    "title": "6  Shell Scripting",
    "section": "6.2 Your First Shell Script",
    "text": "6.2 Your First Shell Script\nA shell script is simply a text file containing commands. Let’s create one:\n#!/bin/bash\n# first_script.sh - My first shell script\n# Author: Your Name\n# Date: 2025-01-15\n\n# Display a greeting\necho \"Hello, Bioengineering!\"\n\n# Show the current date\necho \"Today is: $(date +%Y-%m-%d)\"\n\n# List files in current directory\necho \"Files in this directory:\"\nls -la\n\n6.2.1 The Shebang Line\nThe first line #!/bin/bash is called a shebang (or hashbang). It tells the operating system which interpreter should execute this script. Always include it.\nOther common shebangs:\n\n#!/bin/bash — Bash shell (most common)\n#!/bin/sh — POSIX shell (more portable)\n#!/usr/bin/env python3 — Python 3\n#!/usr/bin/env Rscript — R\n\n\n\n6.2.2 Running Scripts\nThere are several ways to run a script:\n# Method 1: Invoke the interpreter explicitly\n$ bash first_script.sh\n\n# Method 2: Make executable and run directly\n$ chmod +x first_script.sh\n$ ./first_script.sh\nThe chmod +x command adds execute permission to the file. After that, you can run it directly with ./. The ./ is necessary because your current directory typically isn’t in your system’s PATH.\n\n\n\n\n\n\nTip\n\n\n\nAlways use chmod +x on your scripts. It signals that the file is meant to be executed and allows direct invocation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#variables",
    "href": "chapters/06-shell-scripting.html#variables",
    "title": "6  Shell Scripting",
    "section": "6.3 Variables",
    "text": "6.3 Variables\nVariables store values for later use. In Bash, variable assignment has strict syntax rules.\n\n6.3.1 Assigning Variables\n#!/bin/bash\n\n# Assign variables (NO spaces around the =)\nNAME=\"DNA Analysis\"\nCOUNT=100\nINPUT_FILE=\"sequences.fa\"\n\n# Use variables (with $ prefix)\necho \"Running $NAME\"\necho \"Processing $COUNT sequences from $INPUT_FILE\"\n\n# Curly braces for clarity\necho \"File: ${INPUT_FILE}\"\necho \"Creating ${INPUT_FILE}.backup\"\n\n\n\n\n\n\nCommon Mistake\n\n\n\nSpaces around = will cause errors!\n# WRONG - causes errors\nNAME = \"value\"\nCOUNT= 100\n\n# RIGHT - no spaces\nNAME=\"value\"\nCOUNT=100\n\n\n\n\n6.3.2 Command Substitution\nCapture command output in a variable:\n#!/bin/bash\n\n# Old syntax (backticks)\nTODAY=`date +%Y-%m-%d`\n\n# Modern syntax (preferred)\nTODAY=$(date +%Y-%m-%d)\nFILECOUNT=$(ls *.fa | wc -l)\nGENOME_SIZE=$(grep -v \"^&gt;\" genome.fa | tr -d '\\n' | wc -c)\n\necho \"Analysis date: $TODAY\"\necho \"Processing $FILECOUNT FASTA files\"\necho \"Genome size: $GENOME_SIZE bp\"\n\n\n6.3.3 Arithmetic Operations\nBash arithmetic uses $(( )):\n#!/bin/bash\n\nCOUNT=10\nDOUBLE=$((COUNT * 2))\nINCREMENT=$((COUNT + 1))\n\n# Calculate percentage\nTOTAL=1000\nMATCHED=350\nPERCENT=$((MATCHED * 100 / TOTAL))\n\necho \"Doubled: $DOUBLE\"\necho \"Match rate: $PERCENT%\"\nFor floating-point math, use external tools like bc:\nGC_CONTENT=$(echo \"scale=2; 450 / 1000 * 100\" | bc)\necho \"GC content: $GC_CONTENT%\"\n\n\n6.3.4 String Operations\n#!/bin/bash\n\nFILE=\"sample_001.fastq.gz\"\n\n# Remove suffix\nNAME=\"${FILE%.fastq.gz}\"    # sample_001\n\n# Remove prefix  \nNUMBER=\"${FILE#sample_}\"    # 001.fastq.gz\n\n# Remove longest match from end\nBASENAME=\"${FILE%%.*}\"      # sample_001\n\n# String length\nLENGTH=${#FILE}             # 19\n\necho \"Original: $FILE\"\necho \"Name without extension: $NAME\"\necho \"Length: $LENGTH characters\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#arrays",
    "href": "chapters/06-shell-scripting.html#arrays",
    "title": "6  Shell Scripting",
    "section": "6.4 Arrays",
    "text": "6.4 Arrays\nArrays store multiple values:\n#!/bin/bash\n\n# Declare an array\nBASES=(A C G T)\nSAMPLES=(\"control\" \"treatment_1\" \"treatment_2\")\n\n# Access elements (0-indexed)\necho \"First base: ${BASES[0]}\"        # A\necho \"Second sample: ${SAMPLES[1]}\"   # treatment_1\n\n# All elements\necho \"All bases: ${BASES[@]}\"\n\n# Array length\necho \"Number of samples: ${#SAMPLES[@]}\"\n\n# Add element\nBASES+=(N)\n\n# Loop through array\nfor sample in \"${SAMPLES[@]}\"; do\n    echo \"Processing: $sample\"\ndone",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#conditional-statements",
    "href": "chapters/06-shell-scripting.html#conditional-statements",
    "title": "6  Shell Scripting",
    "section": "6.5 Conditional Statements",
    "text": "6.5 Conditional Statements\nConditionals control program flow based on tests.\n\n6.5.1 Basic if/then/else\n#!/bin/bash\n\nCOUNT=100\n\nif [ $COUNT -gt 50 ]; then\n    echo \"High count\"\nelif [ $COUNT -gt 10 ]; then\n    echo \"Medium count\"\nelse\n    echo \"Low count\"\nfi\n\n\n6.5.2 Test Operators\nNumeric comparisons:\n\n\n\nOperator\nMeaning\n\n\n\n\n-eq\nEqual\n\n\n-ne\nNot equal\n\n\n-gt\nGreater than\n\n\n-ge\nGreater than or equal\n\n\n-lt\nLess than\n\n\n-le\nLess than or equal\n\n\n\nString comparisons:\n\n\n\nOperator\nMeaning\n\n\n\n\n=\nEqual\n\n\n!=\nNot equal\n\n\n-z\nZero length (empty)\n\n\n-n\nNon-zero length\n\n\n\nFile tests:\n\n\n\nOperator\nMeaning\n\n\n\n\n-f\nFile exists and is regular file\n\n\n-d\nDirectory exists\n\n\n-e\nFile exists (any type)\n\n\n-r\nFile is readable\n\n\n-w\nFile is writable\n\n\n-x\nFile is executable\n\n\n-s\nFile exists and is not empty\n\n\n\n\n\n6.5.3 Examples\n#!/bin/bash\n\n# File existence check\nif [ -f \"data.txt\" ]; then\n    echo \"File exists\"\nelse\n    echo \"File not found\"\nfi\n\n# Directory check with creation\nif [ ! -d \"results\" ]; then\n    mkdir results\n    echo \"Created results directory\"\nfi\n\n# String comparison\nFILETYPE=\"fasta\"\nif [ \"$FILETYPE\" = \"fasta\" ]; then\n    echo \"Processing FASTA file\"\nfi\n\n# Combining conditions\nif [ -f \"$FILE\" ] && [ -r \"$FILE\" ]; then\n    echo \"File exists and is readable\"\nfi\n\n\n\n\n\n\nImportant\n\n\n\nAlways quote variables in tests: [ \"$VAR\" = \"value\" ]. Without quotes, empty variables cause syntax errors.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#loops",
    "href": "chapters/06-shell-scripting.html#loops",
    "title": "6  Shell Scripting",
    "section": "6.6 Loops",
    "text": "6.6 Loops\nLoops repeat actions multiple times.\n\n6.6.1 For Loops\n#!/bin/bash\n\n# Loop over a list\nfor base in A C G T; do\n    echo \"Base: $base\"\ndone\n\n# Loop over files\nfor file in *.fastq; do\n    echo \"Processing: $file\"\n    gzip \"$file\"\ndone\n\n# Loop with counter\nfor i in {1..10}; do\n    echo \"Iteration $i\"\ndone\n\n# C-style loop\nfor ((i=0; i&lt;10; i++)); do\n    echo \"Index: $i\"\ndone\n\n# Loop over array\nFILES=(\"sample1.fa\" \"sample2.fa\" \"sample3.fa\")\nfor file in \"${FILES[@]}\"; do\n    echo \"Analyzing $file\"\ndone\n\n\n6.6.2 While Loops\n#!/bin/bash\n\n# Count down\nCOUNT=5\nwhile [ $COUNT -gt 0 ]; do\n    echo \"Count: $COUNT\"\n    COUNT=$((COUNT - 1))\ndone\n\n# Read file line by line\nwhile read line; do\n    echo \"Processing: $line\"\ndone &lt; input.txt\n\n# Process command output\nls *.fa | while read file; do\n    echo \"Found: $file\"\ndone\n\n\n6.6.3 Loop Control\n#!/bin/bash\n\nfor file in *.fa; do\n    # Skip files starting with \"test\"\n    if [[ \"$file\" == test* ]]; then\n        continue\n    fi\n    \n    # Stop if we find \"final.fa\"\n    if [ \"$file\" = \"final.fa\" ]; then\n        echo \"Found final file, stopping\"\n        break\n    fi\n    \n    echo \"Processing: $file\"\ndone",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#command-line-arguments",
    "href": "chapters/06-shell-scripting.html#command-line-arguments",
    "title": "6  Shell Scripting",
    "section": "6.7 Command-Line Arguments",
    "text": "6.7 Command-Line Arguments\nScripts become more flexible when they accept arguments:\n#!/bin/bash\n# process_fasta.sh - Process a FASTA file\n\n# $0 is the script name\necho \"Script: $0\"\n\n# $# is the number of arguments\necho \"Arguments received: $#\"\n\n# $1, $2, etc. are positional arguments\nINPUT=$1\nOUTPUT=$2\n\n# $@ is all arguments\necho \"All arguments: $@\"\n\n# Check for required arguments\nif [ $# -lt 2 ]; then\n    echo \"Usage: $0 input.fasta output.txt\"\n    exit 1\nfi\n\n# Now process the files\necho \"Input: $INPUT\"\necho \"Output: $OUTPUT\"\nUsage:\n$ ./process_fasta.sh sequences.fa results.txt\n\n6.7.1 Argument Validation\n#!/bin/bash\n\n# Check argument count\nif [ $# -ne 2 ]; then\n    echo \"Error: Expected 2 arguments, got $#\" &gt;&2\n    echo \"Usage: $0 input.fa output.txt\" &gt;&2\n    exit 1\nfi\n\nINPUT=$1\nOUTPUT=$2\n\n# Validate input file\nif [ ! -f \"$INPUT\" ]; then\n    echo \"Error: Input file '$INPUT' not found\" &gt;&2\n    exit 1\nfi\n\n# Check if output would overwrite\nif [ -f \"$OUTPUT\" ]; then\n    echo \"Warning: Output file exists. Overwrite? (y/n)\"\n    read answer\n    if [ \"$answer\" != \"y\" ]; then\n        exit 0\n    fi\nfi\n\n# Continue with processing...",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#functions",
    "href": "chapters/06-shell-scripting.html#functions",
    "title": "6  Shell Scripting",
    "section": "6.8 Functions",
    "text": "6.8 Functions\nFunctions make scripts modular and reusable:\n#!/bin/bash\n\n# Define a function\ncount_sequences() {\n    local FILE=$1  # Local variable\n    local COUNT=$(grep -c \"^&gt;\" \"$FILE\")\n    echo $COUNT\n}\n\n# Function with multiple parameters\nanalyze_fasta() {\n    local INPUT=$1\n    local OUTPUT=$2\n    \n    echo \"Analyzing $INPUT...\"\n    \n    local SEQ_COUNT=$(count_sequences \"$INPUT\")\n    local TOTAL_LENGTH=$(grep -v \"^&gt;\" \"$INPUT\" | tr -d '\\n' | wc -c)\n    \n    echo \"Sequences: $SEQ_COUNT\" &gt; \"$OUTPUT\"\n    echo \"Total bases: $TOTAL_LENGTH\" &gt;&gt; \"$OUTPUT\"\n}\n\n# Call functions\nSEQ_NUM=$(count_sequences \"proteins.fa\")\necho \"Found $SEQ_NUM sequences\"\n\nanalyze_fasta \"input.fa\" \"statistics.txt\"\n\n6.8.1 Return Values\nFunctions return exit status (0-255), not strings. To return data:\n#!/bin/bash\n\n# Return via stdout\nget_gc_content() {\n    local FILE=$1\n    local GC=$(grep -v \"^&gt;\" \"$FILE\" | tr -d '\\n' | grep -o \"[GC]\" | wc -l)\n    local TOTAL=$(grep -v \"^&gt;\" \"$FILE\" | tr -d '\\n' | wc -c)\n    echo \"$((GC * 100 / TOTAL))\"\n}\n\n# Capture output\nGC_PERCENT=$(get_gc_content \"genome.fa\")\necho \"GC content: $GC_PERCENT%\"\n\n# Return status for success/failure\ncheck_file() {\n    if [ -f \"$1\" ] && [ -r \"$1\" ]; then\n        return 0  # Success\n    else\n        return 1  # Failure\n    fi\n}\n\nif check_file \"data.txt\"; then\n    echo \"File is ready\"\nelse\n    echo \"File not accessible\"\nfi",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#error-handling",
    "href": "chapters/06-shell-scripting.html#error-handling",
    "title": "6  Shell Scripting",
    "section": "6.9 Error Handling",
    "text": "6.9 Error Handling\nRobust scripts handle errors gracefully.\n\n6.9.1 Exit Status\nEvery command returns an exit status: 0 for success, non-zero for failure.\n#!/bin/bash\n\n# Check last command's status\ngrep \"pattern\" file.txt\nif [ $? -ne 0 ]; then\n    echo \"Pattern not found\"\nfi\n\n# Conditional execution\ngrep \"pattern\" file.txt && echo \"Found!\"\ngrep \"pattern\" file.txt || echo \"Not found\"\n\n\n6.9.2 Strict Mode\nStart scripts with these settings for better error detection:\n#!/bin/bash\nset -e          # Exit on any error\nset -u          # Error on undefined variables\nset -o pipefail # Catch errors in pipes\n\n# Or combine: set -euo pipefail\n\n\n6.9.3 Custom Error Handling\n#!/bin/bash\nset -euo pipefail\n\n# Define error handler\nerror_exit() {\n    echo \"Error on line $1\" &gt;&2\n    exit 1\n}\n\n# Set trap to call handler on error\ntrap 'error_exit $LINENO' ERR\n\n# Now errors are caught\ncheck_file() {\n    if [ ! -f \"$1\" ]; then\n        echo \"Error: File $1 not found!\" &gt;&2\n        exit 1\n    fi\n}\n\n# Validate inputs\ncheck_file \"$1\"\n\n# Continue processing...\n\n\n6.9.4 Cleanup on Exit\nEnsure temporary files are removed even if script fails:\n#!/bin/bash\n\n# Create temp file\nTMPFILE=$(mktemp)\n\n# Remove temp file on exit (normal or error)\ntrap \"rm -f $TMPFILE\" EXIT\n\n# Use temp file...\necho \"working data\" &gt; \"$TMPFILE\"\n# ... do processing ...\n\n# Cleanup happens automatically",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#a-complete-bioinformatics-script",
    "href": "chapters/06-shell-scripting.html#a-complete-bioinformatics-script",
    "title": "6  Shell Scripting",
    "section": "6.10 A Complete Bioinformatics Script",
    "text": "6.10 A Complete Bioinformatics Script\nLet’s put it all together with a realistic example:\n#!/bin/bash\n#===============================================================================\n# fasta_stats.sh - Calculate statistics for FASTA files\n# \n# Usage: ./fasta_stats.sh input.fasta [output.txt]\n#\n# Author: Your Name\n# Date: 2025-01-15\n#===============================================================================\n\nset -euo pipefail\n\n#-------------------------------------------------------------------------------\n# Configuration\n#-------------------------------------------------------------------------------\nVERSION=\"1.0.0\"\n\n#-------------------------------------------------------------------------------\n# Functions\n#-------------------------------------------------------------------------------\n\nusage() {\n    cat &lt;&lt; EOF\nUsage: $(basename \"$0\") [OPTIONS] input.fasta [output.txt]\n\nCalculate basic statistics for FASTA files.\n\nOptions:\n    -h, --help      Show this help message\n    -v, --version   Show version information\n    -q, --quiet     Suppress progress messages\n\nArguments:\n    input.fasta     Input FASTA file (required)\n    output.txt      Output file (optional, defaults to stdout)\n\nExamples:\n    $(basename \"$0\") sequences.fa\n    $(basename \"$0\") genome.fa stats.txt\n    $(basename \"$0\") -q input.fa &gt; results.txt\nEOF\n}\n\nlog() {\n    if [ \"$QUIET\" = false ]; then\n        echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" &gt;&2\n    fi\n}\n\nerror() {\n    echo \"Error: $1\" &gt;&2\n    exit 1\n}\n\ncount_sequences() {\n    grep -c \"^&gt;\" \"$1\"\n}\n\ntotal_length() {\n    grep -v \"^&gt;\" \"$1\" | tr -d '\\n' | wc -c | tr -d ' '\n}\n\ngc_content() {\n    local gc=$(grep -v \"^&gt;\" \"$1\" | tr -d '\\n' | grep -o \"[GC]\" | wc -l | tr -d ' ')\n    local total=$(total_length \"$1\")\n    if [ \"$total\" -gt 0 ]; then\n        echo \"$((gc * 100 / total))\"\n    else\n        echo \"0\"\n    fi\n}\n\n#-------------------------------------------------------------------------------\n# Parse Arguments\n#-------------------------------------------------------------------------------\n\nQUIET=false\n\nwhile [[ $# -gt 0 ]]; do\n    case $1 in\n        -h|--help)\n            usage\n            exit 0\n            ;;\n        -v|--version)\n            echo \"$(basename \"$0\") version $VERSION\"\n            exit 0\n            ;;\n        -q|--quiet)\n            QUIET=true\n            shift\n            ;;\n        -*)\n            error \"Unknown option: $1\"\n            ;;\n        *)\n            break\n            ;;\n    esac\ndone\n\n# Check required arguments\nif [ $# -lt 1 ]; then\n    usage &gt;&2\n    exit 1\nfi\n\nINPUT=\"$1\"\nOUTPUT=\"${2:-/dev/stdout}\"\n\n#-------------------------------------------------------------------------------\n# Validate Input\n#-------------------------------------------------------------------------------\n\nif [ ! -f \"$INPUT\" ]; then\n    error \"Input file not found: $INPUT\"\nfi\n\nif [ ! -r \"$INPUT\" ]; then\n    error \"Cannot read input file: $INPUT\"\nfi\n\n#-------------------------------------------------------------------------------\n# Main Analysis\n#-------------------------------------------------------------------------------\n\nlog \"Starting analysis of $INPUT\"\n\nlog \"Counting sequences...\"\nSEQ_COUNT=$(count_sequences \"$INPUT\")\n\nlog \"Calculating total length...\"\nTOTAL_LEN=$(total_length \"$INPUT\")\n\nlog \"Computing GC content...\"\nGC_PCT=$(gc_content \"$INPUT\")\n\n# Calculate average length\nif [ \"$SEQ_COUNT\" -gt 0 ]; then\n    AVG_LEN=$((TOTAL_LEN / SEQ_COUNT))\nelse\n    AVG_LEN=0\nfi\n\n#-------------------------------------------------------------------------------\n# Output Results\n#-------------------------------------------------------------------------------\n\n{\n    echo \"=== FASTA Statistics ===\"\n    echo \"File: $INPUT\"\n    echo \"Date: $(date '+%Y-%m-%d %H:%M:%S')\"\n    echo \"\"\n    echo \"Number of sequences: $SEQ_COUNT\"\n    echo \"Total length: $TOTAL_LEN bp\"\n    echo \"Average length: $AVG_LEN bp\"\n    echo \"GC content: $GC_PCT%\"\n} &gt; \"$OUTPUT\"\n\nlog \"Analysis complete. Results written to $OUTPUT\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#best-practices-summary",
    "href": "chapters/06-shell-scripting.html#best-practices-summary",
    "title": "6  Shell Scripting",
    "section": "6.11 Best Practices Summary",
    "text": "6.11 Best Practices Summary\n\n\n\n\n\n\nShell Scripting Best Practices\n\n\n\n\nAlways include a shebang: #!/bin/bash\nUse strict mode: set -euo pipefail\nComment your code: Explain what and why\nUse meaningful names: SEQ_COUNT not SC\nValidate inputs: Check files exist, arguments are provided\nHandle errors: Provide useful error messages\nMake it portable: Don’t hardcode paths\nTest incrementally: Build and test piece by piece\nVersion control: Track changes with git\nDocument usage: Include help messages",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#summary",
    "href": "chapters/06-shell-scripting.html#summary",
    "title": "6  Shell Scripting",
    "section": "6.12 Summary",
    "text": "6.12 Summary\nThis chapter covered shell scripting essentials:\n\nScripts are text files containing commands with a shebang line\nVariables store values; arrays store multiple values\nConditionals (if/elif/else) control flow based on tests\nLoops (for, while) repeat actions\nFunctions make code modular and reusable\nCommand-line arguments make scripts flexible\nError handling ensures robustness\nCombining these elements creates powerful analysis pipelines\n\nShell scripting is a foundational skill for computational biology. Even as you learn other languages like R and Python, shell scripts remain invaluable for automation, file management, and orchestrating complex workflows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/06-shell-scripting.html#exercises",
    "href": "chapters/06-shell-scripting.html#exercises",
    "title": "6  Shell Scripting",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Basic Script\nCreate a script that: 1. Accepts a directory as an argument 2. Counts files of each type (.txt, .csv, .fa, etc.) 3. Reports the results\nExercise 2: File Processing Loop\nWrite a script that: 1. Loops through all .fastq files in a directory 2. Counts the number of reads in each (hint: lines/4 in FASTQ) 3. Saves results to a summary file\nExercise 3: FASTA Validator\nCreate a script that validates FASTA files: 1. Check that every header line starts with &gt; 2. Check that sequence lines only contain valid characters 3. Report any errors found\nExercise 4: Analysis Pipeline\nBuild a complete analysis script that: 1. Accepts input/output arguments with validation 2. Downloads a bacterial genome if not present 3. Calculates genome statistics 4. Finds all occurrences of a user-specified motif 5. Generates a summary report 6. Includes proper error handling and logging",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Shell Scripting</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html",
    "href": "chapters/07-r-programming.html",
    "title": "7  R Programming Fundamentals",
    "section": "",
    "text": "7.1 Why R for Scientific Computing?\nR is a programming language designed specifically for statistical computing and graphics (R Core Team, 2024). Since its creation in 1995, R has become one of the most widely used tools in data science, statistics, and bioinformatics.\nR offers several advantages for researchers:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#why-r-for-scientific-computing",
    "href": "chapters/07-r-programming.html#why-r-for-scientific-computing",
    "title": "7  R Programming Fundamentals",
    "section": "",
    "text": "Statistical Power\n\nR was built by statisticians for statistics. It includes comprehensive implementations of classical and modern statistical methods.\n\nGraphics Excellence\n\nR produces publication-quality graphics with fine-grained control over every aspect of visualization.\n\nPackage Ecosystem\n\nOver 20,000 packages extend R’s capabilities for virtually every analytical need, from genomics to machine learning.\n\nReproducibility\n\nR scripts document your analysis completely. Combined with Quarto (which produced this book!), you can create reproducible documents that integrate code, results, and narrative.\n\nActive Community\n\nA large, welcoming community provides abundant resources, tutorials, and support.\n\nCost\n\nR is free and open-source, removing financial barriers to sophisticated analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#rstudio-your-r-environment",
    "href": "chapters/07-r-programming.html#rstudio-your-r-environment",
    "title": "7  R Programming Fundamentals",
    "section": "7.2 RStudio: Your R Environment",
    "text": "7.2 RStudio: Your R Environment\nWhile R can run from the command line, most users work in RStudio, an integrated development environment (IDE) that makes R more accessible and productive.\n\n7.2.1 The RStudio Interface\nRStudio organizes your workspace into four panes:\n\nSource (top-left)\n\nWhere you write and edit scripts. Code here isn’t executed until you explicitly run it.\n\nConsole (bottom-left)\n\nThe interactive R session. Commands typed here execute immediately. Output appears here.\n\nEnvironment/History (top-right)\n\nShows variables you’ve created (Environment tab) and commands you’ve run (History tab).\n\nFiles/Plots/Help (bottom-right)\n\nFile browser, plot viewer, and help documentation.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLearn keyboard shortcuts! Ctrl+Enter (Windows/Linux) or Cmd+Enter (Mac) runs the current line or selection. Ctrl+Shift+S runs the entire script.\n\n\n\n\n7.2.2 RStudio Projects\nRStudio Projects organize your work with self-contained directories:\n\nGo to File → New Project\nChoose New Directory → New Project\nName your project and select a location\nRStudio creates a .Rproj file\n\nProjects set your working directory automatically and keep related files together—essential for reproducible research.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#r-basics-arithmetic-and-variables",
    "href": "chapters/07-r-programming.html#r-basics-arithmetic-and-variables",
    "title": "7  R Programming Fundamentals",
    "section": "7.3 R Basics: Arithmetic and Variables",
    "text": "7.3 R Basics: Arithmetic and Variables\n\n7.3.1 R as a Calculator\nThe simplest use of R is arithmetic:\n\n# Basic operations\n4 * 4\n#&gt; [1] 16\n\n# Order of operations applies\n(4 + 3 * 2^2)\n#&gt; [1] 16\n\n# More complex expressions\nsqrt(144) + log(100)\n#&gt; [1] 16.60517\n\nR follows standard mathematical order of operations (PEMDAS).\n\n\n7.3.2 Creating Variables\nVariables store values for later use. In R, the assignment operator is &lt;-:\n\n# Assign values to variables\nx &lt;- 2\ny &lt;- 5\n\n# Use variables in calculations\nx * 3\n#&gt; [1] 6\ny + x\n#&gt; [1] 7\n\n# Store results in new variables\nz &lt;- x * y\nz\n#&gt; [1] 10\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also use = for assignment in R, but &lt;- is the traditional and preferred style. It makes assignment visually distinct from function arguments, which use =.\n\n\n\n\n7.3.3 Variable Naming Rules\n\nNames must start with a letter\nCan contain letters, numbers, periods, and underscores\nR is case-sensitive: Gene and gene are different variables\nUse descriptive names: sample_count is better than sc\n\n\n# Valid names\ngene_count &lt;- 100\nSample.1 &lt;- \"control\"\nmyData2 &lt;- 42\n\n# Invalid (would cause errors)\n# 2samples &lt;- 5      # Can't start with number\n# my-data &lt;- 10      # Hyphens not allowed",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#vectors-rs-fundamental-data-structure",
    "href": "chapters/07-r-programming.html#vectors-rs-fundamental-data-structure",
    "title": "7  R Programming Fundamentals",
    "section": "7.4 Vectors: R’s Fundamental Data Structure",
    "text": "7.4 Vectors: R’s Fundamental Data Structure\nR thinks in terms of vectors—ordered collections of values of the same type. Even a single number is a vector of length 1.\n\n7.4.1 Creating Vectors\nThe c() function (for “combine” or “concatenate”) creates vectors:\n\n# Numeric vector\nmeasurements &lt;- c(2, 3, 4, 2, 1, 2, 4, 5, 10, 8, 9)\nmeasurements\n#&gt;  [1]  2  3  4  2  1  2  4  5 10  8  9\n\n# Character vector\nsamples &lt;- c(\"control\", \"treatment_A\", \"treatment_B\")\nsamples\n#&gt; [1] \"control\"     \"treatment_A\" \"treatment_B\"\n\n# Logical vector\npassed &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE)\npassed\n#&gt; [1]  TRUE FALSE  TRUE  TRUE FALSE\n\n\n\n7.4.2 Sequence Generation\nFor regular sequences, use shortcuts:\n\n# Integer sequence\n1:10\n#&gt;  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Sequence with step\nseq(0, 10, by = 0.5)\n#&gt;  [1]  0.0  0.5  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0\n#&gt; [16]  7.5  8.0  8.5  9.0  9.5 10.0\n\n# Specified length\nseq(0, 1, length.out = 5)\n#&gt; [1] 0.00 0.25 0.50 0.75 1.00\n\n# Repeated values\nrep(\"A\", 5)\n#&gt; [1] \"A\" \"A\" \"A\" \"A\" \"A\"\nrep(c(1, 2), times = 3)\n#&gt; [1] 1 2 1 2 1 2\nrep(c(1, 2), each = 3)\n#&gt; [1] 1 1 1 2 2 2\n\n\n\n7.4.3 Vectorized Operations\nR’s power comes from vectorized operations—operations that apply to entire vectors at once:\n\nx &lt;- c(2, 3, 4, 2, 1, 2, 4, 5, 10, 8, 9)\n\n# Operations apply to each element\nx * 2\n#&gt;  [1]  4  6  8  4  2  4  8 10 20 16 18\nx^2\n#&gt;  [1]   4   9  16   4   1   4  16  25 100  64  81\nlog(x)\n#&gt;  [1] 0.6931472 1.0986123 1.3862944 0.6931472 0.0000000 0.6931472 1.3862944\n#&gt;  [8] 1.6094379 2.3025851 2.0794415 2.1972246\nsqrt(x)\n#&gt;  [1] 1.414214 1.732051 2.000000 1.414214 1.000000 1.414214 2.000000 2.236068\n#&gt;  [9] 3.162278 2.828427 3.000000\n\n# Operations between vectors (element-wise)\ny &lt;- 1:11\nx + y\n#&gt;  [1]  3  5  7  6  6  8 11 13 19 18 20\nx * y\n#&gt;  [1]  2  6 12  8  5 12 28 40 90 80 99\n\nThis is much faster and cleaner than writing loops!\n\n\n7.4.4 Vector Indexing\nAccess specific elements using square brackets:\n\nmeasurements &lt;- c(2, 3, 4, 2, 1, 2, 4, 5, 10, 8, 9)\n\n# Single element (1-indexed!)\nmeasurements[1]\n#&gt; [1] 2\nmeasurements[5]\n#&gt; [1] 1\n\n# Multiple elements\nmeasurements[c(1, 3, 5)]\n#&gt; [1] 2 4 1\n\n# Range\nmeasurements[2:5]\n#&gt; [1] 3 4 2 1\n\n# Negative indices exclude\nmeasurements[-1]          # All but first\n#&gt;  [1]  3  4  2  1  2  4  5 10  8  9\nmeasurements[-(1:3)]      # All but first three\n#&gt; [1]  2  1  2  4  5 10  8  9\n\n# Logical indexing\nmeasurements[measurements &gt; 5]\n#&gt; [1] 10  8  9\n\n\n\n\n\n\n\nImportant\n\n\n\nR uses 1-based indexing! The first element is [1], not [0] as in Python and most other languages.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#data-types-in-r",
    "href": "chapters/07-r-programming.html#data-types-in-r",
    "title": "7  R Programming Fundamentals",
    "section": "7.5 Data Types in R",
    "text": "7.5 Data Types in R\nR has several fundamental data types:\n\n\n\nTable 7.1: R data types\n\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nnumeric\nReal numbers (default)\n3.14, 42\n\n\ninteger\nWhole numbers\n1L, 100L\n\n\ncharacter\nText strings\n\"hello\", \"gene1\"\n\n\nlogical\nBoolean values\nTRUE, FALSE\n\n\nfactor\nCategorical variables\nTreatment levels\n\n\n\n\n\n\n\n# Check types with class()\nclass(3.14)\n#&gt; [1] \"numeric\"\nclass(\"hello\")\n#&gt; [1] \"character\"\nclass(TRUE)\n#&gt; [1] \"logical\"\n\n# Type coercion\nas.numeric(\"42\")\n#&gt; [1] 42\nas.character(123)\n#&gt; [1] \"123\"\nas.logical(0)    # FALSE\n#&gt; [1] FALSE\nas.logical(1)    # TRUE\n#&gt; [1] TRUE\n\n\n7.5.1 Special Values\nR has special values for missing data and undefined results:\n\n# Missing value\nNA\n#&gt; [1] NA\n\n# Not a number (undefined)\n0/0\n#&gt; [1] NaN\nlog(-1)\n#&gt; [1] NaN\n\n# Infinity\n1/0\n#&gt; [1] Inf\n-1/0\n#&gt; [1] -Inf\n\nHandling NA values is a common task in data analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#functions",
    "href": "chapters/07-r-programming.html#functions",
    "title": "7  R Programming Fundamentals",
    "section": "7.6 Functions",
    "text": "7.6 Functions\nFunctions perform operations on inputs and return outputs. R has thousands of built-in functions.\n\n7.6.1 Using Functions\n\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Basic statistical functions\nmean(x)\n#&gt; [1] 5.5\nmedian(x)\n#&gt; [1] 5.5\nsd(x)        # Standard deviation\n#&gt; [1] 3.02765\nvar(x)       # Variance\n#&gt; [1] 9.166667\nsum(x)\n#&gt; [1] 55\nlength(x)\n#&gt; [1] 10\nmin(x)\n#&gt; [1] 1\nmax(x)\n#&gt; [1] 10\nrange(x)     # Returns min and max\n#&gt; [1]  1 10\n\n\n\n7.6.2 Function Arguments\nFunctions take arguments that modify their behavior:\n\n# Round to 2 decimal places\nround(3.14159, digits = 2)\n#&gt; [1] 3.14\n\n# Sample with replacement\nsample(1:10, size = 5, replace = TRUE)\n#&gt; [1] 5 3 6 3 2\n\n# Mean with NA handling\nvalues &lt;- c(1, 2, NA, 4, 5)\nmean(values)              # Returns NA\n#&gt; [1] NA\nmean(values, na.rm = TRUE)  # Removes NA first\n#&gt; [1] 3\n\n\n\n7.6.3 Getting Help\nR has excellent built-in documentation:\n\n# Get help on a function\n?mean\nhelp(mean)\n\n# Search help\nhelp.search(\"correlation\")\n??correlation\n\n# See function arguments\nargs(mean)\n\n# See examples\nexample(mean)\n\nThe help page structure:\n\nDescription: What the function does\nUsage: Function syntax\nArguments: What inputs it accepts\nValue: What it returns\nExamples: Working code examples",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#data-frames",
    "href": "chapters/07-r-programming.html#data-frames",
    "title": "7  R Programming Fundamentals",
    "section": "7.7 Data Frames",
    "text": "7.7 Data Frames\nData frames are R’s workhorse for tabular data—think spreadsheets or database tables. Each column is a vector, and columns can have different types.\n\n7.7.1 Creating Data Frames\n\n# Create vectors\nsample_id &lt;- c(\"S1\", \"S2\", \"S3\", \"S4\", \"S5\")\ntreatment &lt;- c(\"control\", \"drug_A\", \"drug_A\", \"drug_B\", \"drug_B\")\nconcentration &lt;- c(0, 10, 20, 10, 20)\nresponse &lt;- c(1.2, 3.4, 5.6, 2.8, 4.1)\n\n# Combine into data frame\nexperiment &lt;- data.frame(\n  sample_id = sample_id,\n  treatment = treatment,\n  concentration = concentration,\n  response = response\n)\n\nexperiment\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\nS1\ncontrol\n0\n1.2\n\n\nS2\ndrug_A\n10\n3.4\n\n\nS3\ndrug_A\n20\n5.6\n\n\nS4\ndrug_B\n10\n2.8\n\n\nS5\ndrug_B\n20\n4.1\n\n\n\n\n\n\n\n\n7.7.2 Examining Data Frames\n\n# Structure\nstr(experiment)\n#&gt; 'data.frame':    5 obs. of  4 variables:\n#&gt;  $ sample_id    : chr  \"S1\" \"S2\" \"S3\" \"S4\" ...\n#&gt;  $ treatment    : chr  \"control\" \"drug_A\" \"drug_A\" \"drug_B\" ...\n#&gt;  $ concentration: num  0 10 20 10 20\n#&gt;  $ response     : num  1.2 3.4 5.6 2.8 4.1\n\n# Dimensions\ndim(experiment)\n#&gt; [1] 5 4\nnrow(experiment)\n#&gt; [1] 5\nncol(experiment)\n#&gt; [1] 4\n\n# Preview\nhead(experiment, 3)\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\nS1\ncontrol\n0\n1.2\n\n\nS2\ndrug_A\n10\n3.4\n\n\nS3\ndrug_A\n20\n5.6\n\n\n\n\n\ntail(experiment, 2)\n\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\n4\nS4\ndrug_B\n10\n2.8\n\n\n5\nS5\ndrug_B\n20\n4.1\n\n\n\n\n\n\n# Summary statistics\nsummary(experiment)\n#&gt;   sample_id          treatment         concentration    response   \n#&gt;  Length:5           Length:5           Min.   : 0    Min.   :1.20  \n#&gt;  Class :character   Class :character   1st Qu.:10    1st Qu.:2.80  \n#&gt;  Mode  :character   Mode  :character   Median :10    Median :3.40  \n#&gt;                                        Mean   :12    Mean   :3.42  \n#&gt;                                        3rd Qu.:20    3rd Qu.:4.10  \n#&gt;                                        Max.   :20    Max.   :5.60\n\n# Column names\nnames(experiment)\n#&gt; [1] \"sample_id\"     \"treatment\"     \"concentration\" \"response\"\ncolnames(experiment)\n#&gt; [1] \"sample_id\"     \"treatment\"     \"concentration\" \"response\"\n\n\n\n7.7.3 Accessing Data Frame Elements\n\n# Single column (returns vector)\nexperiment$response\n#&gt; [1] 1.2 3.4 5.6 2.8 4.1\nexperiment[[\"response\"]]\n#&gt; [1] 1.2 3.4 5.6 2.8 4.1\nexperiment[, \"response\"]\n#&gt; [1] 1.2 3.4 5.6 2.8 4.1\n\n# Multiple columns\nexperiment[, c(\"sample_id\", \"response\")]\n\n\n\n\n\nsample_id\nresponse\n\n\n\n\nS1\n1.2\n\n\nS2\n3.4\n\n\nS3\n5.6\n\n\nS4\n2.8\n\n\nS5\n4.1\n\n\n\n\n\n\n# Rows by index\nexperiment[1, ]\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\nS1\ncontrol\n0\n1.2\n\n\n\n\n\nexperiment[1:3, ]\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\nS1\ncontrol\n0\n1.2\n\n\nS2\ndrug_A\n10\n3.4\n\n\nS3\ndrug_A\n20\n5.6\n\n\n\n\n\n\n# Combination\nexperiment[1:3, c(\"treatment\", \"response\")]\n\n\n\n\n\ntreatment\nresponse\n\n\n\n\ncontrol\n1.2\n\n\ndrug_A\n3.4\n\n\ndrug_A\n5.6\n\n\n\n\n\n\n# Conditional selection\nexperiment[experiment$treatment == \"drug_A\", ]\n\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\n2\nS2\ndrug_A\n10\n3.4\n\n\n3\nS3\ndrug_A\n20\n5.6\n\n\n\n\n\nexperiment[experiment$response &gt; 3, ]\n\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\n\n\n\n\n2\nS2\ndrug_A\n10\n3.4\n\n\n3\nS3\ndrug_A\n20\n5.6\n\n\n5\nS5\ndrug_B\n20\n4.1\n\n\n\n\n\n\n\n\n7.7.4 Adding and Modifying Columns\n\n# Add a new column\nexperiment$replicate &lt;- c(1, 1, 2, 1, 2)\n\n# Calculated column\nexperiment$log_response &lt;- log(experiment$response)\n\nexperiment\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_id\ntreatment\nconcentration\nresponse\nreplicate\nlog_response\n\n\n\n\nS1\ncontrol\n0\n1.2\n1\n0.1823216\n\n\nS2\ndrug_A\n10\n3.4\n1\n1.2237754\n\n\nS3\ndrug_A\n20\n5.6\n2\n1.7227666\n\n\nS4\ndrug_B\n10\n2.8\n1\n1.0296194\n\n\nS5\ndrug_B\n20\n4.1\n2\n1.4109870",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#reading-and-writing-data",
    "href": "chapters/07-r-programming.html#reading-and-writing-data",
    "title": "7  R Programming Fundamentals",
    "section": "7.8 Reading and Writing Data",
    "text": "7.8 Reading and Writing Data\n\n7.8.1 Reading Files\n\n# CSV files (comma-separated)\ndata &lt;- read.csv(\"data.csv\")\n\n# Tab-separated files\ndata &lt;- read.table(\"data.tsv\", header = TRUE, sep = \"\\t\")\n\n# Excel files (requires readxl package)\nlibrary(readxl)\ndata &lt;- read_excel(\"data.xlsx\")\n\n# Specify options\ndata &lt;- read.csv(\"data.csv\",\n                 header = TRUE,\n                 stringsAsFactors = FALSE,\n                 na.strings = c(\"\", \"NA\", \"N/A\"))\n\n\n\n7.8.2 Writing Files\n\n# CSV output\nwrite.csv(experiment, \"experiment_results.csv\", row.names = FALSE)\n\n# Tab-separated\nwrite.table(experiment, \"results.tsv\", \n            sep = \"\\t\", \n            quote = FALSE, \n            row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#basic-visualization",
    "href": "chapters/07-r-programming.html#basic-visualization",
    "title": "7  R Programming Fundamentals",
    "section": "7.9 Basic Visualization",
    "text": "7.9 Basic Visualization\nR excels at creating graphics. Here’s a quick introduction to base R plotting:\n\n7.9.1 Scatter Plots\n\nx &lt;- 1:10\ny &lt;- x^2\n\nplot(x, y, \n     main = \"Quadratic Relationship\",\n     xlab = \"X values\",\n     ylab = \"Y values\",\n     col = \"darkblue\",\n     pch = 16)  # Filled circles\n\n\n\n\n\n\n\nFigure 7.1: Basic scatter plot showing quadratic relationship\n\n\n\n\n\n\n\n7.9.2 Histograms\n\n# Generate random data\ndata &lt;- rnorm(1000, mean = 50, sd = 10)\n\nhist(data,\n     main = \"Distribution of Values\",\n     xlab = \"Value\",\n     col = \"steelblue\",\n     breaks = 30)\n\n\n\n\n\n\n\nFigure 7.2: Histogram of normally distributed data\n\n\n\n\n\n\n\n7.9.3 Box Plots\n\n# Create sample data\nset.seed(42)\ncontrol &lt;- rnorm(30, mean = 10, sd = 2)\ntreatment &lt;- rnorm(30, mean = 15, sd = 3)\n\nboxplot(control, treatment,\n        names = c(\"Control\", \"Treatment\"),\n        main = \"Treatment Effect\",\n        ylab = \"Response\",\n        col = c(\"lightblue\", \"lightcoral\"))\n\n\n\n\n\n\n\nFigure 7.3: Box plot comparing treatment groups\n\n\n\n\n\n\n\n7.9.4 Multiple Plots\n\n# Create 2x2 layout\npar(mfrow = c(2, 2))\n\n# Four different plots\nplot(1:10, (1:10)^2, type = \"l\", main = \"Line Plot\")\nhist(rnorm(100), main = \"Histogram\")\nboxplot(rnorm(50), main = \"Box Plot\")\nbarplot(c(3, 5, 2, 7), names.arg = c(\"A\", \"B\", \"C\", \"D\"), main = \"Bar Plot\")\n\n# Reset to single plot\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nFigure 7.4: Multiple plots in a single figure\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor publication-quality graphics, explore the ggplot2 package (Wickham et al., 2023), which provides a powerful and consistent grammar of graphics.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#random-sampling-and-simulation",
    "href": "chapters/07-r-programming.html#random-sampling-and-simulation",
    "title": "7  R Programming Fundamentals",
    "section": "7.10 Random Sampling and Simulation",
    "text": "7.10 Random Sampling and Simulation\nR makes it easy to generate random data and run simulations:\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Random samples from distributions\nuniform_sample &lt;- runif(100, min = 0, max = 1)\nnormal_sample &lt;- rnorm(1000, mean = 0, sd = 1)\npoisson_sample &lt;- rpois(100, lambda = 5)\n\n# Visualize normal distribution\nhist(normal_sample, \n     probability = TRUE,  # Density instead of counts\n     main = \"Sample vs. Theoretical Distribution\",\n     col = \"lightblue\")\n\n# Add theoretical curve\ncurve(dnorm(x, mean = 0, sd = 1), \n      add = TRUE, \n      col = \"red\", \n      lwd = 2)\n\n\n\n\n\n\n\nFigure 7.5: Simulated data from a normal distribution with theoretical curve overlay\n\n\n\n\n\n\n7.10.1 Common Distribution Functions\nFor distribution xxx (e.g., norm, unif, pois, binom):\n\nrxxx() — Random samples\ndxxx() — Density/probability\npxxx() — Cumulative distribution function\nqxxx() — Quantile function\n\n\n# Normal distribution\nrnorm(5)        # 5 random values\n#&gt; [1]  0.9159921  0.8006224 -0.9365690 -1.4007874  0.1602775\ndnorm(0)        # Density at x=0\n#&gt; [1] 0.3989423\npnorm(1.96)     # P(X ≤ 1.96)\n#&gt; [1] 0.9750021\nqnorm(0.975)    # Value where P(X ≤ x) = 0.975\n#&gt; [1] 1.959964",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#summary",
    "href": "chapters/07-r-programming.html#summary",
    "title": "7  R Programming Fundamentals",
    "section": "7.11 Summary",
    "text": "7.11 Summary\nThis chapter introduced R programming fundamentals:\n\nR is designed for statistical computing with excellent graphics\nRStudio provides an integrated development environment\nVariables are assigned with &lt;-; R is case-sensitive\nVectors are the fundamental data structure; operations are vectorized\nData frames hold tabular data with columns of different types\nFunctions perform operations; use ?function for help\nR reads/writes CSV, TSV, and Excel files easily\nBasic plotting is built-in; ggplot2 offers advanced graphics\n\nThese fundamentals prepare you for more advanced R programming, including the tidyverse packages covered in subsequent chapters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/07-r-programming.html#exercises",
    "href": "chapters/07-r-programming.html#exercises",
    "title": "7  R Programming Fundamentals",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Vector Operations\n\nCreate a vector of the first 20 integers\nCalculate the mean, median, and standard deviation\nCreate a new vector containing only the even numbers\nCalculate the sum of squares\n\nExercise 2: Data Frame Practice\nCreate a data frame representing an experiment: 1. Include columns for: sample_id, group (control/treatment), measurement 2. Add 10 rows of sample data 3. Calculate the mean measurement for each group 4. Add a new column with log-transformed measurements\nExercise 3: Random Simulation\n\nGenerate 1000 random samples from a normal distribution with mean=100 and sd=15\nCreate a histogram of the data\nCalculate what proportion of values fall between 85 and 115\nCompare to the theoretical proportion for a normal distribution\n\nExercise 4: File I/O\n\nCreate a data frame with experimental data\nSave it as a CSV file\nRead it back into a new variable\nVerify the data matches the original\n\n\n\n\n\n\n\nR Core Team (2024). R: A language and environment for statistical computing.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>R Programming Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html",
    "href": "chapters/08-tidy-data.html",
    "title": "8  Tidy Data Principles",
    "section": "",
    "text": "8.1 Why Data Organization Matters\nBefore you can analyze data, you must organize it. The structure of your data profoundly affects how easily you can work with it. A well-organized dataset can be analyzed in minutes; a poorly organized one might require hours of preprocessing.\nConsider this scenario: you’ve completed an experiment and collected data in a spreadsheet. Now you need to analyze it. If your data is organized consistently and logically, analysis is straightforward. If it’s scattered across multiple tabs with inconsistent formatting, merged cells, and color-coded values, you’re in for a frustrating experience.\nTidy data provides a standard way to organize data that makes analysis easier. It’s not the only valid way to structure data, but it’s particularly well-suited for analysis in R and Python.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#the-three-principles-of-tidy-data",
    "href": "chapters/08-tidy-data.html#the-three-principles-of-tidy-data",
    "title": "8  Tidy Data Principles",
    "section": "8.2 The Three Principles of Tidy Data",
    "text": "8.2 The Three Principles of Tidy Data\nHadley Wickham formalized the concept of tidy data in a influential paper (Wickham, 2014). Tidy data follows three rules:\n\n\n\n\n\n\nThe Three Rules of Tidy Data\n\n\n\n\nEach variable forms a column\nEach observation forms a row\nEach value has its own cell\n\n\n\nLet’s unpack what these mean.\n\n8.2.1 Variables as Columns\nA variable is a characteristic that you measure, observe, or categorize. Examples:\n\nGene expression level\nTreatment group\nPatient age\nSample ID\nMeasurement date\n\nEach variable should have its own column with a descriptive header.\n\n\n8.2.2 Observations as Rows\nAn observation is a single unit of data collection—typically one measurement from one subject at one time point. Each observation gets its own row.\n\n\n8.2.3 Values in Cells\nEach cell contains exactly one value. No merged cells, no multiple values crammed together, no implicit values indicated by color or formatting.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#tidy-vs.-messy-data-examples",
    "href": "chapters/08-tidy-data.html#tidy-vs.-messy-data-examples",
    "title": "8  Tidy Data Principles",
    "section": "8.3 Tidy vs. Messy Data: Examples",
    "text": "8.3 Tidy vs. Messy Data: Examples\n\n8.3.1 A Tidy Dataset\n\ntidy_data &lt;- tibble(\n  sample_id = c(\"S001\", \"S002\", \"S003\", \"S001\", \"S002\", \"S003\"),\n  timepoint = c(\"0h\", \"0h\", \"0h\", \"24h\", \"24h\", \"24h\"),\n  gene = rep(\"BRCA1\", 6),\n  expression = c(4.2, 3.8, 4.5, 8.1, 7.9, 9.2)\n)\n\ntidy_data |&gt;\n  gt() |&gt;\n  tab_header(title = \"Gene Expression Data (Tidy Format)\")\n\n\n\nTable 8.1: Example of tidy data organization\n\n\n\n\n\n\n\n\n\nGene Expression Data (Tidy Format)\n\n\nsample_id\ntimepoint\ngene\nexpression\n\n\n\n\nS001\n0h\nBRCA1\n4.2\n\n\nS002\n0h\nBRCA1\n3.8\n\n\nS003\n0h\nBRCA1\n4.5\n\n\nS001\n24h\nBRCA1\n8.1\n\n\nS002\n24h\nBRCA1\n7.9\n\n\nS003\n24h\nBRCA1\n9.2\n\n\n\n\n\n\n\n\n\n\nThis is tidy because:\n\nEach column is a variable (sample_id, timepoint, gene, expression)\nEach row is an observation (one expression measurement)\nEach cell has one value\n\n\n\n8.3.2 A Messy Dataset (Same Data)\nA common messy format spreads measurements across columns:\n\nmessy_data &lt;- tibble(\n  sample_id = c(\"S001\", \"S002\", \"S003\"),\n  `BRCA1_0h` = c(4.2, 3.8, 4.5),\n  `BRCA1_24h` = c(8.1, 7.9, 9.2)\n)\n\nmessy_data |&gt;\n  gt() |&gt;\n  tab_header(title = \"Gene Expression Data (Messy Format)\")\n\n\n\nTable 8.2: The same data in messy (wide) format\n\n\n\n\n\n\n\n\n\nGene Expression Data (Messy Format)\n\n\nsample_id\nBRCA1_0h\nBRCA1_24h\n\n\n\n\nS001\n4.2\n8.1\n\n\nS002\n3.8\n7.9\n\n\nS003\n4.5\n9.2\n\n\n\n\n\n\n\n\n\n\nThis is messy because:\n\nTime point and gene are embedded in column names\nEach row represents multiple observations\nAdding genes or timepoints requires adding columns\n\n\n\n8.3.3 Why It Matters\nWith tidy data, operations are straightforward:\n\n# Calculate mean expression by timepoint\ntidy_data |&gt;\n  group_by(timepoint) |&gt;\n  summarize(mean_expression = mean(expression))\n\n\n\n\n\ntimepoint\nmean_expression\n\n\n\n\n0h\n4.166667\n\n\n24h\n8.400000\n\n\n\n\n\n\n# Easy to filter and plot\ntidy_data |&gt;\n  filter(timepoint == \"24h\")\n\n\n\n\n\nsample_id\ntimepoint\ngene\nexpression\n\n\n\n\nS001\n24h\nBRCA1\n8.1\n\n\nS002\n24h\nBRCA1\n7.9\n\n\nS003\n24h\nBRCA1\n9.2\n\n\n\n\n\n\nWith messy data, you must first reshape it—or write complex code to work around the structure.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#common-data-problems",
    "href": "chapters/08-tidy-data.html#common-data-problems",
    "title": "8  Tidy Data Principles",
    "section": "8.4 Common Data Problems",
    "text": "8.4 Common Data Problems\n\n8.4.1 Problem 1: Column Headers Are Values\nOften, column names contain data values rather than variable names:\n\n# Messy: years in column headers\npopulation_messy &lt;- tibble(\n  country = c(\"USA\", \"Canada\", \"Mexico\"),\n  `2020` = c(331, 38, 129),\n  `2021` = c(332, 38, 130),\n  `2022` = c(333, 39, 131)\n)\n\npopulation_messy |&gt; gt()\n\n\n\n\n\n\n\ncountry\n2020\n2021\n2022\n\n\n\n\nUSA\n331\n332\n333\n\n\nCanada\n38\n38\n39\n\n\nMexico\n129\n130\n131\n\n\n\n\n\n\n\nSolution: “Pivot” the data to create a year column:\n\npopulation_tidy &lt;- population_messy |&gt;\n  pivot_longer(\n    cols = `2020`:`2022`,\n    names_to = \"year\",\n    values_to = \"population\"\n  )\n\npopulation_tidy |&gt; gt()\n\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nUSA\n2020\n331\n\n\nUSA\n2021\n332\n\n\nUSA\n2022\n333\n\n\nCanada\n2020\n38\n\n\nCanada\n2021\n38\n\n\nCanada\n2022\n39\n\n\nMexico\n2020\n129\n\n\nMexico\n2021\n130\n\n\nMexico\n2022\n131\n\n\n\n\n\n\n\n\n\n8.4.2 Problem 2: Multiple Variables in One Column\nSometimes multiple pieces of information are crammed into one cell:\n\n# Messy: rate contains both values\nmessy_rates &lt;- tibble(\n  country = c(\"USA\", \"Canada\", \"Mexico\"),\n  rate = c(\"1000/50000\", \"200/10000\", \"500/25000\")\n)\n\nmessy_rates |&gt; gt()\n\n\n\n\n\n\n\ncountry\nrate\n\n\n\n\nUSA\n1000/50000\n\n\nCanada\n200/10000\n\n\nMexico\n500/25000\n\n\n\n\n\n\n\nSolution: Separate into distinct columns:\n\ntidy_rates &lt;- messy_rates |&gt;\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\", convert = TRUE)\n\ntidy_rates |&gt; gt()\n\n\n\n\n\n\n\ncountry\ncases\npopulation\n\n\n\n\nUSA\n1000\n50000\n\n\nCanada\n200\n10000\n\n\nMexico\n500\n25000\n\n\n\n\n\n\n\n\n\n8.4.3 Problem 3: Variables in Both Rows and Columns\nComplex tables sometimes mix variables across dimensions:\n\n# Messy: measurement type in rows, conditions in columns\nmessy_experiment &lt;- tibble(\n  measurement = c(\"weight\", \"height\", \"weight\", \"height\"),\n  subject = c(\"A\", \"A\", \"B\", \"B\"),\n  control = c(70, 170, 65, 165),\n  treatment = c(68, 170, 64, 166)\n)\n\nmessy_experiment |&gt; gt()\n\n\n\n\n\n\n\nmeasurement\nsubject\ncontrol\ntreatment\n\n\n\n\nweight\nA\n70\n68\n\n\nheight\nA\n170\n170\n\n\nweight\nB\n65\n64\n\n\nheight\nB\n165\n166\n\n\n\n\n\n\n\nThis requires multiple steps to tidy: pivot longer, then pivot wider.\n\n\n8.4.4 Problem 4: Multiple Observational Units\nWhen a single table contains data about different types of things:\n\n# Messy: patient and hospital info in same table\ncombined_data &lt;- tibble(\n  patient_id = c(\"P001\", \"P002\"),\n  patient_name = c(\"Alice\", \"Bob\"),\n  hospital_id = c(\"H1\", \"H1\"),\n  hospital_name = c(\"General\", \"General\"),\n  hospital_beds = c(500, 500),  # Repeated!\n  diagnosis = c(\"Flu\", \"Cold\")\n)\n\ncombined_data |&gt; gt()\n\n\n\n\n\n\n\npatient_id\npatient_name\nhospital_id\nhospital_name\nhospital_beds\ndiagnosis\n\n\n\n\nP001\nAlice\nH1\nGeneral\n500\nFlu\n\n\nP002\nBob\nH1\nGeneral\n500\nCold\n\n\n\n\n\n\n\nSolution: Normalize into separate tables linked by keys.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#best-practices-for-data-organization",
    "href": "chapters/08-tidy-data.html#best-practices-for-data-organization",
    "title": "8  Tidy Data Principles",
    "section": "8.5 Best Practices for Data Organization",
    "text": "8.5 Best Practices for Data Organization\n\n8.5.1 Naming Conventions\n\n\n\n\n\n\nData File Naming Rules\n\n\n\nFile names:\n\nUse descriptive names: patient_outcomes_2024.csv\nInclude dates in ISO format: YYYY-MM-DD\nAvoid spaces (use underscores or hyphens)\nKeep names concise but informative\n\nColumn names:\n\nUse lowercase with underscores: sample_id, gene_expression\nMake names meaningful: treatment_group not tg\nAvoid special characters and spaces\nStart with a letter, not a number\n\n\n\n\n\n8.5.2 File Formats\n\n\n\nTable 8.3: Common data file formats\n\n\n\n\n\nFormat\nExtension\nBest For\n\n\n\n\nComma-separated\n.csv\nGeneral tabular data\n\n\nTab-separated\n.tsv\nData containing commas\n\n\nPlain text\n.txt\nSimple data, scripts\n\n\nExcel\n.xlsx\nSharing with non-programmers\n\n\nRDS\n.rds\nR-specific data with types preserved\n\n\nParquet\n.parquet\nLarge datasets, fast reading\n\n\n\n\n\n\nFor long-term storage and sharing, prefer plain text formats (CSV, TSV) over proprietary formats. They’re human-readable and don’t require specific software.\n\n\n8.5.3 Documentation\nAlways create a data dictionary (also called a codebook) documenting:\n\nVariable names and descriptions\nUnits of measurement\nAllowable values or categories\nCoding of missing values\nSource of the data\nDate created/modified\n\n\n\n8.5.4 Preserving Raw Data\n\n\n\n\n\n\nNever Modify Raw Data\n\n\n\nAlways keep an unchanged copy of your original data. Create a separate processed version for analysis. This allows you to:\n\nRetrace your steps if something goes wrong\nRerun analysis with different preprocessing\nShare the original data with collaborators",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#data-types",
    "href": "chapters/08-tidy-data.html#data-types",
    "title": "8  Tidy Data Principles",
    "section": "8.6 Data Types",
    "text": "8.6 Data Types\nUnderstanding data types helps you organize and analyze correctly.\n\n8.6.1 Categorical vs. Quantitative\n\ndata_types &lt;- tibble(\n  Category = c(\"Categorical\", \"Categorical\", \"Quantitative\", \"Quantitative\"),\n  Type = c(\"Ordinal\", \"Nominal\", \"Ratio\", \"Interval\"),\n  Description = c(\n    \"Ordered categories\",\n    \"Unordered categories\",\n    \"True zero, ratios meaningful\",\n    \"No true zero\"\n  ),\n  Examples = c(\n    \"small, medium, large\",\n    \"red, blue, green\",\n    \"height, weight, concentration\",\n    \"temperature (°C), calendar year\"\n  )\n)\n\ndata_types |&gt;\n  gt() |&gt;\n  tab_header(title = \"Data Type Classification\")\n\n\n\nTable 8.4: Classification of data types\n\n\n\n\n\n\n\n\n\nData Type Classification\n\n\nCategory\nType\nDescription\nExamples\n\n\n\n\nCategorical\nOrdinal\nOrdered categories\nsmall, medium, large\n\n\nCategorical\nNominal\nUnordered categories\nred, blue, green\n\n\nQuantitative\nRatio\nTrue zero, ratios meaningful\nheight, weight, concentration\n\n\nQuantitative\nInterval\nNo true zero\ntemperature (°C), calendar year\n\n\n\n\n\n\n\n\n\n\n\n\n8.6.2 R Data Types\nR has specific data types for these categories:\n\n# Numeric (continuous)\ntemperature &lt;- c(37.2, 36.8, 38.1)\nclass(temperature)\n#&gt; [1] \"numeric\"\n\n# Integer\ncounts &lt;- c(1L, 2L, 3L)\nclass(counts)\n#&gt; [1] \"integer\"\n\n# Character\nsample_names &lt;- c(\"control\", \"treatment\", \"treatment\")\nclass(sample_names)\n#&gt; [1] \"character\"\n\n# Factor (categorical)\ntreatment &lt;- factor(c(\"low\", \"medium\", \"high\"), \n                   levels = c(\"low\", \"medium\", \"high\"),\n                   ordered = TRUE)\nclass(treatment)\n#&gt; [1] \"ordered\" \"factor\"\ntreatment\n#&gt; [1] low    medium high  \n#&gt; Levels: low &lt; medium &lt; high\n\n# Logical\npassed_qc &lt;- c(TRUE, TRUE, FALSE)\nclass(passed_qc)\n#&gt; [1] \"logical\"\n\n\n\n8.6.3 When to Use Factors\nFactors are R’s way of handling categorical variables. Use them when:\n\nYou have a limited set of possible values\nThe order matters (ordinal data)\nYou want specific grouping in analyses and plots\n\n\n# Character vector sorts alphabetically\ntreatments &lt;- c(\"High\", \"Low\", \"Medium\", \"Low\", \"High\")\nsort(treatments)\n#&gt; [1] \"High\"   \"High\"   \"Low\"    \"Low\"    \"Medium\"\n\n# Factor maintains meaningful order\ntreatments_factor &lt;- factor(treatments, \n                           levels = c(\"Low\", \"Medium\", \"High\"))\nsort(treatments_factor)\n#&gt; [1] Low    Low    Medium High   High  \n#&gt; Levels: Low Medium High",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#structuring-spreadsheets-for-analysis",
    "href": "chapters/08-tidy-data.html#structuring-spreadsheets-for-analysis",
    "title": "8  Tidy Data Principles",
    "section": "8.7 Structuring Spreadsheets for Analysis",
    "text": "8.7 Structuring Spreadsheets for Analysis\nIf you must use spreadsheets for data entry, follow these guidelines:\n\n\n\n\n\n\nSpreadsheet Best Practices\n\n\n\n\nOne header row only — Put variable names in row 1\nNo merged cells — They break data structure\nNo color-coding as data — Use a column instead\nConsistent missing values — Use NA, not blank, -, or N/A\nOne sheet per dataset — Don’t spread data across tabs\nExport as CSV — Preserve as plain text\nNo calculations in raw data — Keep raw and calculated data separate\n\n\n\n\n8.7.1 Before and After\nBad spreadsheet practices:\n\nMerged cells for visual grouping\nColor-coded cells indicating categories\nMultiple header rows\nNotes in random cells\nMixed data types in columns\nEmbedded calculations\n\nGood spreadsheet practices:\n\nClean rectangular data\nSingle header row\nConsistent formatting throughout\nSeparate documentation sheet\nExplicit coding of all information",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#summary",
    "href": "chapters/08-tidy-data.html#summary",
    "title": "8  Tidy Data Principles",
    "section": "8.8 Summary",
    "text": "8.8 Summary\nTidy data provides a consistent structure that simplifies analysis:\n\nEach variable is a column\nEach observation is a row\nEach value occupies one cell\n\nKey principles for data organization:\n\nUse descriptive, consistent naming conventions\nDocument your data with a data dictionary\nPreserve raw data separately from processed data\nChoose appropriate file formats for your needs\nUnderstand the difference between data types\n\nThese practices apply regardless of whether you’re using R, Python, or other tools. Well-organized data is easier to analyze, share, and preserve.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/08-tidy-data.html#exercises",
    "href": "chapters/08-tidy-data.html#exercises",
    "title": "8  Tidy Data Principles",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: Identify Tidy Data\nFor each dataset description, determine if it’s tidy. If not, explain what violates tidy principles:\n\nPatient data with columns: patient_id, age, blood_pressure_systolic, blood_pressure_diastolic\nGene expression with columns: gene_name, sample_1, sample_2, sample_3\nSurvey data with columns: respondent_id, question, response\nWeather data with columns: city, 2023_temp, 2024_temp, 2023_precip, 2024_precip\n\nExercise 2: Reshape Data\nGiven this messy dataset:\nCountry   1990   2000   2010\nUSA       250    280    310  \nCanada    27     31     35\nMexico    86     100    120\nWrite R code using pivot_longer() to convert it to tidy format.\nExercise 3: Data Dictionary\nCreate a data dictionary for an experiment tracking: - Gene expression levels measured at 0, 12, and 24 hours - Three treatment groups (control, low dose, high dose) - Twenty biological replicates per group - Quality control pass/fail flags\nExercise 4: Spot the Problems\nReview a dataset you’ve worked with (or your lab’s data). Identify any violations of tidy data principles and propose solutions.\n\n\n\n\n\n\nWickham, H. (2014). Tidy data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data Principles</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html",
    "href": "chapters/09-git-github.html",
    "title": "9  Version Control with Git and GitHub",
    "section": "",
    "text": "9.1 Why Version Control?\nHave you ever had files named like this?\nThis ad-hoc versioning is error-prone, confusing, and doesn’t scale. Version control solves this problem by systematically tracking changes to files over time.\nVersion control systems provide:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#why-version-control",
    "href": "chapters/09-git-github.html#why-version-control",
    "title": "9  Version Control with Git and GitHub",
    "section": "",
    "text": "analysis_final.R\nanalysis_final_v2.R\nanalysis_final_v2_REALLY_FINAL.R\nanalysis_final_v2_REALLY_FINAL_fixed.R\n\n\n\nComplete History\n\nEvery change is recorded. You can see what changed, when, and why.\n\nSafe Experimentation\n\nTry new approaches without fear. You can always return to a working version.\n\nCollaboration\n\nMultiple people can work on the same project without overwriting each other’s work.\n\nBackup\n\nYour code exists in multiple places, protecting against data loss.\n\nReproducibility\n\nTag specific versions for publications. Anyone can access exactly what you used (Blischak et al., 2016).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#git-distributed-version-control",
    "href": "chapters/09-git-github.html#git-distributed-version-control",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.2 Git: Distributed Version Control",
    "text": "9.2 Git: Distributed Version Control\nGit is the most widely used version control system. Created by Linus Torvalds (who also created Linux), Git is:\n\nDistributed: Every copy is a complete repository with full history\nFast: Operations happen locally without network delays\nFlexible: Supports many workflows from solo to large team projects\n\n\n9.2.1 Key Concepts\nUnderstanding Git’s mental model helps everything else make sense:\n\nflowchart LR\n    A[Working Directory] --&gt; GA([git add])\n    GA --&gt; B[Staging Area]\n    B --&gt; GC([git commit])\n    GC --&gt; C[Local Repository]\n    C --&gt; GP([git push])\n    GP --&gt; D[Remote Repository]\n    D --&gt; GPL([git pull])\n    GPL --&gt; A\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n\n\n\n\n\nflowchart LR\n    A[Working Directory] --&gt; GA([git add])\n    GA --&gt; B[Staging Area]\n    B --&gt; GC([git commit])\n    GC --&gt; C[Local Repository]\n    C --&gt; GP([git push])\n    GP --&gt; D[Remote Repository]\n    D --&gt; GPL([git pull])\n    GPL --&gt; A\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 9.1: The Git workflow: files move from working directory to staging area to repository\n\n\n\n\n\n\nWorking Directory\n\nThe files you see and edit on your computer.\n\nStaging Area (Index)\n\nA preparation area where you compose your next commit.\n\nLocal Repository\n\nThe .git directory containing all version history.\n\nRemote Repository\n\nA copy hosted elsewhere (like GitHub) for backup and collaboration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#setting-up-git",
    "href": "chapters/09-git-github.html#setting-up-git",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.3 Setting Up Git",
    "text": "9.3 Setting Up Git\n\n9.3.1 Installation\nCheck if Git is already installed:\n$ git --version\ngit version 2.43.0\nIf not installed:\n# macOS (with Homebrew)\n$ brew install git\n\n# Ubuntu/Debian Linux\n$ sudo apt-get install git\n\n# Windows: Download from git-scm.com\n\n\n9.3.2 Configuration\nConfigure your identity (this appears in your commit history):\n# Set your name\n$ git config --global user.name \"Your Name\"\n\n# Set your email (use the same email as your GitHub account)\n$ git config --global user.email \"your.email@example.com\"\n\n# Set default branch name to 'main'\n$ git config --global init.defaultBranch main\n\n# Verify settings\n$ git config --list\n\n\n\n\n\n\nTip\n\n\n\nThe --global flag sets these options for all repositories on your computer. You can override them per-repository by omitting --global while inside a specific repo.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#creating-a-repository",
    "href": "chapters/09-git-github.html#creating-a-repository",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.4 Creating a Repository",
    "text": "9.4 Creating a Repository\n\n9.4.1 Method 1: Initialize Locally\nCreate a new repository from an existing directory:\n# Navigate to your project directory\n$ cd my_project\n\n# Initialize Git repository\n$ git init\n\n# You'll see a message about creating .git directory\nInitialized empty Git repository in /home/user/my_project/.git/\n\n\n9.4.2 Method 2: Clone from GitHub\nCopy an existing repository:\n# Clone a repository\n$ git clone https://github.com/username/repository.git\n\n# Clone into a specific directory\n$ git clone https://github.com/username/repository.git my_local_name",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#basic-git-workflow",
    "href": "chapters/09-git-github.html#basic-git-workflow",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.5 Basic Git Workflow",
    "text": "9.5 Basic Git Workflow\nThe daily rhythm of Git involves four main commands: status, add, commit, and push.\n\n9.5.1 Checking Status\nAlways start by checking your repository’s status:\n$ git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n        modified:   analysis.R\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        new_data.csv\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\n9.5.2 Staging Changes\nAdd files to the staging area:\n# Add specific file\n$ git add analysis.R\n\n# Add all changes in current directory\n$ git add .\n\n# Add all changes everywhere\n$ git add -A\n\n# Add only certain file types\n$ git add *.R\n\n\n9.5.3 Committing Changes\nRecord staged changes with a descriptive message:\n$ git commit -m \"Add initial data analysis script\"\n[main 7d4e8c2] Add initial data analysis script\n 1 file changed, 45 insertions(+)\n create mode 100644 analysis.R\n\n\n\n\n\n\nWrite Good Commit Messages\n\n\n\nA good commit message:\n\nUses present tense: “Add feature” not “Added feature”\nIs concise but descriptive\nExplains what and why, not how\nReferences issues if applicable: “Fix alignment bug (#42)”\n\nBad: “updates”\nGood: “Add GC content calculation to genome analysis”\n\n\n\n\n9.5.4 Viewing History\nSee your commit history:\n# Full history\n$ git log\n\n# Condensed one-line format\n$ git log --oneline\n\n# With graph showing branches\n$ git log --oneline --graph --all\n\n# Last 5 commits\n$ git log -5\nExample output:\n$ git log --oneline\n7d4e8c2 (HEAD -&gt; main) Add initial data analysis script\n3a1b2c3 Update README with project description\n9f8e7d6 Initial commit",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#working-with-github",
    "href": "chapters/09-git-github.html#working-with-github",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.6 Working with GitHub",
    "text": "9.6 Working with GitHub\nGitHub is a web-based platform for hosting Git repositories. It adds:\n\nCloud backup for your code\nCollaboration features (issues, pull requests)\nProject management tools\nFree website hosting (GitHub Pages)\nIntegration with other services\n\n\n9.6.1 Creating a GitHub Account\n\nGo to github.com\nClick “Sign up”\nChoose a professional username\nUse your academic email for the Student Developer Pack\n\n\n\n\n\n\n\nTip\n\n\n\nApply for the GitHub Student Developer Pack for free access to premium features and developer tools.\n\n\n\n\n9.6.2 Connecting Local to Remote\nAfter creating a repository on GitHub:\n# Add remote repository (origin is conventional name)\n$ git remote add origin https://github.com/username/repository.git\n\n# Verify the remote\n$ git remote -v\norigin  https://github.com/username/repository.git (fetch)\norigin  https://github.com/username/repository.git (push)\n\n# Push your commits to GitHub\n$ git push -u origin main\nThe -u flag sets up tracking, so future pushes only need git push.\n\n\n9.6.3 Push and Pull\nPushing sends your local commits to GitHub:\n$ git push origin main\n# or simply, if tracking is set up:\n$ git push\nPulling retrieves commits from GitHub:\n$ git pull origin main\n# or simply:\n$ git pull\n\n\n\n\n\n\nWarning\n\n\n\nAlways pull before starting work to get any changes from collaborators. Push frequently to back up your work.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#branching",
    "href": "chapters/09-git-github.html#branching",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.7 Branching",
    "text": "9.7 Branching\nBranches let you develop features in isolation without affecting the main codebase.\n\ngitGraph\n    commit id: \"Initial\"\n    commit id: \"Add README\"\n    branch feature\n    checkout feature\n    commit id: \"New feature\"\n    commit id: \"Fix bug\"\n    checkout main\n    commit id: \"Update docs\"\n    merge feature\n    commit id: \"Continue\"\n\n\n\n\n\ngitGraph\n    commit id: \"Initial\"\n    commit id: \"Add README\"\n    branch feature\n    checkout feature\n    commit id: \"New feature\"\n    commit id: \"Fix bug\"\n    checkout main\n    commit id: \"Update docs\"\n    merge feature\n    commit id: \"Continue\"\n\n\n\n\nFigure 9.2: Branching allows parallel development that can later be merged\n\n\n\n\n\n\n9.7.1 Creating and Switching Branches\n# Create a new branch\n$ git branch feature-analysis\n\n# Switch to the branch\n$ git checkout feature-analysis\n\n# Or create and switch in one command\n$ git checkout -b feature-analysis\n\n# List all branches\n$ git branch\n* feature-analysis\n  main\n\n\n9.7.2 Merging Branches\nWhen your feature is complete, merge it back to main:\n# Switch to main\n$ git checkout main\n\n# Merge the feature branch\n$ git merge feature-analysis\n\n# Delete the branch after merging (optional)\n$ git branch -d feature-analysis\n\n\n9.7.3 Resolving Conflicts\nIf Git can’t automatically merge changes, you’ll get a conflict:\n$ git merge feature-branch\nAuto-merging analysis.R\nCONFLICT (content): Merge conflict in analysis.R\nAutomatic merge failed; fix conflicts and then commit the result.\nThe file will contain conflict markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Main branch version\nresult &lt;- mean(data)\n=======\n# Feature branch version\nresult &lt;- median(data)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\nTo resolve:\n\nEdit the file to keep the code you want\nRemove the conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;)\nStage and commit the resolved file",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#the-.gitignore-file",
    "href": "chapters/09-git-github.html#the-.gitignore-file",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.8 The .gitignore File",
    "text": "9.8 The .gitignore File\nNot everything should be tracked. The .gitignore file specifies intentionally untracked files:\n# Create .gitignore\n$ touch .gitignore\nCommon patterns for scientific projects:\n# Data files (too large or sensitive)\n*.fastq\n*.fastq.gz\n*.bam\n*.vcf\ndata/raw/*\n\n# R artifacts\n.Rhistory\n.RData\n.Rproj.user/\n\n# Python artifacts\n__pycache__/\n*.pyc\n.ipynb_checkpoints/\n\n# Output files\nresults/temp/*\n*.log\n\n# System files\n.DS_Store\nThumbs.db\n\n# Credentials (NEVER commit these!)\n*.pem\n.env\nsecrets.txt\n\n\n\n\n\n\nWarning\n\n\n\nAdd .gitignore early! Once files are tracked, adding them to .gitignore won’t untrack them.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#github-pages-free-website-hosting",
    "href": "chapters/09-git-github.html#github-pages-free-website-hosting",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.9 GitHub Pages: Free Website Hosting",
    "text": "9.9 GitHub Pages: Free Website Hosting\nGitHub Pages turns your repository into a website—perfect for project documentation, lab websites, or personal portfolios.\n\nflowchart LR\n    A[Quarto Documents] --&gt; R([Render])\n    R --&gt; B[HTML Files]\n    B --&gt; GP([git push])\n    GP --&gt; C[GitHub Repository]\n    C --&gt; AU([Automatic])\n    AU --&gt; D[GitHub Pages Website]\n    D --&gt; E[yourname.github.io/repo]\n\n\n\n\n\nflowchart LR\n    A[Quarto Documents] --&gt; R([Render])\n    R --&gt; B[HTML Files]\n    B --&gt; GP([git push])\n    GP --&gt; C[GitHub Repository]\n    C --&gt; AU([Automatic])\n    AU --&gt; D[GitHub Pages Website]\n    D --&gt; E[yourname.github.io/repo]\n\n\n\n\nFigure 9.3: GitHub Pages workflow: from Quarto documents to live website\n\n\n\n\n\n\n9.9.1 Setting Up GitHub Pages\n\nGo to your repository on GitHub\nClick Settings → Pages\nUnder “Source”, select Deploy from a branch\nSelect branch: main and folder: /docs (or root)\nClick Save\n\nYour site will be live at: https://username.github.io/repository/\n\n\n9.9.2 For Quarto Projects\nIn your _quarto.yml:\nproject:\n  type: website\n  output-dir: docs\nThen render and push:\n# Render your site\n$ quarto render\n\n# Commit the docs folder\n$ git add docs/\n$ git commit -m \"Update website\"\n\n# Push to GitHub\n$ git push",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#best-practices",
    "href": "chapters/09-git-github.html#best-practices",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.10 Best Practices",
    "text": "9.10 Best Practices\n\n9.10.1 Repository Organization\nproject/\n├── README.md          # Project description (always include!)\n├── LICENSE            # How others can use your code\n├── .gitignore         # Files to ignore\n├── data/              # Data files (consider .gitignore for large files)\n│   ├── raw/           # Original, immutable data\n│   └── processed/     # Cleaned data\n├── scripts/           # Analysis scripts\n│   ├── 01_clean.R\n│   └── 02_analyze.R\n├── results/           # Output files\n└── docs/              # Documentation or website\n\n\n9.10.2 Commit Frequency\n\nCommit early, commit often\nEach commit should be a logical unit of change\nDon’t commit half-finished work to main\nUse branches for experimental work\n\n\n\n9.10.3 Collaboration Workflow\nFor team projects:\n\nPull before starting work\nCreate a branch for your changes\nMake commits with clear messages\nPush your branch to GitHub\nCreate a Pull Request for review\nAfter approval, merge to main",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#useful-commands-reference",
    "href": "chapters/09-git-github.html#useful-commands-reference",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.11 Useful Commands Reference",
    "text": "9.11 Useful Commands Reference\n\n\n\nTable 9.1: Essential Git commands\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ngit init\nInitialize new repository\n\n\ngit clone &lt;url&gt;\nCopy remote repository\n\n\ngit status\nShow current state\n\n\ngit add &lt;file&gt;\nStage changes\n\n\ngit commit -m \"msg\"\nRecord staged changes\n\n\ngit push\nUpload to remote\n\n\ngit pull\nDownload from remote\n\n\ngit branch\nList branches\n\n\ngit checkout -b &lt;name&gt;\nCreate and switch branch\n\n\ngit merge &lt;branch&gt;\nMerge branch\n\n\ngit log --oneline\nView commit history\n\n\ngit diff\nShow unstaged changes\n\n\ngit stash\nTemporarily store changes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#summary",
    "href": "chapters/09-git-github.html#summary",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.12 Summary",
    "text": "9.12 Summary\nVersion control with Git and GitHub is essential for modern research:\n\nGit tracks all changes to your files with complete history\nThe workflow is: edit → stage → commit → push\nBranches enable parallel development and experimentation\nGitHub provides cloud hosting, collaboration, and free websites\nGood practices include meaningful commits, .gitignore, and regular pushing\n\nLearning Git has a steep initial curve, but the investment pays dividends throughout your career. Start simple, use it consistently, and gradually adopt more advanced features.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#resources",
    "href": "chapters/09-git-github.html#resources",
    "title": "9  Version Control with Git and GitHub",
    "section": "9.13 Resources",
    "text": "9.13 Resources\n\nPro Git Book — Comprehensive free reference (Chacon & Straub, 2014)\nGitHub Documentation — Official guides\nSoftware Carpentry Git Lesson — Beginner-friendly tutorial\nQuarto + GitHub Pages — Publishing documentation",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/09-git-github.html#exercises",
    "href": "chapters/09-git-github.html#exercises",
    "title": "9  Version Control with Git and GitHub",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: First Repository\n\nCreate a new directory for a test project\nInitialize it as a Git repository\nCreate a README.md file with a project description\nStage and commit the file\nView your commit history\n\nExercise 2: GitHub Integration\n\nCreate a new repository on GitHub (don’t initialize with README)\nConnect your local repository to GitHub\nPush your commits\nVerify your files appear on GitHub\n\nExercise 3: Branching Practice\n\nCreate a branch called add-analysis\nAdd a new script file on that branch\nCommit your changes\nSwitch back to main\nMerge the branch\nDelete the branch\n\nExercise 4: Collaboration Simulation\n\nClone a repository (your own or a public one)\nMake changes on GitHub directly (edit a file in the browser)\nPull those changes to your local repository\nMake local changes and push them back\n\n\n\n\n\n\n\nBlischak, J. D., Davenport, E. R., & Wilson, G. (2016). A quick introduction to version control with git and GitHub.\n\n\nChacon, S., & Straub, B. (2014). Pro git.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Version Control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html",
    "href": "chapters/10-hpc-talapas.html",
    "title": "10  High-Performance Computing with Talapas",
    "section": "",
    "text": "10.1 What is High-Performance Computing?\nWhen your laptop isn’t enough—when analyses run for days, require hundreds of gigabytes of memory, or need specialized hardware like GPUs—you need high-performance computing (HPC).\nAn HPC cluster is a collection of interconnected computers (called nodes) that work together to solve computational problems. Instead of upgrading your personal computer indefinitely, you submit jobs to a shared resource with far more capability.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#what-is-high-performance-computing",
    "href": "chapters/10-hpc-talapas.html#what-is-high-performance-computing",
    "title": "10  High-Performance Computing with Talapas",
    "section": "",
    "text": "10.1.1 When to Use HPC\nConsider HPC when:\n\nAnalyses take hours or days on your laptop\nData is too large to fit in your computer’s memory\nYou need GPUs for machine learning\nYou want to run many similar jobs in parallel\nYour analysis would monopolize your personal computer\n\n\n\n10.1.2 Talapas: UO’s Computing Cluster\nTalapas is the University of Oregon’s HPC cluster, named from a Chinook word for coyote (the educator and keeper of knowledge). It’s managed by Research Advanced Computing Services (RACS).\nKey resources:\n\nOver 14,520 CPU cores\n129 TB of total memory\n89 GPUs (including NVIDIA A100 and H100)\n3+ petabytes of storage",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#talapas-architecture",
    "href": "chapters/10-hpc-talapas.html#talapas-architecture",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.2 Talapas Architecture",
    "text": "10.2 Talapas Architecture\nUnderstanding the cluster architecture helps you use it effectively.\n\nflowchart TB\n    U[Your Computer] --&gt; SSH([SSH])\n    SSH --&gt; L1[Login Node 1]\n    SSH --&gt; L2[Login Node 2]\n    SSH --&gt; L3[Login Node 3]\n    SSH --&gt; L4[Login Node 4]\n    L1 --&gt; S[SLURM Scheduler]\n    L2 --&gt; S\n    L3 --&gt; S\n    L4 --&gt; S\n    S --&gt; C1[CPU Nodes]\n    S --&gt; C2[GPU Nodes]\n    S --&gt; C3[High-Memory Nodes]\n    C1 --&gt; ST[(Shared Storage)]\n    C2 --&gt; ST\n    C3 --&gt; ST\n    \n    style U fill:#e3f2fd,stroke:#333,stroke-width:2px\n    style L1 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L2 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L3 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L4 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style S fill:#f9d71c,stroke:#333,stroke-width:2px\n    style C1 fill:#98fb98,stroke:#333,stroke-width:2px\n    style C2 fill:#ffa07a,stroke:#333,stroke-width:2px\n    style C3 fill:#dda0dd,stroke:#333,stroke-width:2px\n    style ST fill:#c8e6c9,stroke:#333,stroke-width:2px\n\n\n\n\n\nflowchart TB\n    U[Your Computer] --&gt; SSH([SSH])\n    SSH --&gt; L1[Login Node 1]\n    SSH --&gt; L2[Login Node 2]\n    SSH --&gt; L3[Login Node 3]\n    SSH --&gt; L4[Login Node 4]\n    L1 --&gt; S[SLURM Scheduler]\n    L2 --&gt; S\n    L3 --&gt; S\n    L4 --&gt; S\n    S --&gt; C1[CPU Nodes]\n    S --&gt; C2[GPU Nodes]\n    S --&gt; C3[High-Memory Nodes]\n    C1 --&gt; ST[(Shared Storage)]\n    C2 --&gt; ST\n    C3 --&gt; ST\n    \n    style U fill:#e3f2fd,stroke:#333,stroke-width:2px\n    style L1 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L2 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L3 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style L4 fill:#87ceeb,stroke:#333,stroke-width:2px\n    style S fill:#f9d71c,stroke:#333,stroke-width:2px\n    style C1 fill:#98fb98,stroke:#333,stroke-width:2px\n    style C2 fill:#ffa07a,stroke:#333,stroke-width:2px\n    style C3 fill:#dda0dd,stroke:#333,stroke-width:2px\n    style ST fill:#c8e6c9,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 10.1: Talapas cluster architecture showing login nodes, scheduler, and compute nodes\n\n\n\n\n\n\n10.2.1 Login Nodes vs. Compute Nodes\n\n\n\n\n\n\nNever Run Jobs on Login Nodes\n\n\n\nLogin nodes are where you:\n\nLog in and manage files\nEdit scripts\nSubmit jobs\nCheck job status\n\nDo NOT run computationally intensive work on login nodes! They’re shared by everyone, and heavy processes will be terminated.\nCompute nodes are where your actual work runs, accessed through the job scheduler.\n\n\n\n\n10.2.2 Node Types\n\n\n\nTable 10.1: Talapas node types\n\n\n\n\n\nNode Type\nCores\nMemory\nPurpose\n\n\n\n\nStandard CPU\n28-128\n128-512 GB\nGeneral computation\n\n\nLarge Memory\n56\n1-4 TB\nMemory-intensive work\n\n\nGPU\n28-48\n256-512 GB\nMachine learning, CUDA\n\n\n\n\n\n\n\n\n10.2.3 Partitions (Queues)\nJobs are submitted to partitions with different characteristics:\n\n\n\nTable 10.2: Talapas partitions\n\n\n\n\n\nPartition\nTime Limit\nNotes\n\n\n\n\ncompute\n24 hours\nGeneral purpose (default)\n\n\nshort\n24 hours\nShort jobs\n\n\nlong\n14 days\nExtended jobs\n\n\ngpu / longgpu\n24h / 14d\nGPU-enabled nodes\n\n\nfat / longfat\n24h / 14d\nHigh-memory nodes\n\n\npreempt\nvaries\nCan use idle resources (may be interrupted)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#getting-access",
    "href": "chapters/10-hpc-talapas.html#getting-access",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.3 Getting Access",
    "text": "10.3 Getting Access\n\n10.3.1 Step 1: Join a PIRG\nA PIRG (Principal Investigator Research Group) is your access group. Your PI must add you to their PIRG, or contact RACS to create a new one.\n\n\n10.3.2 Step 2: Request Access\nVisit racs.uoregon.edu/request-access\nYour credentials:\n\nUsername: Your Duck ID\nPassword: Your UO password",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#connecting-to-talapas",
    "href": "chapters/10-hpc-talapas.html#connecting-to-talapas",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.4 Connecting to Talapas",
    "text": "10.4 Connecting to Talapas\n\n10.4.1 Option 1: SSH (Command Line)\nFrom your terminal:\n# Connect to Talapas\n$ ssh yourduckid@login.talapas.uoregon.edu\n\n# Or connect to a specific login node\n$ ssh yourduckid@login1.talapas.uoregon.edu\nOff campus? Connect to the UO VPN first.\nWindows users: Use PuTTY, MobaXterm, Windows Terminal, or VS Code with the Remote-SSH extension.\n\n\n10.4.2 Option 2: Open OnDemand (Web Browser)\nFor a graphical interface, use Open OnDemand:\nondemand.talapas.uoregon.edu\nOpen OnDemand provides:\n\nFile browser for uploads/downloads\nInteractive applications (RStudio, JupyterLab, MATLAB)\nJob composer and monitoring\nTerminal in browser\n\nThis is excellent for beginners and interactive data analysis.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#storage-on-talapas",
    "href": "chapters/10-hpc-talapas.html#storage-on-talapas",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.5 Storage on Talapas",
    "text": "10.5 Storage on Talapas\n\nflowchart LR\n    R[\"/\"] --&gt; H[\"/home\"]\n    R --&gt; P[\"/projects\"]\n    H --&gt; HD[\"/home/duckid&lt;br/&gt;250 GB\"]\n    P --&gt; PG[\"/projects/PIRG\"]\n    PG --&gt; PGU[\"/projects/PIRG/duckid&lt;br/&gt;Your workspace\"]\n    PG --&gt; PGS[\"/projects/PIRG/shared&lt;br/&gt;Shared with group\"]\n    \n    style HD fill:#87ceeb,stroke:#333,stroke-width:2px\n    style PGU fill:#98fb98,stroke:#333,stroke-width:2px\n    style PGS fill:#f9d71c,stroke:#333,stroke-width:2px\n\n\n\n\n\nflowchart LR\n    R[\"/\"] --&gt; H[\"/home\"]\n    R --&gt; P[\"/projects\"]\n    H --&gt; HD[\"/home/duckid&lt;br/&gt;250 GB\"]\n    P --&gt; PG[\"/projects/PIRG\"]\n    PG --&gt; PGU[\"/projects/PIRG/duckid&lt;br/&gt;Your workspace\"]\n    PG --&gt; PGS[\"/projects/PIRG/shared&lt;br/&gt;Shared with group\"]\n    \n    style HD fill:#87ceeb,stroke:#333,stroke-width:2px\n    style PGU fill:#98fb98,stroke:#333,stroke-width:2px\n    style PGS fill:#f9d71c,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 10.2: Talapas storage hierarchy\n\n\n\n\n\n\n10.5.1 Directory Structure\n\n\n\nTable 10.3: Talapas storage locations\n\n\n\n\n\n\n\n\n\n\nLocation\nQuota\nPurpose\n\n\n\n\n/home/duckid\n250 GB\nConfig files, small scripts\n\n\n/projects/PIRG/duckid\nShared (default 2 TB)\nLarge data, analysis\n\n\n/projects/PIRG/shared\nShared\nFiles shared with group\n\n\n/tmp (on compute nodes)\nLocal to node\nTemporary scratch space\n\n\n\n\n\n\n\n\n10.5.2 Check Your Storage\n# Home directory usage\n$ df -h ~\n\n# Project directory usage\n$ du -sh /projects/myPIRG/myusername\n\n# Detailed breakdown\n$ du -h --max-depth=1 /projects/myPIRG/myusername\n\n\n\n\n\n\nImportant\n\n\n\nData is NOT automatically backed up! You are responsible for backing up important files. Consider using Git for code and external storage for irreplaceable data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#slurm-the-job-scheduler",
    "href": "chapters/10-hpc-talapas.html#slurm-the-job-scheduler",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.6 SLURM: The Job Scheduler",
    "text": "10.6 SLURM: The Job Scheduler\nSLURM (Simple Linux Utility for Resource Management) is the software that manages the cluster (Yoo et al., 2003). It:\n\nAllocates resources fairly among users\nQueues jobs when resources are busy\nTracks resource usage\n\n\nflowchart LR\n    A[Submit Job] --&gt; SB([sbatch])\n    SB --&gt; B[Queue]\n    B --&gt; W([Wait])\n    W --&gt; C[Run on Compute Node]\n    C --&gt; CO([Complete])\n    CO --&gt; D[Output Files]\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n\n\n\n\n\nflowchart LR\n    A[Submit Job] --&gt; SB([sbatch])\n    SB --&gt; B[Queue]\n    B --&gt; W([Wait])\n    W --&gt; C[Run on Compute Node]\n    C --&gt; CO([Complete])\n    CO --&gt; D[Output Files]\n    \n    style A fill:#f9d71c,stroke:#333,stroke-width:2px\n    style B fill:#87ceeb,stroke:#333,stroke-width:2px\n    style C fill:#98fb98,stroke:#333,stroke-width:2px\n    style D fill:#ffa07a,stroke:#333,stroke-width:2px\n\n\n\n\nFigure 10.3: SLURM job lifecycle from submission to completion",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#writing-slurm-batch-scripts",
    "href": "chapters/10-hpc-talapas.html#writing-slurm-batch-scripts",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.7 Writing SLURM Batch Scripts",
    "text": "10.7 Writing SLURM Batch Scripts\nA batch script combines SLURM directives with your commands:\n#!/bin/bash\n#SBATCH --job-name=my_analysis      # Job name (shows in queue)\n#SBATCH --partition=compute         # Partition (queue)\n#SBATCH --account=myPIRG            # Your PIRG name\n#SBATCH --output=output_%j.log      # Standard output (%j = job ID)\n#SBATCH --error=error_%j.log        # Standard error\n#SBATCH --time=04:00:00             # Time limit (HH:MM:SS)\n#SBATCH --nodes=1                   # Number of nodes\n#SBATCH --ntasks-per-node=1         # Tasks per node\n#SBATCH --cpus-per-task=4           # CPUs per task\n#SBATCH --mem=16G                   # Memory per node\n\n# Load required modules\nmodule load R/4.3.3\n\n# Change to working directory\ncd /projects/myPIRG/myusername/analysis\n\n# Run your program\nRscript my_analysis.R\n\n10.7.1 Understanding SLURM Directives\n\n\n\nTable 10.4: SLURM directives reference\n\n\n\n\n\nDirective\nDescription\nExample\n\n\n\n\n--partition\nQueue to use\ncompute, gpu, fat\n\n\n--account\nPIRG for accounting\nmyPIRG\n\n\n--time\nMaximum runtime\n1-12:00:00 (1 day, 12 hrs)\n\n\n--nodes\nNumber of nodes\n1\n\n\n--cpus-per-task\nCores per task\n8\n\n\n--mem\nTotal memory per node\n32G\n\n\n--mem-per-cpu\nMemory per CPU\n4G\n\n\n--gres\nGeneric resources\ngpu:1\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRequest only what you need! Smaller resource requests often run sooner. Start conservative and increase if jobs fail.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#submitting-and-managing-jobs",
    "href": "chapters/10-hpc-talapas.html#submitting-and-managing-jobs",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.8 Submitting and Managing Jobs",
    "text": "10.8 Submitting and Managing Jobs\n\n10.8.1 Submit a Job\n# Submit batch job\n$ sbatch my_script.sh\nSubmitted batch job 12345678\n\n\n10.8.2 Check Job Status\n# Your jobs\n$ squeue -u $USER\n\n# All jobs on a partition\n$ squeue -p compute\n\n# Detailed job info\n$ scontrol show job 12345678\n\n\n10.8.3 Job Status Codes\n\n\n\nTable 10.5: SLURM job status codes\n\n\n\n\n\nCode\nMeaning\n\n\n\n\nPD\nPending (waiting in queue)\n\n\nR\nRunning\n\n\nCG\nCompleting\n\n\nCD\nCompleted\n\n\nF\nFailed\n\n\nCA\nCancelled\n\n\n\n\n\n\n\n\n10.8.4 Cancel a Job\n# Cancel specific job\n$ scancel 12345678\n\n# Cancel all your jobs\n$ scancel -u $USER\n\n\n10.8.5 Check Resource Usage\nAfter a job completes:\n$ sacct -j 12345678 --format=JobID,JobName,Elapsed,MaxRSS,State",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#interactive-jobs",
    "href": "chapters/10-hpc-talapas.html#interactive-jobs",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.9 Interactive Jobs",
    "text": "10.9 Interactive Jobs\nFor testing and development, use interactive jobs instead of batch:\n# Start interactive session\n$ srun --pty --partition=compute --account=myPIRG \\\n       --time=2:00:00 --cpus-per-task=4 --mem=8G /bin/bash\nYour prompt will change from login1 to something like n049, indicating you’re on a compute node.\nInteractive jobs are ideal for:\n\nTesting workflows before batch submission\nDebugging code\nQuick data exploration\nRunning GUI applications via Open OnDemand",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#software-modules",
    "href": "chapters/10-hpc-talapas.html#software-modules",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.10 Software Modules",
    "text": "10.10 Software Modules\nTalapas uses the Lmod module system to manage software. This lets multiple versions coexist without conflicts.\n\n10.10.1 Basic Module Commands\n# List available modules\n$ module avail\n\n# Search for specific software\n$ module spider R\n$ module spider python\n\n# Load a module\n$ module load R/4.3.3\n$ module load miniconda3/20240410\n\n# See loaded modules\n$ module list\n\n# Unload a module\n$ module unload R\n\n# Reset to default state\n$ module purge\n\n\n10.10.2 In Batch Scripts\n\n\n\n\n\n\nImportant\n\n\n\nAlways load modules after the #SBATCH directives in your batch scripts:\n#!/bin/bash\n#SBATCH --job-name=analysis\n#SBATCH --partition=compute\n#SBATCH --account=myPIRG\n# ... other directives ...\n\n# Load modules here\nmodule load R/4.3.3\n\n# Now run your code\nRscript analysis.R",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#gpu-jobs",
    "href": "chapters/10-hpc-talapas.html#gpu-jobs",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.11 GPU Jobs",
    "text": "10.11 GPU Jobs\nFor machine learning and GPU-accelerated computing:\n#!/bin/bash\n#SBATCH --job-name=gpu_job\n#SBATCH --partition=gpu             # GPU partition\n#SBATCH --account=myPIRG\n#SBATCH --time=1-00:00:00           # 1 day\n#SBATCH --nodes=1\n#SBATCH --gres=gpu:1                # Request 1 GPU\n#SBATCH --mem=32G\n\n# Load CUDA and your software\nmodule load cuda/12.4\nmodule load miniconda3/20240410\n\n# Activate your environment\nconda activate ml_env\n\n# Run GPU code\npython train_model.py\nCheck available GPUs:\n$ /packages/racs/bin/slurm-show-gpus",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#a-complete-example",
    "href": "chapters/10-hpc-talapas.html#a-complete-example",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.12 A Complete Example",
    "text": "10.12 A Complete Example\nLet’s walk through a realistic bioinformatics workflow:\n\n10.12.1 1. Prepare Your Script\nCreate genome_analysis.sh:\n#!/bin/bash\n#SBATCH --job-name=genome_analysis\n#SBATCH --partition=compute\n#SBATCH --account=myPIRG\n#SBATCH --output=logs/genome_%j.out\n#SBATCH --error=logs/genome_%j.err\n#SBATCH --time=08:00:00\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=32G\n\n# Exit on error\nset -e\n\n# Load modules\nmodule load bwa/0.7.17\nmodule load samtools/1.17\n\n# Set variables\nGENOME=\"/projects/myPIRG/shared/reference/human_GRCh38.fa\"\nREADS_R1=\"/projects/myPIRG/myuser/data/sample_R1.fastq.gz\"\nREADS_R2=\"/projects/myPIRG/myuser/data/sample_R2.fastq.gz\"\nOUTPUT=\"/projects/myPIRG/myuser/results\"\n\n# Create output directory\nmkdir -p $OUTPUT\n\n# Log start time\necho \"Starting analysis at $(date)\"\n\n# Run alignment\nbwa mem -t $SLURM_CPUS_PER_TASK $GENOME $READS_R1 $READS_R2 | \\\n    samtools sort -@ 4 -o $OUTPUT/aligned.bam\n\n# Index BAM file\nsamtools index $OUTPUT/aligned.bam\n\n# Log completion\necho \"Analysis completed at $(date)\"\n\n\n10.12.2 2. Submit and Monitor\n# Create logs directory\n$ mkdir -p logs\n\n# Submit job\n$ sbatch genome_analysis.sh\nSubmitted batch job 12345678\n\n# Monitor status\n$ squeue -u $USER\n             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n          12345678   compute genome_a   duckid   R       5:32      1 n042\n\n# Check output in real-time\n$ tail -f logs/genome_12345678.out",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#best-practices",
    "href": "chapters/10-hpc-talapas.html#best-practices",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.13 Best Practices",
    "text": "10.13 Best Practices\n\n10.13.1 Do\n\nRequest only the resources you need\nTest with small datasets first\nUse --time conservatively (but not too short)\nClean up old files regularly\nUse /projects for large data\nDocument your workflows\nAcknowledge RACS in publications\n\n\n\n10.13.2 Don’t\n\nRun heavy computation on login nodes\nLeave jobs running indefinitely\nStore sensitive data without encryption\nIgnore error messages\nRequest maximum resources “just in case”\n\n\n\n10.13.3 Acknowledgment\nWhen publishing research using Talapas, include this acknowledgment:\n\n“The authors acknowledge Research Advanced Computing Services (RACS) at the University of Oregon for providing computing resources that have contributed to the research results reported within this publication.”",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#getting-help",
    "href": "chapters/10-hpc-talapas.html#getting-help",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.14 Getting Help",
    "text": "10.14 Getting Help\n\n10.14.1 Resources\n\nDocumentation: uoracs.github.io/talapas2-knowledge-base\nService Desk: hpcrcf.atlassian.net/servicedesk\nEmail: racs@uoregon.edu\nOffice Hours: Knight Library, Dream Lab 122E (Wednesdays 1-3pm)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#summary",
    "href": "chapters/10-hpc-talapas.html#summary",
    "title": "10  High-Performance Computing with Talapas",
    "section": "10.15 Summary",
    "text": "10.15 Summary\nHigh-performance computing extends your analytical capabilities beyond personal computers:\n\nTalapas provides thousands of CPUs, GPUs, and terabytes of storage\nConnect via SSH or the Open OnDemand web interface\nSLURM manages job scheduling and resource allocation\nBatch scripts combine resource requests with your commands\nSoftware modules manage different software versions\nResponsible use ensures fair access for everyone\n\nStart simple—submit small test jobs, verify they work, then scale up. The HPC learning curve is real, but the capability it provides is essential for modern computational research.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "chapters/10-hpc-talapas.html#exercises",
    "href": "chapters/10-hpc-talapas.html#exercises",
    "title": "10  High-Performance Computing with Talapas",
    "section": "Exercises",
    "text": "Exercises\n\n\n\n\n\n\nPractice Exercises\n\n\n\nExercise 1: First Connection\n\nLog into Talapas via SSH\nCheck which login node you’re on with hostname\nExplore your home directory and check your quota\nFind your project directory\n\nExercise 2: Module Practice\n\nSearch for available R versions\nLoad R and verify with R --version\nSearch for Python versions\nCreate a script that loads both R and Python\n\nExercise 3: Interactive Job\n\nStart an interactive session with 2 CPUs and 4GB memory\nNote how your prompt changes\nRun a simple R command\nExit the interactive session\n\nExercise 4: Batch Job\n\nWrite a batch script that:\n\nRequests 1 hour, 2 CPUs, 8GB memory\nLoads R\nRuns a simple R script that generates a plot\n\nSubmit the job\nMonitor its progress\nExamine the output files\n\n\n\n\n\n\n\nYoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple linux utility for resource management.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>High-Performance Computing with Talapas</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Blischak, J. D., Davenport, E. R., & Wilson, G. (2016). A quick introduction\nto version control with git and GitHub.\n\n\nChacon, S., & Straub, B. (2014). Pro git.\n\n\nKernighan, B. W., & Pike, R. (1984). The UNIX programming\nenvironment.\n\n\nPosit Software, PBC (2024). Quarto:\nOpen-source scientific and technical publishing system.\n\n\nR Core Team (2024). R: A language\nand environment for statistical computing.\n\n\nSandve, G. K. et al. (2013). Ten simple rules for\nreproducible computational research.\n\n\nSoftware Carpentry (2024b). The unix shell.\n\n\nSoftware Carpentry (2024a). Version control with\ngit.\n\n\nWickham, H. (2014). Tidy\ndata.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy,\ntransform, visualize, and model data.\n\n\nYoo, A. B., Jette, M. A., & Grondona, M. (2003). SLURM: Simple linux utility\nfor resource management.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html",
    "href": "chapters/appendix-commands.html",
    "title": "Appendix A — Command Reference",
    "section": "",
    "text": "A.1 Unix Navigation Commands\nThis appendix provides a quick reference for commands covered throughout the book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#unix-navigation-commands",
    "href": "chapters/appendix-commands.html#unix-navigation-commands",
    "title": "Appendix A — Command Reference",
    "section": "",
    "text": "Table A.1: Navigation commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\npwd\nPrint working directory\npwd\n\n\nls\nList directory contents\nls -la\n\n\ncd\nChange directory\ncd ~/projects\n\n\nmkdir\nCreate directory\nmkdir -p data/raw\n\n\nrmdir\nRemove empty directory\nrmdir old_folder",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#file-operations",
    "href": "chapters/appendix-commands.html#file-operations",
    "title": "Appendix A — Command Reference",
    "section": "A.2 File Operations",
    "text": "A.2 File Operations\n\n\n\nTable A.2: File operation commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ncp\nCopy files/directories\ncp file.txt backup/\n\n\nmv\nMove or rename\nmv old.txt new.txt\n\n\nrm\nRemove files\nrm -r old_directory/\n\n\ntouch\nCreate empty file\ntouch notes.txt\n\n\ncat\nDisplay file contents\ncat data.csv\n\n\nhead\nShow first lines\nhead -n 20 file.txt\n\n\ntail\nShow last lines\ntail -f log.txt\n\n\nless\nPage through file\nless huge_file.txt\n\n\nwc\nCount lines/words/bytes\nwc -l data.csv",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#text-processing",
    "href": "chapters/appendix-commands.html#text-processing",
    "title": "Appendix A — Command Reference",
    "section": "A.3 Text Processing",
    "text": "A.3 Text Processing\n\n\n\nTable A.3: Text processing commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngrep\nSearch for patterns\ngrep \"error\" log.txt\n\n\ngrep -E\nExtended regex\ngrep -E \"gene[0-9]+\"\n\n\ngrep -v\nInvert match\ngrep -v \"^#\" data.txt\n\n\ngrep -c\nCount matches\ngrep -c \"&gt;\" seqs.fa\n\n\ngrep -i\nCase insensitive\ngrep -i \"warning\"\n\n\nsort\nSort lines\nsort -n numbers.txt\n\n\nuniq\nRemove duplicates\nsort \\| uniq -c\n\n\ncut\nExtract columns\ncut -f2 data.tsv\n\n\ntr\nTranslate characters\ntr 'a-z' 'A-Z'\n\n\nsed\nStream editor\nsed 's/old/new/g'\n\n\nawk\nPattern processing\nawk '{print $1}'",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#redirection-and-pipes",
    "href": "chapters/appendix-commands.html#redirection-and-pipes",
    "title": "Appendix A — Command Reference",
    "section": "A.4 Redirection and Pipes",
    "text": "A.4 Redirection and Pipes\n\n\n\nTable A.4: Redirection operators\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&gt;\nRedirect output (overwrite)\nls &gt; files.txt\n\n\n&gt;&gt;\nRedirect output (append)\necho \"done\" &gt;&gt; log.txt\n\n\n&lt;\nRedirect input\nwc -l &lt; data.txt\n\n\n2&gt;\nRedirect stderr\ncmd 2&gt; errors.txt\n\n\n&&gt;\nRedirect both stdout/stderr\ncmd &&gt; all.txt\n\n\n\\|\nPipe to next command\ncat file \\| sort \\| uniq\n\n\n\\|&\nPipe stdout and stderr\ncmd \\|& less",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#file-compression",
    "href": "chapters/appendix-commands.html#file-compression",
    "title": "Appendix A — Command Reference",
    "section": "A.5 File Compression",
    "text": "A.5 File Compression\n\n\n\nTable A.5: Compression commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngzip\nCompress file\ngzip large_file.txt\n\n\ngunzip\nDecompress\ngunzip file.txt.gz\n\n\nzcat\nView compressed\nzcat data.gz \\| head\n\n\nzgrep\nSearch compressed\nzgrep \"pattern\" file.gz\n\n\ntar\nArchive files\ntar -czvf archive.tar.gz dir/\n\n\ntar\nExtract archive\ntar -xzvf archive.tar.gz",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#file-permissions",
    "href": "chapters/appendix-commands.html#file-permissions",
    "title": "Appendix A — Command Reference",
    "section": "A.6 File Permissions",
    "text": "A.6 File Permissions\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nchmod\nChange permissions\nchmod +x script.sh\n\n\nchmod\nNumeric permissions\nchmod 755 script.sh\n\n\nchown\nChange owner\nchown user:group file\n\n\n\n\nA.6.1 Permission Codes\n\n\n\nTable A.6: Permission codes\n\n\n\n\n\nMode\nMeaning\n\n\n\n\nr (4)\nRead\n\n\nw (2)\nWrite\n\n\nx (1)\nExecute\n\n\n755\nrwx for owner, rx for others\n\n\n644\nrw for owner, r for others",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#system-information",
    "href": "chapters/appendix-commands.html#system-information",
    "title": "Appendix A — Command Reference",
    "section": "A.7 System Information",
    "text": "A.7 System Information\n\n\n\nTable A.7: System information commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nwhoami\nCurrent username\nwhoami\n\n\nhostname\nComputer name\nhostname\n\n\ndate\nCurrent date/time\ndate +%Y-%m-%d\n\n\ndf\nDisk space\ndf -h\n\n\ndu\nDirectory size\ndu -sh folder/\n\n\ntop\nRunning processes\ntop\n\n\nps\nProcess status\nps aux\n\n\nkill\nTerminate process\nkill -9 PID",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#network-and-remote",
    "href": "chapters/appendix-commands.html#network-and-remote",
    "title": "Appendix A — Command Reference",
    "section": "A.8 Network and Remote",
    "text": "A.8 Network and Remote\n\n\n\nTable A.8: Network commands\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nssh\nSecure shell\nssh user@host\n\n\nscp\nSecure copy\nscp file user@host:path/\n\n\nrsync\nSync files\nrsync -avz src/ dest/\n\n\nwget\nDownload file\nwget URL\n\n\ncurl\nTransfer data\ncurl -O URL",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#git-commands",
    "href": "chapters/appendix-commands.html#git-commands",
    "title": "Appendix A — Command Reference",
    "section": "A.9 Git Commands",
    "text": "A.9 Git Commands\n\nA.9.1 Setup and Configuration\n\n\n\nCommand\nDescription\n\n\n\n\ngit config --global user.name \"Name\"\nSet username\n\n\ngit config --global user.email \"email\"\nSet email\n\n\ngit config --list\nShow configuration\n\n\n\n\n\nA.9.2 Repository Operations\n\n\n\nCommand\nDescription\n\n\n\n\ngit init\nInitialize repository\n\n\ngit clone URL\nClone remote repository\n\n\ngit status\nShow current state\n\n\ngit log --oneline\nView commit history\n\n\n\n\n\nA.9.3 Making Changes\n\n\n\nCommand\nDescription\n\n\n\n\ngit add file\nStage file\n\n\ngit add .\nStage all changes\n\n\ngit commit -m \"message\"\nCommit staged changes\n\n\ngit diff\nShow unstaged changes\n\n\ngit diff --staged\nShow staged changes\n\n\n\n\n\nA.9.4 Branching\n\n\n\nCommand\nDescription\n\n\n\n\ngit branch\nList branches\n\n\ngit branch name\nCreate branch\n\n\ngit checkout name\nSwitch branch\n\n\ngit checkout -b name\nCreate and switch\n\n\ngit merge name\nMerge branch\n\n\ngit branch -d name\nDelete branch\n\n\n\n\n\nA.9.5 Remote Operations\n\n\n\nCommand\nDescription\n\n\n\n\ngit remote -v\nShow remotes\n\n\ngit remote add origin URL\nAdd remote\n\n\ngit push origin branch\nPush to remote\n\n\ngit pull origin branch\nPull from remote\n\n\ngit fetch origin\nFetch without merge",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#slurm-commands",
    "href": "chapters/appendix-commands.html#slurm-commands",
    "title": "Appendix A — Command Reference",
    "section": "A.10 SLURM Commands",
    "text": "A.10 SLURM Commands\n\nA.10.1 Job Submission\n\n\n\nCommand\nDescription\n\n\n\n\nsbatch script.sh\nSubmit batch job\n\n\nsrun --pty bash\nStart interactive job\n\n\n\n\n\nA.10.2 Job Management\n\n\n\nCommand\nDescription\n\n\n\n\nsqueue -u $USER\nShow your jobs\n\n\nsqueue -p partition\nShow partition jobs\n\n\nscancel JOBID\nCancel job\n\n\nscancel -u $USER\nCancel all your jobs\n\n\nscontrol show job JOBID\nJob details\n\n\nsacct -j JOBID\nJob accounting\n\n\n\n\n\nA.10.3 Information\n\n\n\nCommand\nDescription\n\n\n\n\nsinfo\nPartition information\n\n\nsinfo -p partition\nSpecific partition\n\n\n\n\n\nA.10.4 Common SBATCH Directives\n#SBATCH --job-name=name       # Job name\n#SBATCH --partition=compute   # Partition\n#SBATCH --account=PIRG        # Account\n#SBATCH --time=HH:MM:SS       # Time limit\n#SBATCH --nodes=1             # Number of nodes\n#SBATCH --cpus-per-task=4     # CPUs per task\n#SBATCH --mem=16G             # Memory\n#SBATCH --gres=gpu:1          # GPUs\n#SBATCH --output=out_%j.log   # Output file\n#SBATCH --error=err_%j.log    # Error file",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#module-commands",
    "href": "chapters/appendix-commands.html#module-commands",
    "title": "Appendix A — Command Reference",
    "section": "A.11 Module Commands",
    "text": "A.11 Module Commands\n\n\n\nCommand\nDescription\n\n\n\n\nmodule avail\nList available modules\n\n\nmodule spider name\nSearch for module\n\n\nmodule load name\nLoad module\n\n\nmodule unload name\nUnload module\n\n\nmodule list\nShow loaded modules\n\n\nmodule purge\nUnload all modules",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#r-quick-reference",
    "href": "chapters/appendix-commands.html#r-quick-reference",
    "title": "Appendix A — Command Reference",
    "section": "A.12 R Quick Reference",
    "text": "A.12 R Quick Reference\n\nA.12.1 Basic Operations\n# Assignment\nx &lt;- 5\n\n# Vectors\nv &lt;- c(1, 2, 3, 4, 5)\n\n# Sequences\n1:10\nseq(0, 1, by = 0.1)\n\n# Basic stats\nmean(v)\nsd(v)\nsummary(v)\n\n\nA.12.2 Data Frames\n# Create\ndf &lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n# Access\ndf$x\ndf[1, ]\ndf[, \"x\"]\n\n# Filter\ndf[df$x &gt; 1, ]\n\n# Read/Write\nread.csv(\"file.csv\")\nwrite.csv(df, \"file.csv\", row.names = FALSE)\n\n\nA.12.3 Getting Help\n?function_name\nhelp(function_name)\nexample(function_name)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#regular-expression-metacharacters",
    "href": "chapters/appendix-commands.html#regular-expression-metacharacters",
    "title": "Appendix A — Command Reference",
    "section": "A.13 Regular Expression Metacharacters",
    "text": "A.13 Regular Expression Metacharacters\n\n\n\nTable A.9: Regular expression metacharacters\n\n\n\n\n\nCharacter\nMeaning\n\n\n\n\n.\nAny single character\n\n\n*\nZero or more of preceding\n\n\n+\nOne or more of preceding\n\n\n?\nZero or one of preceding\n\n\n^\nStart of line\n\n\n$\nEnd of line\n\n\n[]\nCharacter class\n\n\n[^]\nNegated class\n\n\n\\|\nAlternation (OR)\n\n\n()\nGrouping\n\n\n{n}\nExactly n repetitions\n\n\n{n,m}\nBetween n and m repetitions\n\n\n\\\nEscape special character",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/appendix-commands.html#useful-shortcuts",
    "href": "chapters/appendix-commands.html#useful-shortcuts",
    "title": "Appendix A — Command Reference",
    "section": "A.14 Useful Shortcuts",
    "text": "A.14 Useful Shortcuts\n\nA.14.1 Bash Keyboard Shortcuts\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+C\nCancel current command\n\n\nCtrl+D\nEnd of input / logout\n\n\nCtrl+L\nClear screen\n\n\nCtrl+A\nMove to start of line\n\n\nCtrl+E\nMove to end of line\n\n\nCtrl+R\nSearch command history\n\n\nTab\nAutocomplete\n\n\n↑ / ↓\nPrevious/next command\n\n\n\n\n\nA.14.2 RStudio Keyboard Shortcuts\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+Enter\nRun current line\n\n\nCtrl+Shift+Enter\nRun current chunk\n\n\nCtrl+Shift+S\nRun entire script\n\n\nCtrl+Shift+C\nComment/uncomment\n\n\nCtrl+Shift+M\nInsert pipe %&gt;%\n\n\nAlt+-\nInsert assignment &lt;-\n\n\nCtrl+Shift+K\nKnit document",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Command Reference</span>"
    ]
  }
]